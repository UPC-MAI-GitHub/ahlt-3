{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NERC Project for Recognizing and Classifying Drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "from os import listdir\n",
    "from xml.dom.minidom import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mponsclo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mponsclo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence examples\n",
    "sent_1 = \"Activation of an effector immediate-early gene arc by methamphetamine\"\n",
    "sent_2 = \"In situations in which concurrent therapy is necessary, careful patient monitoring is essential.\"\n",
    "sent_3 = \"Phenothiazines and butyrophenones may reduce or reverse the pressor effect of epinephrine.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Activation', 'of', 'an', 'effector', 'immediate-early', 'gene', 'arc', 'by', 'methamphetamine']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize word\n",
    "tokenized_sent_1 = word_tokenize(sent_1)\n",
    "tokenized_sent_2 = word_tokenize(sent_2)\n",
    "print(tokenized_sent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OffsetFrom: 0\n",
      "OffsetTo: 9\n",
      "\n",
      "OffsetFrom: 3\n",
      "OffsetTo: 12\n"
     ]
    }
   ],
   "source": [
    "# Use the .find() method to find offset and end position\n",
    "print(\"OffsetFrom: \" + str(sent_1.find(tokenized_sent_1[0]))) # offset\n",
    "print(\"OffsetTo: \" + str(sent_1.find(tokenized_sent_1[0]) + len(tokenized_sent_1[0]) - 1)) # end\n",
    "print(\"\")\n",
    "print(\"OffsetFrom: \" + str(sent_2.find(tokenized_sent_2[1]))) # offset\n",
    "print(\"OffsetTo: \" + str(sent_2.find(tokenized_sent_2[1]) + len(tokenized_sent_2[1]) - 1)) # end\n",
    "\n",
    "# From this results generate desired output: list of tuples (word, offsetFrom, offsetTo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['Activation', 'of', 'an', 'effector', 'immediate-early', 'gene', 'arc', 'by', 'methamphetamine']\n",
      "Filterd Sentence: ['Activation', 'effector', 'gene', 'arc', 'methamphetamine']\n"
     ]
    }
   ],
   "source": [
    "# Removing Stopwords and Punctuations to Reduce Workload\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_sent = []\n",
    "\n",
    "for w in tokenized_sent_1:\n",
    "    if (w not in stop_words) & (w.isalpha()):\n",
    "        filtered_sent.append(w)\n",
    "\n",
    "print(\"Tokenized Sentence:\",tokenized_sent_1)\n",
    "print(\"Filterd Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "tokens = []\n",
    "for t in tokenized_word_1:\n",
    "    offset = sent_1.find(t, offset)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    '''\n",
    "    Given a sentence , calls nltk.tokenize to split it in tokens, and adds to each token its start / end offset \n",
    "    in the original sentence .\n",
    "    Input - s: string containing the text for one sentence\n",
    "    Output - Returns a list of tuples (word , offsetFrom , offsetTo )'''\n",
    "\n",
    "    token_list = []\n",
    "    tokens = word_tokenize(s)\n",
    "    \n",
    "    for t in tokens:\n",
    "        if (t in stop_words) & (not t.isalpha()):\n",
    "            continue\n",
    "        else:\n",
    "            offsetFrom = s.find(t)\n",
    "            offsetTo = offsetFrom + len(t) - 1\n",
    "            token_list.append((t, offsetFrom, offsetTo))\n",
    "            \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Activation', 0, 9),\n",
       " ('of', 11, 12),\n",
       " ('an', 14, 15),\n",
       " ('effector', 17, 24),\n",
       " ('immediate-early', 26, 40),\n",
       " ('gene', 42, 45),\n",
       " ('arc', 47, 49),\n",
       " ('by', 51, 52),\n",
       " ('methamphetamine', 54, 68)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Activation of an effector immediate-early gene arc by methamphetamine\"\n",
    "tokenize(sent_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine (by hand or collecting simple statistics) the train dataset and try to infer general rules that are right in most cases, even if they seldom apply (high precision, low recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coumaphos\n",
      "drug_n\n",
      "coumaphos\n",
      "drug_n\n",
      "bishydroxycoumarin\n",
      "drug\n",
      "trichlorfon\n",
      "drug_n\n",
      "phenobarbital sodium\n",
      "drug\n",
      "coumaphos\n",
      "drug_n\n",
      "bishydroxycoumarin\n",
      "drug\n",
      "anticoagulant\n",
      "group\n",
      "trichlorfon\n",
      "drug_n\n",
      "organophosphorous compound\n",
      "group\n",
      "phenobarbital sodium\n",
      "drug\n",
      "coumaphos\n",
      "drug_n\n",
      "bishydroxy-coumarin\n",
      "drug\n",
      "trichlorfon\n",
      "drug_n\n",
      "coumaphos\n",
      "drug_n\n",
      "phenobarbital sodium\n",
      "drug\n",
      "coumaphos\n",
      "drug_n\n",
      "coumaphos\n",
      "drug_n\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(\"/Users/mponsclo/Documents/DataScience/ALHT_Project/train/46730.xml\")\n",
    "root = tree.getroot()\n",
    "    \n",
    "# find all \"item\" objects and print their \"name\" attribute\n",
    "for elem in root:\n",
    "    for subelem in elem.findall('entity'):\n",
    "    \n",
    "        # if we don't need to know the name of the attribute(s), get the dict\n",
    "        #print(subelem.attrib)      \n",
    "    \n",
    "        # if we know the name of the attribute, access it directly\n",
    "        print(subelem.get('text'))\n",
    "        print(subelem.get('type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word.isupper(): return True, \"brand\"\n",
    "elif word[-5:] in ['azole ', 'idine ', 'amine ', 'mycin ']:\n",
    "    return True, \"drug\"\n",
    "else: return False, \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(s):\n",
    "    ''' Given a tokenized sentence , identify which tokens (or groups of consecutive tokens) are drugs\n",
    "    Input - s: A tokenized sentence ( list of triples (word , offsetFrom , offsetTo ) )\n",
    "    Output - A list of entities. Each entity is a dictionary with the keys 'name ', ' offset ', and 'type '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(datadir, outfile):\n",
    "    '''datadir - directory with XML files\n",
    "       outfile - name for the outputfile'''\n",
    "\n",
    "    # process each file in directory\n",
    "    for f in listdir(datadir):\n",
    "        # parse XML file, obtaining a DOM tree\n",
    "        tree = parse(datadir + \"/\" + f)\n",
    "        # process each senetence in the file\n",
    "        sentences = tree.getElementsByTagName(\"sentence\")\n",
    "        for s in sentences:\n",
    "                sid = s.attributes[\"id\"].value        # get sentence id\n",
    "                stext = s.attributes[\"text\"].value    # get sentence text\n",
    "                # tokenize text\n",
    "                tokens = tokenize(stext)\n",
    "                # extract entities from tokenized sentence text\n",
    "                entities = extract_entities(tokens)\n",
    "\n",
    "                # print sentence entities in format requested for evaluation\n",
    "                for e in entities:\n",
    "                    print(sid + \"|\" + e[\"offset\"] + \"|\" + e[\"text\"] + \"|\" e[\"type\"], file = outf)\n",
    "        # print performance score\n",
    "        evaluator.evaluate(\"NER\", datadir, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiriQ3",
   "language": "python",
   "name": "miriq3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
