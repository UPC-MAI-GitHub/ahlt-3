{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extractor\n",
    "- Must be an independent program, separate from learner and classifier.\n",
    "- Must get as argument the directory with the XML files to encode. \n",
    "- Must print the feature vectors to `stdout`\n",
    "\n",
    "> DDI-DrugBank.d658.s0 When 0 3 O form=When formlower=when suf3=hen\n",
    "suf4=When isTitle BoS formNext=administered\n",
    "formlowerNext=administered suf3Next=red suf4Next=ered\n",
    "\n",
    "> DDI-DrugBank.d658.s0 administered 5 16 O form=administered\n",
    "formlower=administered suf3=red suf4=ered formPrev=When\n",
    "formlowerPrev=when suf3Prev=hen suf4Prev=When isTitlePrev\n",
    "formNext=concurrently formlowerNext=concurrently suf3Next=tly\n",
    "suf4Next=ntly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('universal_tagset')\n",
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't remove stopwords and punctuations here since they may be used for feature extracting\n",
    "def tokenize(s):\n",
    "    '''\n",
    "    Given a sentence , calls nltk.tokenize to split it in tokens, and adds to each token its start / end offset \n",
    "    in the original sentence .\n",
    "    Input - s: string containing the text for one sentence\n",
    "    Output - Returns a list of tuples (word , offsetFrom , offsetTo )\n",
    "    '''\n",
    "\n",
    "    token_list = []\n",
    "    tokens = word_tokenize(s)\n",
    "    \n",
    "    for t in tokens:\n",
    "            offsetFrom = s.find(t)\n",
    "            offsetTo = offsetFrom + len(t) - 1\n",
    "            token_list.append((t, offsetFrom, offsetTo))\n",
    "            \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features\n",
    "Given a tokenized sentence, return a feature vector fo each token.\n",
    "Example :\n",
    "> `extract_features` ([(\" Ascorbic \" ,0 ,7) , (\" acid \" ,9 ,12) , (\" ,\" ,13 ,13) ,\n",
    "(\" aspirin \" ,15 ,21) , (\" ,\" ,22 ,22) , (\" and \" ,24 ,26) , (\" the \" ,28 ,30) ,\n",
    "(\" common \" ,32 ,37) , (\" cold \" ,39 ,42) , (\".\" ,43 ,43) ])\n",
    "[ [ \" form = Ascorbic \", \" suf4 = rbic \", \" next = acid \", \" prev = _BoS_ \", \"\n",
    "capitalized \" ],\n",
    "\n",
    "> Output -> [ \" form = acid \", \" suf4 = acid \", \" next =,\", \" prev = Ascorbic \" ],\n",
    "[ \" form =,\", \" suf4 =,\", \" next = aspirin \", \" prev = acid \", \" punct \" ],\n",
    "[ \" form = aspirin \", \" suf4 = irin \", \" next =,\", \" prev =,\" ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = 'Ascorbic acid, aspirin, and the common cold'\n",
    "sent_2 = 'Phenothiazines and 3-butyrophenones may reduce or reverse the depressor effect of epinephrine'\n",
    "\n",
    "tokenized_sent_1 = tokenize(sent_1)\n",
    "tokenized_sent_2 = tokenize(sent_2)\n",
    "\n",
    "def has_numbers(word):\n",
    "    return any(l.isdigit() for l in word)\n",
    "def num_digits(word):\n",
    "    return sum(l.isdigit() for l in word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tag\n",
    "Given a token and a list of ground truth entities in a sentence, decide which is the B-I-O tag for the token.\n",
    "\n",
    "**B-I-O Approach** = Mark each token as **B**egin of a **sub**sequence, **I**nside a subsequence, or **O**utside any subsequence.\n",
    "> `get_tag` ((\" Ascorbic \" ,0 ,7) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> B-drug\n",
    "\n",
    "> `get_tag` ((\" acid \" ,9 ,12) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> I-drug\n",
    "\n",
    "> `get_tag` ((\" common \" ,32 ,37) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> 0\n",
    "\n",
    "> `get_tag` ((\" aspirin \" ,15 ,21) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> B-brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(token, gold):\n",
    "    '''\n",
    "    Input:\n",
    "        token: A token, i.e. one triple (word, offsetFrom, offsetTo)\n",
    "        gold: A list of ground truth entities, i.e. a list of triples (offsetFrom, offsetTo, type)\n",
    "        \n",
    "    Output:\n",
    "        The B-I-O ground truth tag for the given token (\"B-drug\", \"I-drug\", \"B-group\", \"I-group\", \"O\", ...)\n",
    "    '''\n",
    "    (form, start, end) = token\n",
    "    for (offsetFrom, offsetTo, Type) in gold:\n",
    "        if start == offsetFrom and end<=offsetTo:\n",
    "            return \"B-\"+Type # First letter of token equals 0 -> Beginning\n",
    "        elif start > offsetFrom and end <=offsetTo:\n",
    "            return \"I-\"+Type # Word not in the beginning\n",
    "    return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B- drug \n",
      "I- drug \n",
      "O\n",
      "B- brand \n"
     ]
    }
   ],
   "source": [
    "print(get_tag ((\" Ascorbic \" ,0 ,7) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]))\n",
    "print(get_tag ((\" acid \" ,9 ,12) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]))\n",
    "print(get_tag ((\" common \" ,32 ,37) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) )\n",
    "print(get_tag ((\" aspirin \" ,15 ,21) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tokenized_sentence):\n",
    "    '''\n",
    "    Input:\n",
    "        s: A tokenized sentence (list of triples (word, offsetFrom, offsetTo) )\n",
    "        \n",
    "    Output: \n",
    "        A list of feature vectors, one per token.\n",
    "        Features are binary and vectors are in sparse representeation (i.e. only active features are listed)\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for i in range(0, len(tokenized_sentence)):\n",
    "        t = tokenized_sentence[i][0]\n",
    "        punct = [\".\",\",\",\";\",\":\",\"?\",\"!\"]\n",
    "        \n",
    "        # length, number of digits, rules \n",
    "        \n",
    "        tokenFeatures = [\n",
    "            \"form=\" + t,\n",
    "            \"formlower=\" + t.lower(),\n",
    "            \"suf3=\" + t[-3:],\n",
    "            \"suf4=\" + t[-4:],\n",
    "            \"capitalized=%s \" % t.istitle(),\n",
    "            \"uppercase=%s\" % t.isupper(),\n",
    "            \"digit=%s\" % t.isdigit(),\n",
    "            \"hasNumber=%s\" % has_numbers(t),\n",
    "            \"stopword=%s\" % (t in stopwords),\n",
    "            \"punctuation=%s\" % (t in punct),\n",
    "            #\"length=%s\" % len(t),\n",
    "            #\"posTag=%s\" % pos_tag(t, tagset = 'universal')[0][1]\n",
    "            #\"numDigits=%s\" % num_digits(t)\n",
    "        ]  \n",
    "        \n",
    "  \n",
    "        features.append(tokenFeatures)\n",
    "        \n",
    "    for i, current_token in enumerate(features):\n",
    "        # add previous token\n",
    "        if i > 0:\n",
    "            prev_token = features[i-1][0][5:]\n",
    "            current_token.append(\"prev=%s\" % prev_token)\n",
    "            current_token.append(\"suf3Prev = %s\" % prev_token[-3:])\n",
    "            current_token.append(\"suf4Prev = %s\" % prev_token[-4:])\n",
    "            #current_token.append(\"prevIsTitle = %s\" % prev_token.istitle())\n",
    "        else:\n",
    "            current_token.append(\"prev=_BoS_\") #beginning of sentence?\n",
    "            \n",
    "        # add next token\n",
    "        if i < len(features)-1:\n",
    "            next_token = features[i+1][0][5:]\n",
    "            current_token.append(\"next=%s\" % next_token)\n",
    "            current_token.append(\"suf3Next = %s\" % next_token[-3:])\n",
    "            current_token.append(\"suf4Next = %s\" % next_token[-3:])\n",
    "            #current_token.append(\"NextIsTitle = %s\" % next_token.istitle())\n",
    "        else:\n",
    "            current_token.append(\"next=_EoS_\") # end of sentence\n",
    "\n",
    "        # we could also add the suffixes of the previous/next word\n",
    "            \n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['form=Ascorbic', 'formlower=ascorbic', 'suf3=bic', 'suf4=rbic', 'capitalized=True ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=_BoS_', 'next=acid', 'suf3Next = cid', 'suf4Next = cid'], ['form=acid', 'formlower=acid', 'suf3=cid', 'suf4=acid', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=Ascorbic', 'suf3Prev = bic', 'suf4Prev = rbic', 'next=,', 'suf3Next = ,', 'suf4Next = ,'], ['form=,', 'formlower=,', 'suf3=,', 'suf4=,', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=True', 'prev=acid', 'suf3Prev = cid', 'suf4Prev = acid', 'next=aspirin', 'suf3Next = rin', 'suf4Next = rin'], ['form=aspirin', 'formlower=aspirin', 'suf3=rin', 'suf4=irin', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=,', 'suf3Prev = ,', 'suf4Prev = ,', 'next=,', 'suf3Next = ,', 'suf4Next = ,'], ['form=,', 'formlower=,', 'suf3=,', 'suf4=,', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=True', 'prev=aspirin', 'suf3Prev = rin', 'suf4Prev = irin', 'next=and', 'suf3Next = and', 'suf4Next = and'], ['form=and', 'formlower=and', 'suf3=and', 'suf4=and', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=,', 'suf3Prev = ,', 'suf4Prev = ,', 'next=the', 'suf3Next = the', 'suf4Next = the'], ['form=the', 'formlower=the', 'suf3=the', 'suf4=the', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=and', 'suf3Prev = and', 'suf4Prev = and', 'next=common', 'suf3Next = mon', 'suf4Next = mon'], ['form=common', 'formlower=common', 'suf3=mon', 'suf4=mmon', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=the', 'suf3Prev = the', 'suf4Prev = the', 'next=cold', 'suf3Next = old', 'suf4Next = old'], ['form=cold', 'formlower=cold', 'suf3=old', 'suf4=cold', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=common', 'suf3Prev = mon', 'suf4Prev = mmon', 'next=_EoS_']]\n"
     ]
    }
   ],
   "source": [
    "feats_sent_1 = extract_features(tokenized_sent_1)\n",
    "feats_sent_2 = extract_features(tokenized_sent_2)\n",
    "print(feats_sent_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/Users/mponsclo/Downloads/labAHLT/data/train'\n",
    "#datadir = \"../labAHLT/data/train\"\n",
    "\n",
    "def feature_extractor(datadir, resultpath):\n",
    "    result_f = open(resultpath, 'w')\n",
    "    # process each file in directory\n",
    "    for f in listdir(datadir):\n",
    "\n",
    "        # parse XML file, obtaining a DOM tree\n",
    "        tree = parse(datadir + \"/\" + f)\n",
    "\n",
    "        # process each sentence in the file\n",
    "        sentences = tree.getElementsByTagName(\"sentence\")\n",
    "        for s in sentences:\n",
    "            sid = s.attributes[\"id\"].value # get sentence id\n",
    "            stext = s.attributes[\"text\"].value # get sentence text\n",
    "            # load ground truth entities\n",
    "            gold = []\n",
    "            entities = s.getElementsByTagName(\"entity\")\n",
    "            for e in entities:\n",
    "                # for discontinuous entities, we only get the first span\n",
    "                offset = e.attributes[\"charOffset\"].value      # e.g. 24-44\n",
    "                try: # too many values to unpack in some iteration\n",
    "                    (start, end) = offset.split(\":\")[0].split(\"-\") # e.g. start:24, end:44\n",
    "                except:\n",
    "                    pass\n",
    "                gold.append((int(start), int(end), e.attributes[\"type\"].value)) # e.g. [(24, 44, 'drug')] \n",
    "\n",
    "            # tokenize text\n",
    "            tokens = tokenize(stext)\n",
    "\n",
    "            # extract features for each word in the sentence\n",
    "            features = extract_features(tokens)\n",
    "\n",
    "            # print features in format suitable for the learner/classifier\n",
    "            for i in range (0, len(tokens)):\n",
    "                # see if the token is part of an entity, and which part (B/I)\n",
    "                tag = get_tag(tokens[i], gold)\n",
    "                joined_features = \"\\t\".join(features[i])\n",
    "                result_f.write(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\n\".format(sid, tokens[i][0], tokens[i][1], tokens[i][2], tag, joined_features))\n",
    "                #print(sid, tokens[i][0], tokens[i][1], tokens[i][2], tag, \"\\t\".join(features[i]), sep='\\t')\n",
    "        \n",
    "\n",
    "            # black line to separate sentences\n",
    "            #print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: CRF\n",
    "The learner needs only the right class and the features, so you'll need to remove the 4 extra fields _(sent\\_id, token, span\\_start, span\\_end)_ added by the feature extractor, before feeding the vector to the learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindatadir = \"/Users/mponsclo/Downloads/labAHLT/data/train\"\n",
    "testdatadir = \"/Users/mponsclo/Downloads/labAHLT/data/test\"\n",
    "\n",
    "#train_data_filename = \"/Users/mponsclo/Desktop/train_data.txt\"\n",
    "#test_data_filename = \"/Users/mponsclo/Desktop/test_data.txt\"\n",
    "\n",
    "train_data_filename = \"train_data.out\"\n",
    "test_data_filename = \"test_data.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor(traindatadir, train_data_filename)\n",
    "feature_extractor(testdatadir, test_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "def sentence2features(sentence):\n",
    "    classes = []\n",
    "    features = []\n",
    "    tokens = []\n",
    "    for token in sentence:\n",
    "        split_line = token[:-1].split('\\t')\n",
    "        sentence_id = split_line[0]\n",
    "        feature_dict = {\n",
    "            \"form\": split_line[5],\n",
    "            \"formlower\": split_line[6]\n",
    "\n",
    "        }\n",
    "        #TODO read all features from text files\n",
    "        features.append(split_line[5:])\n",
    "        classes.append(split_line[4])\n",
    "        tokens.append((split_line[1], split_line[2], split_line[3]))\n",
    "    return features, classes, tokens\n",
    "\n",
    "def read_feature_file(filepath):\n",
    "    f = open(filepath, 'r')\n",
    "    lines = f.readlines()\n",
    "    features = []\n",
    "    classes = []\n",
    "    metadata=[] # sentence ids, token form, offsets - for later reconstruction\n",
    "    sentences = set()\n",
    "    #print(lines)\n",
    "    sentences = groupby(lines, lambda l: l.split('\\t')[0])\n",
    "    for sid, sentence in sentences:\n",
    "        s_features, s_classes, s_tokens = sentence2features(sentence)\n",
    "        features.append(s_features)\n",
    "        classes.append(s_classes)\n",
    "        metadata.append((sid, s_tokens))\n",
    "           \n",
    "    return metadata, features, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata, X_train, y_train = read_feature_file(train_data_filename)\n",
    "test_metadata, X_test, y_test = read_feature_file(test_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train[:5])\n",
    "#print(y_train[:5])\n",
    "#print(train_metadata[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_result_file = 'conll2002-esp.crfsuite'\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "    \n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "trainer.train(train_result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 50,\n",
       " 'scores': {},\n",
       " 'loss': 14856.484925,\n",
       " 'feature_norm': 77.854707,\n",
       " 'error_norm': 471.42687,\n",
       " 'active_features': 9359,\n",
       " 'linesearch_trials': 1,\n",
       " 'linesearch_step': 1.0,\n",
       " 'time': 0.096}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logparser.last_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 {'num': 50, 'scores': {}, 'loss': 14856.484925, 'feature_norm': 77.854707, 'error_norm': 471.42687, 'active_features': 9359, 'linesearch_trials': 1, 'linesearch_step': 1.0, 'time': 0.096}\n"
     ]
    }
   ],
   "source": [
    "print(len(trainer.logparser.iterations), trainer.logparser.iterations[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: O O O O B-drug O O O O O O O O O O O O O O O\n",
      "Correct  : O O O O B-drug O O O I-drug O O O O O O O O O O O\n",
      "\n",
      "Predicted: O O O O O B-drug O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Correct:   O O O O O B-drug O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "# Predict \n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(train_result_file)\n",
    "\n",
    "predicted = tagger.tag(X_test[0])\n",
    "\n",
    "print(\"Predicted:\", ' '.join(predicted))\n",
    "print(\"Correct  :\", ' '.join(y_test[0]))\n",
    "\n",
    "print()\n",
    "\n",
    "predicted = tagger.tag(X_test[1])\n",
    "\n",
    "print(\"Predicted:\", ' '.join(predicted))\n",
    "print(\"Correct:  \", ' '.join(y_test[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Maximum Entropy\n",
    "`megam`does not expect the extra information in the features file, so:\n",
    "- Remove the first 3 fields _(sent\\_id, span\\_start, span\\_end)_ and the blank lines between spaces.\n",
    "- You can modify the print statement in the feature extractor to directly produce two versions of the feature file, one with the extra information, and one without. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Own choice\n",
    "Adapt the feature file format to the needs of the selected algorithm. Train a classification model for the task of predicting BI-O tags for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: CRF\n",
    "Load the vectors produced by the feature extractor and feed them to the classifier.\n",
    "The classifier needs only the features, so you'll need to remove the other extra fields _(sent\\_id, token, span\\_start, span\\_end)_ added by the feature extractor, before feeding the vector to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. Maximum Entropy\n",
    "Follow examples (and reuse code) for MaxEnt classifiers seen in class to get a B-I-O tag for each token in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Your choice\n",
    "Write the necessary code to call your choice classifier and get a B-I-O tag for each token in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Default output for all options*\n",
    "Given a list of tokens and the B-I-O tag for each token, produce a list of drugs in the format expected by the evaluator. \n",
    "\n",
    "> `output_entities` (\" DDI - DrugBank . d553 .s0\",\n",
    "[(\" Ascorbic \" ,0 ,7) , (\" acid \" ,9 ,12) , (\" ,\" ,13 ,13) ,\n",
    "(\" aspirin \" ,15 ,21) , (\" ,\" ,22 ,22) , (\" and \" ,24 ,26) ,\n",
    "(\" the \" ,28 ,30) ,(\" common \" ,32 ,37) , (\" cold \" ,39 ,42) ],\n",
    "[\"B- drug \", \"I- drug \", \"O\", \"B- brand \", \"O\", \"O\", \"O\",\n",
    "\"O\", \"O \"])\n",
    "\n",
    "DDI - DrugBank . d553 .s0 |0 -12| Ascorbic acid | drug\n",
    "\n",
    "DDI - DrugBank . d553 .s0 |15 -21| aspirin | brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_entities(sid, tokens, tags):\n",
    "    '''\n",
    "    Input:\n",
    "        sid: sentence identifier (required by the evaluator output format)\n",
    "        tokens: List of tokens in the sentence, i.e. list of tuples (word, offsetFrom, offsetTo)\n",
    "        tags: List of B-I-O tags for each token\n",
    "        \n",
    "    Output:\n",
    "        Prints to stdout the entities in the right format: one line per entity, fields separated by '|', \n",
    "        field order: id, offset, name, type.\n",
    "    '''\n",
    "    \n",
    "    print(sid + \" | \" + e[\"offset\"] + \" | \" + e[\"name\"] + \" |\" + e[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating Results\n",
    "- Repeat training: evaluation cycle on devel dataset to find out which is the best parametrization for the used algorithm.\n",
    "- Repeat feature extraction: training-evaluation cycle on devel dataset to find out which features are useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiriQ3",
   "language": "python",
   "name": "miriq3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
