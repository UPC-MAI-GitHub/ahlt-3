{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extractor\n",
    "- Must be an independent program, separate from learner and classifier.\n",
    "- Must get as argument the directory with the XML files to encode. \n",
    "- Must print the feature vectors to `stdout`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    '''\n",
    "    Given a sentence , calls nltk.tokenize to split it in tokens, and adds to each token its start / end offset \n",
    "    in the original sentence .\n",
    "    Input - s: string containing the text for one sentence\n",
    "    Output - Returns a list of tuples (word , offsetFrom , offsetTo )\n",
    "    '''\n",
    "\n",
    "    token_list = []\n",
    "    tokens = word_tokenize(s)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    for t in tokens:\n",
    "        if (t in stop_words) | (not t.isalpha()):  # reduce workload\n",
    "            continue\n",
    "        else:\n",
    "            offsetFrom = s.find(t)\n",
    "            offsetTo = offsetFrom + len(t) - 1\n",
    "            token_list.append((t, offsetFrom, offsetTo))\n",
    "            \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features\n",
    "Given a tokenized sentence, return a feature vector fo each token.\n",
    "Example :\n",
    "> `extract_features` ([(\" Ascorbic \" ,0 ,7) , (\" acid \" ,9 ,12) , (\" ,\" ,13 ,13) ,\n",
    "(\" aspirin \" ,15 ,21) , (\" ,\" ,22 ,22) , (\" and \" ,24 ,26) , (\" the \" ,28 ,30) ,\n",
    "(\" common \" ,32 ,37) , (\" cold \" ,39 ,42) , (\".\" ,43 ,43) ])\n",
    "[ [ \" form = Ascorbic \", \" suf4 = rbic \", \" next = acid \", \" prev = _BoS_ \", \"\n",
    "capitalized \" ],\n",
    "[ \" form = acid \", \" suf4 = acid \", \" next =,\", \" prev = Ascorbic \" ],\n",
    "[ \" form =,\", \" suf4 =,\", \" next = aspirin \", \" prev = acid \", \" punct \" ],\n",
    "[ \" form = aspirin \", \" suf4 = irin \", \" next =,\", \" prev =,\" ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(s):\n",
    "    '''\n",
    "    Input:\n",
    "        s: A tokenized sentence (list of triples (word, offsetFrom, offsetTo) )\n",
    "        \n",
    "    Output: \n",
    "        A list of feature vectors, one per token.\n",
    "        Features are binary and vectors are in sparse representeation (i.e. only active features are listed)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tag\n",
    "Given a token and a list of ground truth entities in a sentence, decide which is the B-I-O tag for the token.\n",
    "\n",
    "**B-I-O Approach** = Mark each token as **B**egin of a sequence, **I**nside a sequence, or **O**utside any sequence.\n",
    "> `get_tag` ((\" Ascorbic \" ,0 ,7) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> B- drug\n",
    "\n",
    "> `get_tag` ((\" acid \" ,9 ,12) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> I- drug\n",
    "\n",
    "> `get_tag` ((\" common \" ,32 ,37) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> 0\n",
    "\n",
    "> `get_tag` ((\" aspirin \" ,15 ,21) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> B- brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(token, gold):\n",
    "    '''\n",
    "    Input:\n",
    "        token: A token, i.e. one triple (word, offsetFrom, offsetTo)\n",
    "        gold: A list of ground truth entities, i.e. a list of triples (offsetFrom, offsetTo, type)\n",
    "        \n",
    "    Output:\n",
    "        The B-I-O ground truth tag for the given token (\"B-drug\", \"I-drug\", \"B-group\", \"I-group\", \"O\", ...)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process each file in directory\n",
    "for f in listdir(datadir):\n",
    "    \n",
    "    # parse XML file, obtaining a DOM tree\n",
    "    tree = parse(datadir + \"/\" + f)\n",
    "    \n",
    "    # process each sentence in the file\n",
    "    sentences = tree.getElementsByTagName(\"sentence\")\n",
    "    for s in sentence:\n",
    "        sid = s.attributes[\"id\"].value # get sentence id\n",
    "        stext = s.attributes[\"text\"].value # get sentence text\n",
    "        # load ground truth entities\n",
    "        gold = []\n",
    "        entities = s.getElementsByTagNameByTagName(\"entity\")\n",
    "        for e in entities:\n",
    "            # for discontinuous entities, we only get the first span\n",
    "            offset = e.attributes[\"charOffset\"].value\n",
    "            (start, end) = offset.split(\":\")[0].split(\"-\")\n",
    "            gold.append((int(start), int(end), e.attributes[\"type\"].value))\n",
    "            \n",
    "        # tokenize text\n",
    "        tokens = tokenize(stext)\n",
    "        \n",
    "        # extract features for each word in the sentence\n",
    "        features = extract_features(tokens)\n",
    "        \n",
    "        # print features in format suitable for the learner/classifier\n",
    "        for i in range (0, len(tokens)):\n",
    "            # see if the token is part of an entity, and which part (B/I)\n",
    "            tag = get_tag(tokens[i], gold)\n",
    "            print(sid, tokens[i][0], tokens[i][1], tokens[i][2], tag, \"\\t\".join(features[i]), sep='\\t')\n",
    "            \n",
    "        # black line to separate sentences\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: CRF\n",
    "The learner needs only the right class and the features, so you'll need to remove the 4 extra fields _(sent\\_id, token, span\\_start, span\\_end)_ added by the feature extractor, before feeding the vector to the learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "pip install python-crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Maximum Entropy\n",
    "`megam`does not expect the extra information in the features file, so:\n",
    "- Remove the first 3 fields _(sent\\_id, span\\_start, span\\_end)_ and the blank lines between spaces.\n",
    "- You can modify the print statement in the feature extractor to directly produce two versions of the feature file, one with the extra information, and one without. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Own choice\n",
    "Adapt the feature file format to the needs of the selected algorithm. Train a classification model for the task of predicting BI-O tags for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: CRF\n",
    "Load the vectors produced by the feature extractor and feed them to the classifier.\n",
    "The classifier needs only the features, so you'll need to remove the other extra fields _(sent\\_id, token, span\\_start, span\\_end)_ added by the feature extractor, before feeding the vector to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. Maximum Entropy\n",
    "Follow examples (and reuse code) for MaxEnt classifiers seen in class to get a B-I-O tag for each token in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Your choice\n",
    "Write the necessary code to call your choice classifier and get a B-I-O tag for each token in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Default output for all options*\n",
    "Given a list of tokens and the B-I-O tag for each token, produce a list of drugs in the format expected by the evaluator. \n",
    "\n",
    "> `output_entities` (\" DDI - DrugBank . d553 .s0\",\n",
    "[(\" Ascorbic \" ,0 ,7) , (\" acid \" ,9 ,12) , (\" ,\" ,13 ,13) ,\n",
    "(\" aspirin \" ,15 ,21) , (\" ,\" ,22 ,22) , (\" and \" ,24 ,26) ,\n",
    "(\" the \" ,28 ,30) ,(\" common \" ,32 ,37) , (\" cold \" ,39 ,42) ],\n",
    "[\"B- drug \", \"I- drug \", \"O\", \"B- brand \", \"O\", \"O\", \"O\",\n",
    "\"O\", \"O \"])\n",
    "\n",
    "DDI - DrugBank . d553 .s0 |0 -12| Ascorbic acid | drug\n",
    "\n",
    "DDI - DrugBank . d553 .s0 |15 -21| aspirin | brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_entities(sid, tokens, tags):\n",
    "    '''\n",
    "    Input:\n",
    "        sid: sentence identifier (required by the evaluator output format)\n",
    "        tokens: List of tokens in the sentence, i.e. list of tuples (word, offsetFrom, offsetTo)\n",
    "        tags: List of B-I-O tags for each token\n",
    "        \n",
    "    Output:\n",
    "        Prints to stdout the entities in the right format: one line per entity, fields separated by '|', \n",
    "        field order: id, offset, name, type.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating Results\n",
    "- Repeat training: evaluation cycle on devel dataset to find out which is the best parametrization for the used algorithm.\n",
    "- Repeat feature extraction: training-evaluation cycle on devel dataset to find out which features are useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiriQ3",
   "language": "python",
   "name": "miriq3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
