{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extractor\n",
    "- Must be an independent program, separate from learner and classifier.\n",
    "- Must get as argument the directory with the XML files to encode. \n",
    "- Must print the feature vectors to `stdout`\n",
    "\n",
    "> DDI-DrugBank.d658.s0 When 0 3 O form=When formlower=when suf3=hen\n",
    "suf4=When isTitle BoS formNext=administered\n",
    "formlowerNext=administered suf3Next=red suf4Next=ered\n",
    "\n",
    "> DDI-DrugBank.d658.s0 administered 5 16 O form=administered\n",
    "formlower=administered suf3=red suf4=ered formPrev=When\n",
    "formlowerPrev=when suf3Prev=hen suf4Prev=When isTitlePrev\n",
    "formNext=concurrently formlowerNext=concurrently suf3Next=tly\n",
    "suf4Next=ntly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('universal_tagset')\n",
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't remove stopwords and punctuations here since they may be used for feature extracting\n",
    "def tokenize(s):\n",
    "    '''\n",
    "    Given a sentence , calls nltk.tokenize to split it in tokens, and adds to each token its start / end offset \n",
    "    in the original sentence .\n",
    "    Input - s: string containing the text for one sentence\n",
    "    Output - Returns a list of tuples (word , offsetFrom , offsetTo )\n",
    "    '''\n",
    "\n",
    "    token_list = []\n",
    "    tokens = word_tokenize(s)\n",
    "    \n",
    "    for t in tokens:\n",
    "            offsetFrom = s.find(t)\n",
    "            offsetTo = offsetFrom + len(t) - 1\n",
    "            token_list.append((t, offsetFrom, offsetTo))\n",
    "            \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features\n",
    "Given a tokenized sentence, return a feature vector fo each token.\n",
    "Example :\n",
    "> `extract_features` ([(\" Ascorbic \" ,0 ,7) , (\" acid \" ,9 ,12) , (\" ,\" ,13 ,13) ,\n",
    "(\" aspirin \" ,15 ,21) , (\" ,\" ,22 ,22) , (\" and \" ,24 ,26) , (\" the \" ,28 ,30) ,\n",
    "(\" common \" ,32 ,37) , (\" cold \" ,39 ,42) , (\".\" ,43 ,43) ])\n",
    "[ [ \" form = Ascorbic \", \" suf4 = rbic \", \" next = acid \", \" prev = _BoS_ \", \"\n",
    "capitalized \" ],\n",
    "\n",
    "> Output -> [ \" form = acid \", \" suf4 = acid \", \" next =,\", \" prev = Ascorbic \" ],\n",
    "[ \" form =,\", \" suf4 =,\", \" next = aspirin \", \" prev = acid \", \" punct \" ],\n",
    "[ \" form = aspirin \", \" suf4 = irin \", \" next =,\", \" prev =,\" ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = 'Ascorbic acid, aspirin, and the common cold'\n",
    "sent_2 = 'Phenothiazines and 3-butyrophenones may reduce or reverse the depressor effect of epinephrine'\n",
    "\n",
    "tokenized_sent_1 = tokenize(sent_1)\n",
    "tokenized_sent_2 = tokenize(sent_2)\n",
    "\n",
    "def has_numbers(word):\n",
    "    return any(l.isdigit() for l in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example extraction\n",
    "# https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb\n",
    "\n",
    "def extract_features(tokenized_sentence):\n",
    "    '''\n",
    "    Input:\n",
    "        s: A tokenized sentence (list of triples (word, offsetFrom, offsetTo) )\n",
    "        \n",
    "    Output: \n",
    "        A list of feature vectors, one per token.\n",
    "        Features are binary and vectors are in sparse representeation (i.e. only active features are listed)\n",
    "    '''\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(tokenized_sentence)):\n",
    "        t = tokenized_sentence[i][0]\n",
    "        tokenFeatures = []\n",
    "        # length, number of digits, rules \n",
    "        \n",
    "        tokenFeatures.append(\"form = \" + t)\n",
    "        tokenFeatures.append(\"formlower = \" + t.lower())\n",
    "        tokenFeatures.append(\"suf3 = \" + t[-3:])\n",
    "        tokenFeatures.append(\"suf4 = \" + t[-4:])\n",
    "        #tokenFeatures.append(\"PoStag = \" + pos_tag(t,tagset = 'universal')[0][1])\n",
    "        \n",
    "        if (t.istitle()): tokenFeatures.append(\"capitalized\")\n",
    "        if (t.isupper()): tokenFeatures.append(\"uppercase\")\n",
    "        if (t.isdigit()): tokenFeatures.append(\"digit\")\n",
    "        if (has_numbers(t)): tokenFeatures.append(\"containsNumber\")\n",
    "        if (t in stopwords): tokenFeatures.append(\"stopword\")\n",
    "        if (t in [\".\",\",\",\";\",\":\",\"?\",\"!\"]): tokenFeatures.append(\"punctuation\") # necessary having PoS Tag?\n",
    "            \n",
    "        \n",
    "        if i > 0: # offsetFrom > 0\n",
    "            tPrev = tokenized_sentence[i-1][0]\n",
    "            tokenFeatures.append(\"fromPrev = \" + tPrev)\n",
    "        else:\n",
    "            tokenFeatures.append(\"BoS\")\n",
    "            \n",
    "        if i < len(tokenized_sentence)-1:\n",
    "            tNext = tokenized_sentence[i+1][0]\n",
    "            tokenFeatures.append(\"formNext = \" + tNext)\n",
    "        else:\n",
    "            tokenFeatures.append(\"EoS\")\n",
    "            \n",
    "        \n",
    "        result.append(tokenFeatures)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['form = Ascorbic', 'formlower = ascorbic', 'suf3 = bic', 'suf4 = rbic', 'capitalized', 'BoS', 'formNext = acid'], ['form = acid', 'formlower = acid', 'suf3 = cid', 'suf4 = acid', 'fromPrev = Ascorbic', 'formNext = ,'], ['form = ,', 'formlower = ,', 'suf3 = ,', 'suf4 = ,', 'punctuation', 'fromPrev = acid', 'formNext = aspirin'], ['form = aspirin', 'formlower = aspirin', 'suf3 = rin', 'suf4 = irin', 'fromPrev = ,', 'formNext = ,'], ['form = ,', 'formlower = ,', 'suf3 = ,', 'suf4 = ,', 'punctuation', 'fromPrev = aspirin', 'formNext = and'], ['form = and', 'formlower = and', 'suf3 = and', 'suf4 = and', 'stopword', 'fromPrev = ,', 'formNext = the'], ['form = the', 'formlower = the', 'suf3 = the', 'suf4 = the', 'stopword', 'fromPrev = and', 'formNext = common'], ['form = common', 'formlower = common', 'suf3 = mon', 'suf4 = mmon', 'fromPrev = the', 'formNext = cold'], ['form = cold', 'formlower = cold', 'suf3 = old', 'suf4 = cold', 'fromPrev = common', 'EoS']]\n",
      "\n",
      "[['form = Phenothiazines', 'formlower = phenothiazines', 'suf3 = nes', 'suf4 = ines', 'capitalized', 'BoS', 'formNext = and'], ['form = and', 'formlower = and', 'suf3 = and', 'suf4 = and', 'stopword', 'fromPrev = Phenothiazines', 'formNext = 3-butyrophenones'], ['form = 3-butyrophenones', 'formlower = 3-butyrophenones', 'suf3 = nes', 'suf4 = ones', 'containsNumber', 'fromPrev = and', 'formNext = may'], ['form = may', 'formlower = may', 'suf3 = may', 'suf4 = may', 'fromPrev = 3-butyrophenones', 'formNext = reduce'], ['form = reduce', 'formlower = reduce', 'suf3 = uce', 'suf4 = duce', 'fromPrev = may', 'formNext = or'], ['form = or', 'formlower = or', 'suf3 = or', 'suf4 = or', 'stopword', 'fromPrev = reduce', 'formNext = reverse'], ['form = reverse', 'formlower = reverse', 'suf3 = rse', 'suf4 = erse', 'fromPrev = or', 'formNext = the'], ['form = the', 'formlower = the', 'suf3 = the', 'suf4 = the', 'stopword', 'fromPrev = reverse', 'formNext = depressor'], ['form = depressor', 'formlower = depressor', 'suf3 = sor', 'suf4 = ssor', 'fromPrev = the', 'formNext = effect'], ['form = effect', 'formlower = effect', 'suf3 = ect', 'suf4 = fect', 'fromPrev = depressor', 'formNext = of'], ['form = of', 'formlower = of', 'suf3 = of', 'suf4 = of', 'stopword', 'fromPrev = effect', 'formNext = epinephrine'], ['form = epinephrine', 'formlower = epinephrine', 'suf3 = ine', 'suf4 = rine', 'fromPrev = of', 'EoS']]\n"
     ]
    }
   ],
   "source": [
    "feats_sent_1 = extract_features(tokenized_sent_1)\n",
    "feats_sent_2 = extract_features(tokenized_sent_2)\n",
    "print(feats_sent_1)\n",
    "print()\n",
    "print(feats_sent_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tag\n",
    "Given a token and a list of ground truth entities in a sentence, decide which is the B-I-O tag for the token.\n",
    "\n",
    "**B-I-O Approach** = Mark each token as **B**egin of a **sub**sequence, **I**nside a subsequence, or **O**utside any subsequence.\n",
    "> `get_tag` ((\" Ascorbic \" ,0 ,7) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> B-drug\n",
    "\n",
    "> `get_tag` ((\" acid \" ,9 ,12) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> I-drug\n",
    "\n",
    "> `get_tag` ((\" common \" ,32 ,37) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> 0\n",
    "\n",
    "> `get_tag` ((\" aspirin \" ,15 ,21) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> B-brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(token, gold):\n",
    "    '''\n",
    "    Input:\n",
    "        token: A token, i.e. one triple (word, offsetFrom, offsetTo)\n",
    "        gold: A list of ground truth entities, i.e. a list of triples (offsetFrom, offsetTo, type)\n",
    "        \n",
    "    Output:\n",
    "        The B-I-O ground truth tag for the given token (\"B-drug\", \"I-drug\", \"B-group\", \"I-group\", \"O\", ...)\n",
    "    '''\n",
    "    (form, start, end) = token\n",
    "    for (offsetFrom, offsetTo, Type) in gold:\n",
    "        if start == offsetFrom and end<=offsetTo: return \"B-\"+Type # First letter of token equals 0 -> Beginning\n",
    "        elif start >= offsetFrom and end <=offsetTo: return \"I-\"+Type # Word not in the beginning\n",
    "        else: return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HOW TO KNOW THAT ASPIRIN IS IN THE BEGINNING OF A SUBSEQUENCE? (AFTER COMMA)\n",
    "# sent_1 = 'Ascorbic acid, aspirin, and the common cold' \n",
    "sent_1.find(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I- drug \n",
      "B- drug \n",
      "O\n"
     ]
    }
   ],
   "source": [
    "print(get_tag((\" acid \",9 ,12) , [(0 , 12, \" drug \") , (15 , 21, \" brand \")]))\n",
    "print(get_tag((\" Ascorbic \" ,0 ,7) , [(0 , 12, \" drug \") , (15 , 21, \" brand \")]))\n",
    "print(get_tag((\" common \" ,32 ,37) , [(0 , 12, \" drug \") , (15 , 21, \" brand \")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/Users/mponsclo/Downloads/labAHLT/data/train'\n",
    "\n",
    "# process each file in directory\n",
    "for f in listdir(datadir):\n",
    "    \n",
    "    # parse XML file, obtaining a DOM tree\n",
    "    tree = parse(datadir + \"/\" + f)\n",
    "    \n",
    "    # process each sentence in the file\n",
    "    sentences = tree.getElementsByTagName(\"sentence\")\n",
    "    for s in sentences:\n",
    "        sid = s.attributes[\"id\"].value # get sentence id\n",
    "        stext = s.attributes[\"text\"].value # get sentence text\n",
    "        # load ground truth entities\n",
    "        gold = []\n",
    "        entities = s.getElementsByTagName(\"entity\")\n",
    "        for e in entities:\n",
    "            # for discontinuous entities, we only get the first span\n",
    "            offset = e.attributes[\"charOffset\"].value      # e.g. 24-44\n",
    "            try: # too many values to unpack in some iteration\n",
    "                (start, end) = offset.split(\":\")[0].split(\"-\") # e.g. start:24, end:44\n",
    "            except:\n",
    "                pass\n",
    "            gold.append((int(start), int(end), e.attributes[\"type\"].value)) # e.g. [(24, 44, 'drug')] \n",
    "            \n",
    "        # tokenize text\n",
    "        tokens = tokenize(stext)\n",
    "        \n",
    "        # extract features for each word in the sentence\n",
    "        features = extract_features(tokens)\n",
    "        \n",
    "        # print features in format suitable for the learner/classifier\n",
    "        for i in range (0, len(tokens)):\n",
    "            # see if the token is part of an entity, and which part (B/I)\n",
    "            tag = get_tag(tokens[i], gold)\n",
    "            print(sid, tokens[i][0], tokens[i][1], tokens[i][2], tag, \"\\t\".join(features[i]), sep='\\t')\n",
    "            \n",
    "        # black line to separate sentences\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: CRF\n",
    "The learner needs only the right class and the features, so you'll need to remove the 4 extra fields _(sent\\_id, token, span\\_start, span\\_end)_ added by the feature extractor, before feeding the vector to the learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "pip install python-crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Maximum Entropy\n",
    "`megam`does not expect the extra information in the features file, so:\n",
    "- Remove the first 3 fields _(sent\\_id, span\\_start, span\\_end)_ and the blank lines between spaces.\n",
    "- You can modify the print statement in the feature extractor to directly produce two versions of the feature file, one with the extra information, and one without. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Own choice\n",
    "Adapt the feature file format to the needs of the selected algorithm. Train a classification model for the task of predicting BI-O tags for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: CRF\n",
    "Load the vectors produced by the feature extractor and feed them to the classifier.\n",
    "The classifier needs only the features, so you'll need to remove the other extra fields _(sent\\_id, token, span\\_start, span\\_end)_ added by the feature extractor, before feeding the vector to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. Maximum Entropy\n",
    "Follow examples (and reuse code) for MaxEnt classifiers seen in class to get a B-I-O tag for each token in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Your choice\n",
    "Write the necessary code to call your choice classifier and get a B-I-O tag for each token in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Default output for all options*\n",
    "Given a list of tokens and the B-I-O tag for each token, produce a list of drugs in the format expected by the evaluator. \n",
    "\n",
    "> `output_entities` (\" DDI - DrugBank . d553 .s0\",\n",
    "[(\" Ascorbic \" ,0 ,7) , (\" acid \" ,9 ,12) , (\" ,\" ,13 ,13) ,\n",
    "(\" aspirin \" ,15 ,21) , (\" ,\" ,22 ,22) , (\" and \" ,24 ,26) ,\n",
    "(\" the \" ,28 ,30) ,(\" common \" ,32 ,37) , (\" cold \" ,39 ,42) ],\n",
    "[\"B- drug \", \"I- drug \", \"O\", \"B- brand \", \"O\", \"O\", \"O\",\n",
    "\"O\", \"O \"])\n",
    "\n",
    "DDI - DrugBank . d553 .s0 |0 -12| Ascorbic acid | drug\n",
    "\n",
    "DDI - DrugBank . d553 .s0 |15 -21| aspirin | brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_entities(sid, tokens, tags):\n",
    "    '''\n",
    "    Input:\n",
    "        sid: sentence identifier (required by the evaluator output format)\n",
    "        tokens: List of tokens in the sentence, i.e. list of tuples (word, offsetFrom, offsetTo)\n",
    "        tags: List of B-I-O tags for each token\n",
    "        \n",
    "    Output:\n",
    "        Prints to stdout the entities in the right format: one line per entity, fields separated by '|', \n",
    "        field order: id, offset, name, type.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating Results\n",
    "- Repeat training: evaluation cycle on devel dataset to find out which is the best parametrization for the used algorithm.\n",
    "- Repeat feature extraction: training-evaluation cycle on devel dataset to find out which features are useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiriQ3",
   "language": "python",
   "name": "miriq3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
