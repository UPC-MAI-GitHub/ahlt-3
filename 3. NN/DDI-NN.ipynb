{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network DDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "import pickle\n",
    "from xml.dom.minidom import parse\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import *\n",
    "#from keras_contrib.layers import CRF\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "from evaluator import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_data()`- Use XML parsing and tokenization functions from previous exercises. Adding a PoS tagger or lemmatizer may be useful. \\\n",
    "Masking the target drugs as e.g. `<DRUG1>`, `<DRUG2>`, and the rest as `<DRUG_OTHER>` will help the algorithm generalize and avoid it focusing in the drug names, which are not relevant for the DDI task (and also make it easier for it to spot the target entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    '''\n",
    "    Task: Load XML files in given directory, tokenize each sentence, and extract\n",
    "    learning examples (tokenized sentence + entity pair).\n",
    "    \n",
    "    Input: \n",
    "        datadir: A directory containing XML files\n",
    "        \n",
    "    Output: A list of classification cases. Each case is a list containing sentenceid, entity1_id, entity2_id,\n",
    "            ground truth relation label, and a list of sentence tokens (each token containing any needed information:\n",
    "            word, lemma, PoS, offsets, etc.\n",
    "    '''\n",
    "    dataset = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for f in listdir(datadir):\n",
    "        \n",
    "        tree = parse(datadir + \"/\" + f)\n",
    "        sentences = tree.getElementsByTagName(\"sentence\")\n",
    "\n",
    "        for s in sentences: \n",
    "            sid = s.attributes[\"id\"].value   \n",
    "            stext = s.attributes[\"text\"].value\n",
    "            stext = stext.replace(\"-\",\" \")\n",
    "            \n",
    "            ents = {}\n",
    "            entities = s.getElementsByTagName(\"entity\")\n",
    "            for e in entities:\n",
    "                e_id = e.attributes[\"id\"].value\n",
    "                offset = e.attributes[\"charOffset\"].value\n",
    "                name = e.attributes[\"text\"].value\n",
    "                ents[e_id] = name\n",
    "\n",
    "            pairs = s.getElementsByTagName(\"pair\")\n",
    "            for p in pairs:\n",
    "                pair_sent = stext\n",
    "                e1_id = p.attributes['e1'].value\n",
    "                e2_id = p.attributes['e2'].value\n",
    "                if p.attributes['ddi'].value == 'true':\n",
    "                    ddi = p.attributes['type'].value\n",
    "                else:\n",
    "                    ddi = 'null'\n",
    "                \n",
    "                aux = [sid, e1_id, e2_id, ddi]\n",
    "                for key, item in ents.items():\n",
    "                    if key == e1_id:\n",
    "                        drug_1 = item\n",
    "                        pair_sent = pair_sent.replace(drug_1, 'DRUG1')\n",
    "                    elif key == e2_id:\n",
    "                        drug_2 = item\n",
    "                        pair_sent = pair_sent.replace(drug_2, 'DRUG2')\n",
    "                    else:\n",
    "                        other_drug = item\n",
    "                        pair_sent = pair_sent.replace(other_drug, 'DRUGOTHER')\n",
    "        \n",
    "                punct = [\".\",\",\",\";\",\":\",\"?\",\"!\", \"'\"] # TODO: Change all punctuations to <PUNCT> and stopwords to <SW>\n",
    "                pair_sent = pair_sent.replace(\"(\", \"\") # Remove Parenthesis\n",
    "                pair_sent = pair_sent.replace(\")\", \"\") # Remove Parenthesis\n",
    "                tokens = word_tokenize(pair_sent)\n",
    "                aux2 = []\n",
    "                for t in tokens:\n",
    "                    if t in punct:\n",
    "                        tk = ('<PUNCT>', '<PUNCT>')\n",
    "                    elif t in stopwords:\n",
    "                        tk = ('<SW>', '<SW>')\n",
    "                    else:\n",
    "                        t_l = t.lower()\n",
    "                        lemma = wordnet_lemmatizer.lemmatize(t_l)\n",
    "                        tk = (t, lemma) # TODO: Add PoS Tag and maybe use stemmer instead of lemmatizer\n",
    "                    aux2.append(tk)\n",
    "                aux.append(aux2)\n",
    "                dataset.append(aux)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DDI-DrugBank.d234.s2', 'DDI-DrugBank.d234.s2.e0', 'DDI-DrugBank.d234.s2.e1', 'effect', [('Particular', 'particular'), ('caution', 'caution'), ('<SW>', '<SW>'), ('necessary', 'necessary'), ('<SW>', '<SW>'), ('using', 'using'), ('DRUG1', 'drug1'), ('<SW>', '<SW>'), ('cases', 'case'), ('<SW>', '<SW>'), ('mixed', 'mixed'), ('drug', 'drug'), ('overdosage', 'overdosage'), ('since', 'since'), ('<SW>', '<SW>'), ('toxic', 'toxic'), ('effects', 'effect'), ('<SW>', '<SW>'), ('<SW>', '<SW>'), ('convulsions', 'convulsion'), ('<SW>', '<SW>'), ('cardiac', 'cardiac'), ('dysrhythmias', 'dysrhythmias'), ('<SW>', '<SW>'), ('<SW>', '<SW>'), ('drugs', 'drug'), ('taken', 'taken'), ('<SW>', '<SW>'), ('overdose', 'overdose'), ('especially', 'especially'), ('DRUG2', 'drug2'), ('may', 'may'), ('emerge', 'emerge'), ('<SW>', '<SW>'), ('<SW>', '<SW>'), ('reversal', 'reversal'), ('<SW>', '<SW>'), ('<SW>', '<SW>'), ('DRUGOTHER', 'drugother'), ('effect', 'effect'), ('<SW>', '<SW>'), ('DRUGOTHER', 'drugother'), ('<PUNCT>', '<PUNCT>')]]\n"
     ]
    }
   ],
   "source": [
    "train_path = '../../labAHLT/data/train'\n",
    "devel_path = '../../labAHLT/data/devel'\n",
    "dataset = load_data(train_path)\n",
    "val_dataset = load_data(devel_path)\n",
    "print(dataset[65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(dataset, max_length):\n",
    "    '''\n",
    "    Task: Create index dictionaries both for input (words) and output (labels) from given dataset\n",
    "    Input: \n",
    "        dataset: dataset produced by load_data.\n",
    "        max_length: maximum length of a sentence (longer sentences will be cut, shorter ones will be padded).\n",
    "        \n",
    "    Output: A dictionary where each key is an index name (e.g. \"words\", \"labels\"), and the value is a \n",
    "            dictionary mapping each word/label to a number. An entry with the value for maxlen is also stored\n",
    "    Example: \n",
    "        >>> create_indx(traindata)\n",
    "        {'words': {'<PAD>':0, '<UNK>':1, '11-day':2, 'murine':3, 'criteria':4,\n",
    "                   'stroke':5, ... ,'levodopa':8511, 'terfenadine': 8512}\n",
    "         'labels': {' null ':0, 'mechanism ':1, 'advise ':2, 'effect ':3, 'int ':4}\n",
    "         'maxlen': 100 }\n",
    "    '''\n",
    "    index_words = {'<PAD>':0, '<UNK>':1}\n",
    "    i = 2\n",
    "    \n",
    "    index_labels = {}\n",
    "    j = 0\n",
    "\n",
    "    # We can create other indexes (lemmas, PoS, etc)  \n",
    "    \n",
    "    for s in dataset:\n",
    "        words = s[4]\n",
    "        label = s[3]\n",
    "        if label not in index_labels:\n",
    "            index_labels[label] = j\n",
    "            j += 1\n",
    "        for tup in words:\n",
    "            w = tup[0]\n",
    "            if w not in index_words:\n",
    "                index_words[w] = i\n",
    "                i += 1\n",
    "    idx = {'words': index_words, 'labels':index_labels, 'maxlen':max_length}     \n",
    "    \n",
    "    return idx\n",
    "    \n",
    "# Add '<PAD>': 0 and '<UNK>':1 codes to 'words' index. The coding of the rest of the words/labels is arbitrary.\n",
    "# You may add to the dictionary entries with indexes for other elements you want to use (lemmas, PoS, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = create_index(dataset, 100)\n",
    "#print(idx[\"words\"])\n",
    "#print(idx[\"labels\"])\n",
    "#print(idx['maxlen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the words in a sentence dataset formed by lists of tokens into lists of indexes\n",
    "          suitable for NN input.\n",
    "    Input: \n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, as well\n",
    "             as the maximum sentence length.\n",
    "             \n",
    "    Output: The dataset encoded as a list of sentence, each of them is a list of word indices.\n",
    "            If the word is not in the index, <UNK> code is used. If the sentence is shorter than\n",
    "            max_len it is padded with <PAD> code.\n",
    "    Example: \n",
    "        >>> encode_words(traindata, idx)\n",
    "            [ [6882 1049 4911 ... 0 0 0 ]\n",
    "            [  2290 7548 8069 ... 0 0 0 ]\n",
    "               ...\n",
    "            [  2002 6582 7518 ... 0 0 0 ] ]\n",
    "    '''\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for s in dataset:\n",
    "        words = s[4]\n",
    "        aux = []\n",
    "        for tup in words:\n",
    "            w = tup[0]\n",
    "            if w in idx['words']:\n",
    "                i = idx['words'][w]\n",
    "            else:\n",
    "                i = idx['words']['<UNK>']\n",
    "            aux.append(i)\n",
    "        seq.append(aux)\n",
    "        \n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    \n",
    "    return seq_padded\n",
    "        \n",
    "        \n",
    "def encode_labels(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the ground truth labels in a dataset of classification examples (sentence + entity pair).\n",
    "    Input:\n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_index, containing word and label indexes, as well as the maximum length.\n",
    "        \n",
    "    Output: The dataset encoded as a list DDI labels, one per classification example. \n",
    "    \n",
    "    Example :\n",
    "     >>> encode_labels ( traindata , idx )\n",
    "     [  [0] [0] [2] ... [4] [0] [0] [1] [0] ]\n",
    "     [  [4] [6] [4] [4] [4] [4] ... [0] [0] ]\n",
    "     [  [4] [4] [8] [4] [6] [4] ... [0] [0] ]\n",
    "         ...\n",
    "     [  [4] [8] [9] [4] [4] [4] ... [0] [0] ]\n",
    "     ] \n",
    "    '''\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    seq = [idx['labels'][s[3]] for s in dataset] # Changed [[idx['labels'][s[3]]] for s in dataset]\n",
    "    Y = [to_categorical(i, num_classes=5) for i in seq]\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    3    4 ...    0    0    0]\n",
      " [  15   16   17 ...    0    0    0]\n",
      " [  15   16   17 ...    0    0    0]\n",
      " ...\n",
      " [1198  996    6 ...    0    0    0]\n",
      " [1198  996    6 ...    0    0    0]\n",
      " [1198  996    6 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "X_train = encode_words(dataset, idx)\n",
    "X_val = encode_words(val_dataset, idx)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23148, 5)\n",
      "(4669, 5)\n"
     ]
    }
   ],
   "source": [
    "Y_train = encode_labels(dataset, idx)\n",
    "Y_val = encode_labels(val_dataset, idx)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GloVe\n",
    "# http://nlp.stanford.edu/data/glove.6B.zip\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index['words']) + 1  \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index['words']:\n",
    "                idx = word_index['words'][word] \n",
    "                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = \"../../labAHLT/data/glove.6B/glove.6B.50d.txt\"\n",
    "embedding_matrix = create_embedding_matrix(glove_path, idx, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx, embedding_dim, embedding_matrix):\n",
    "    '''\n",
    "    Task: Create network for the learner. \n",
    "    Input:\n",
    "        idx: index dictionary with word/labels codes, plus maximum sentence length.\n",
    "    Output: Returns a compiled Keras neural network with the specified layers\n",
    "    '''\n",
    "    \n",
    "    #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # create network layers\n",
    "    inp = Input(shape=(max_len,))\n",
    "    emb_layer = Embedding(input_dim=n_words+1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len)(inp) # TransferLearning -> GloVe, BERT\n",
    "    model = Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.4))(emb_layer)\n",
    "    #model = Conv1D(64, 5, activation='relu')(model)\n",
    "    model = GlobalMaxPooling1D()(model)\n",
    "    model = Dense(24, activation='relu')(model)\n",
    "    out = Dense(n_labels, activation='softmax')(model)\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "    optimiz = Adam(lr=0.01, decay=1e-6)\n",
    "    model.compile(optimizer=optimiz, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx, embedding_dim, embedding_matrix):\n",
    "\n",
    "    #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # create network layers\n",
    "    inp = Input(shape=(max_len,))\n",
    "    emb_layer = Embedding(input_dim=n_words+1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len)(inp) # TransferLearning -> GloVe, BERT\n",
    "    model = Conv1D(128, 5, activation='relu')(emb_layer)\n",
    "    model = GlobalMaxPooling1D()(model)\n",
    "    model = Dense(24, activation='relu')(model)\n",
    "    out = Dense(n_labels, activation='softmax')(model)\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "    optimiz = Adam(lr=0.01, decay=1e-6)\n",
    "    model.compile(optimizer=optimiz, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 100, 16)           79760     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 96, 128)           10368     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 96, 32)            18560     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 24)                792       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 125       \n",
      "=================================================================\n",
      "Total params: 109,605\n",
      "Trainable params: 109,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_network(idx, 16, embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                   batch_size=16, #16/64\n",
    "                   epochs=2,\n",
    "                   verbose=1, \n",
    "                   validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_indexes(model, idx):\n",
    "    '''\n",
    "    Task: Save given model and indexs to disk\n",
    "    Input: \n",
    "        model: Keras model created by _build_network, and trained.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, \n",
    "             as well as the maximum sentence length. \n",
    "        filename: Saves the mode into filename.nn and the indexes into filename.idx\n",
    "    '''\n",
    "    model.save(\"ddi-nn.nn\")\n",
    "    \n",
    "    file = open(\"index.pkl\", \"wb\")\n",
    "    pickle.dump(idx, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ddi-nn.nn/assets\n"
     ]
    }
   ],
   "source": [
    "save_model_and_indexes(model, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Learner()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner(traindir, validationdir):\n",
    "    '''\n",
    "    Learns a NN model using traindir as training data, and validationdir as validation data.\n",
    "    Saves learnt model in a file named modelname\n",
    "    '''\n",
    "    # load train and validation data in a suitable form\n",
    "    traindata = load_data(traindir)\n",
    "    valdata = load_data(validationdir)\n",
    "    \n",
    "    # create indexes from trainindg data \n",
    "    max_len = 100\n",
    "    idx = create_index(traindata, max_len)\n",
    "    embedding_matrix = create_embedding_matrix(glove_path, idx, 16)\n",
    "    \n",
    "    # build network \n",
    "    model = build_network(idx, 16, embedding_matrix)\n",
    "    \n",
    "    # encode datasets\n",
    "    Xtrain = encode_words(traindata, idx)\n",
    "    Ytrain = encode_labels(traindata, idx)\n",
    "    Xval = encode_words(valdata, idx)\n",
    "    Yval = encode_labels(valdata, idx)\n",
    "    \n",
    "    # train model\n",
    "    model.fit(Xtrain, Ytrain,\n",
    "              batch_size=26,\n",
    "              verbose=1,\n",
    "              epochs=6,\n",
    "              validation_data=(Xval, Yval))\n",
    "    \n",
    "    #return history\n",
    "    \n",
    "    #save model and indexs, for later use in prediction\n",
    "    save_model_and_indexes(model, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "891/891 [==============================] - 4s 4ms/step - loss: 0.4973 - accuracy: 0.8542 - val_loss: 0.5092 - val_accuracy: 0.8068\n",
      "Epoch 2/6\n",
      "891/891 [==============================] - 3s 4ms/step - loss: 0.2686 - accuracy: 0.9007 - val_loss: 0.4451 - val_accuracy: 0.8447\n",
      "Epoch 3/6\n",
      "891/891 [==============================] - 3s 4ms/step - loss: 0.1956 - accuracy: 0.9288 - val_loss: 0.5073 - val_accuracy: 0.8124\n",
      "Epoch 4/6\n",
      "891/891 [==============================] - 4s 4ms/step - loss: 0.1752 - accuracy: 0.9375 - val_loss: 0.4546 - val_accuracy: 0.8563\n",
      "Epoch 5/6\n",
      "891/891 [==============================] - 4s 4ms/step - loss: 0.1576 - accuracy: 0.9475 - val_loss: 0.4796 - val_accuracy: 0.8353\n",
      "Epoch 6/6\n",
      "891/891 [==============================] - 4s 4ms/step - loss: 0.1393 - accuracy: 0.9530 - val_loss: 0.5414 - val_accuracy: 0.8188\n",
      "INFO:tensorflow:Assets written to: ddi-nn.nn/assets\n"
     ]
    }
   ],
   "source": [
    "train_path = '../../labAHLT/data/train'\n",
    "devel_path = '../../labAHLT/data/devel'\n",
    "\n",
    "learner(train_path, devel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')\n",
    "# Need more epochs but it seems to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Classifier Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_indexs():\n",
    "    '''\n",
    "    Task: Load model and associate indexs from disk.\n",
    "    Input:\n",
    "        filename: filename to be loaded\n",
    "    Output: Loads a model from filename.nn and its indexes from filename.idx\n",
    "            Returns the loaded model and indexes. \n",
    "    '''\n",
    "    \n",
    "    model = load_model(\"ddi-nn.nn\")\n",
    "    index = open(\"index.pkl\", \"rb\")\n",
    "    idx = pickle.load(index)\n",
    "    \n",
    "    return model, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_interactions(dataset, preds, outf): \n",
    "    '''\n",
    "    Task: Output detected DDIs in the format expected by the evaluator.\n",
    "    Input:\n",
    "        dataset: A dataset produced by load_data\n",
    "        preds: Fore each sentence in dataset, a label for its DDI type (or 'null' if no DDI detected)\n",
    "        \n",
    "    Output: prints the detected interactions to dtdout in the format required by the evaluator.\n",
    "    Example:\n",
    "        >>> output_interactions ( dataset , preds )\n",
    "            DDI-DrugBank.d398.s0|DDI-DrugBank.d398.s0.e0|DDI-DrugBank.d398.s0.e1|effect\n",
    "            DDI-DrugBank.d398.s0|DDI-DrugBank.d398.s0.e0|DDI-DrugBank.d398.s0.e2|effect\n",
    "            DDI-DrugBank.d211.s2|DDI-DrugBank.d211.s2.e0|DDI-DrugBank.d211.s2.e5|mechanism\n",
    "            ...\n",
    "    '''\n",
    "    length = len(dataset)\n",
    "    for i in range(length):\n",
    "        sid = dataset[i][0]\n",
    "        id_e1 = dataset[i][1]\n",
    "        id_e2 = dataset[i][2]\n",
    "        ddi_type = preds[i]\n",
    "        outf.write(str(sid) +\"|\"+ str(id_e1) +\"|\"+ str(id_e2) +\"|\"+ str(ddi_type))\n",
    "        outf.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Classifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(datadir, outfile):\n",
    "    '''\n",
    "    Loads a NN model from a file 'modelname' and uses it to extract drugs in datadir. Saves\n",
    "    results to 'outfile' in the appropriate format\n",
    "    '''\n",
    "    \n",
    "    # load model and associated encoding data\n",
    "    model, idx = load_model_and_indexs()\n",
    "    \n",
    "    # load data to annotate\n",
    "    testdata = load_data(datadir)\n",
    "    \n",
    "    # encode dataset\n",
    "    X = encode_words(testdata, idx)\n",
    "    \n",
    "    # tag sentences in dataset\n",
    "    Y = model.predict(X)\n",
    "    # get most likely tag for each word\n",
    "    preds = []\n",
    "    for s in Y:\n",
    "        it = np.argmax(s)\n",
    "        for key, item in idx['labels'].items():\n",
    "            if item == it:\n",
    "                preds.append(key)\n",
    "    #Y = [[idx['labels'][np.argmax(y)] for y in s] for s in Y] # np.argmax(y) is not a key\n",
    "    \n",
    "    # extract entities and dump them to output file\n",
    "    outf = open(outfile, \"w\")\n",
    "    output_interactions(testdata, preds, outf)\n",
    "    outf.close()\n",
    "    \n",
    "    # evaluate using official evaluator\n",
    "    evaluate(\"DDI\", datadir, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
      "------------------------------------------------------------------------------\n",
      "advise             85\t 175\t  53\t 260\t 138\t32.7%\t61.6%\t42.7%\n",
      "effect            174\t 235\t 141\t 409\t 315\t42.5%\t55.2%\t48.1%\n",
      "int                15\t   0\t  20\t  15\t  35\t100.0%\t42.9%\t60.0%\n",
      "mechanism         176\t 207\t  88\t 383\t 264\t46.0%\t66.7%\t54.4%\n",
      "------------------------------------------------------------------------------\n",
      "M.avg            -\t-\t-\t-\t-\t55.3%\t56.6%\t51.3%\n",
      "------------------------------------------------------------------------------\n",
      "m.avg             450\t4219\t 302\t4669\t 752\t9.6%\t59.8%\t16.6%\n",
      "m.avg(no class)   752\t3917\t   0\t4669\t 752\t16.1%\t100.0%\t27.7%\n"
     ]
    }
   ],
   "source": [
    "devel_path = '../../labAHLT/data/devel'\n",
    "predict(devel_path, 'val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
      "------------------------------------------------------------------------------\n",
      "advise            118\t 191\t  94\t 309\t 212\t38.2%\t55.7%\t45.3%\n",
      "effect            170\t 243\t 113\t 413\t 283\t41.2%\t60.1%\t48.9%\n",
      "int                 7\t   9\t  11\t  16\t  18\t43.8%\t38.9%\t41.2%\n",
      "mechanism         205\t 329\t 132\t 534\t 337\t38.4%\t60.8%\t47.1%\n",
      "------------------------------------------------------------------------------\n",
      "M.avg            -\t-\t-\t-\t-\t40.4%\t53.9%\t45.6%\n",
      "------------------------------------------------------------------------------\n",
      "m.avg             500\t5191\t 350\t5691\t 850\t8.8%\t58.8%\t15.3%\n",
      "m.avg(no class)   850\t4841\t   0\t5691\t 850\t14.9%\t100.0%\t26.0%\n"
     ]
    }
   ],
   "source": [
    "test_path = '../../labAHLT/data/test'\n",
    "predict(test_path, 'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
