{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network DDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Flatten\n",
    "#from keras_contrib.layers import CRF\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_data()`- Use XML parsing and tokenization functions from previous exercises. Adding a PoS tagger or lemmatizer may be useful. \\\n",
    "Masking the target drugs as e.g. `<DRUG1>`, `<DRUG2>`, and the rest as `<DRUG_OTHER>` will help the algorithm generalize and avoid it focusing in the drug names, which are not relevant for the DDI task (and also make it easier for it to spot the target entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    '''\n",
    "    Task: Load XML files in given directory, tokenize each sentence, and extract\n",
    "    learning examples (tokenized sentence + entity pair).\n",
    "    \n",
    "    Input: \n",
    "        datadir: A directory containing XML files\n",
    "        \n",
    "    Output: A list of classification cases. Each case is a list containing sentenceid, entity1_id, entity2_id,\n",
    "            ground truth relation label, and a list of sentence tokens (each token containing any needed information:\n",
    "            word, lemma, PoS, offsets, etc.\n",
    "    '''\n",
    "    dataset = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for f in listdir(datadir):\n",
    "        \n",
    "        tree = parse(datadir + \"/\" + f)\n",
    "        sentences = tree.getElementsByTagName(\"sentence\")\n",
    "\n",
    "        for s in sentences: \n",
    "            sid = s.attributes[\"id\"].value   \n",
    "            stext = s.attributes[\"text\"].value\n",
    "            stext = stext.replace(\"-\",\" \")\n",
    "            \n",
    "            ents = {}\n",
    "            entities = s.getElementsByTagName(\"entity\")\n",
    "            for e in entities:\n",
    "                e_id = e.attributes[\"id\"].value\n",
    "                offset = e.attributes[\"charOffset\"].value\n",
    "                name = e.attributes[\"text\"].value\n",
    "                ents[e_id] = name\n",
    "\n",
    "            pairs = s.getElementsByTagName(\"pair\")\n",
    "            for p in pairs:\n",
    "                pair_sent = stext\n",
    "                e1_id = p.attributes['e1'].value\n",
    "                e2_id = p.attributes['e2'].value\n",
    "                if p.attributes['ddi'].value == 'true':\n",
    "                    ddi = p.attributes['type'].value\n",
    "                else:\n",
    "                    ddi = 'null'\n",
    "                \n",
    "                aux = [sid, e1_id, e2_id, ddi]\n",
    "                for key, item in ents.items():\n",
    "                    if key == e1_id:\n",
    "                        drug_1 = item\n",
    "                        pair_sent = pair_sent.replace(drug_1, 'DRUG1')\n",
    "                    elif key == e2_id:\n",
    "                        drug_2 = item\n",
    "                        pair_sent = pair_sent.replace(drug_2, 'DRUG2')\n",
    "                    else:\n",
    "                        other_drug = item\n",
    "                        pair_sent = pair_sent.replace(other_drug, 'DRUGOTHER')\n",
    "        \n",
    "                #punct = [\".\",\",\",\";\",\":\",\"?\",\"!\", \"'\"] # TODO: Change all punctuations to <PUNCT> and stopwords to <SW>\n",
    "                tokens = word_tokenize(pair_sent)\n",
    "                aux2 = []\n",
    "                for t in tokens:\n",
    "                    t_l = t.lower()\n",
    "                    lemma = wordnet_lemmatizer.lemmatize(t_l)\n",
    "                    tk = (t, lemma) # TODO: Add PoS Tag and maybe use stemmer instead of lemmatizer\n",
    "                    aux2.append(tk)\n",
    "                aux.append(aux2)\n",
    "                dataset.append(aux)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DDI-DrugBank.d234.s2', 'DDI-DrugBank.d234.s2.e0', 'DDI-DrugBank.d234.s2.e1', 'effect', [('Particular', 'particular'), ('caution', 'caution'), ('is', 'is'), ('necessary', 'necessary'), ('when', 'when'), ('using', 'using'), ('DRUG1', 'drug1'), ('in', 'in'), ('cases', 'case'), ('of', 'of'), ('mixed', 'mixed'), ('drug', 'drug'), ('overdosage', 'overdosage'), ('since', 'since'), ('the', 'the'), ('toxic', 'toxic'), ('effects', 'effect'), ('(', '('), ('such', 'such'), ('as', 'a'), ('convulsions', 'convulsion'), ('and', 'and'), ('cardiac', 'cardiac'), ('dysrhythmias', 'dysrhythmias'), (')', ')'), ('of', 'of'), ('other', 'other'), ('drugs', 'drug'), ('taken', 'taken'), ('in', 'in'), ('overdose', 'overdose'), ('(', '('), ('especially', 'especially'), ('DRUG2', 'drug2'), (')', ')'), ('may', 'may'), ('emerge', 'emerge'), ('with', 'with'), ('the', 'the'), ('reversal', 'reversal'), ('of', 'of'), ('the', 'the'), ('DRUGOTHER', 'drugother'), ('effect', 'effect'), ('by', 'by'), ('DRUGOTHER', 'drugother'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "train_path = '../../labAHLT/data/train'\n",
    "dataset = load_data(train_path)\n",
    "print(dataset[65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(dataset, max_length):\n",
    "    '''\n",
    "    Task: Create index dictionaries both for input (words) and output (labels) from given dataset\n",
    "    Input: \n",
    "        dataset: dataset produced by load_data.\n",
    "        max_length: maximum length of a sentence (longer sentences will be cut, shorter ones will be padded).\n",
    "        \n",
    "    Output: A dictionary where each key is an index name (e.g. \"words\", \"labels\"), and the value is a \n",
    "            dictionary mapping each word/label to a number. An entry with the value for maxlen is also stored\n",
    "    Example: \n",
    "        >>> create_indx(traindata)\n",
    "        {'words': {'<PAD>':0, '<UNK>':1, '11-day':2, 'murine':3, 'criteria':4,\n",
    "                   'stroke':5, ... ,'levodopa':8511, 'terfenadine': 8512}\n",
    "         'labels': {' null ':0, 'mechanism ':1, 'advise ':2, 'effect ':3, 'int ':4}\n",
    "         'maxlen': 100 }\n",
    "    '''\n",
    "    index_words = {'<PAD>':0, '<UNK>':1}\n",
    "    i = 2\n",
    "    \n",
    "    index_labels = {}\n",
    "    j = 0\n",
    "\n",
    "    # We can create other indexes (lemmas, PoS, etc)  \n",
    "    \n",
    "    for s in dataset:\n",
    "        words = s[4]\n",
    "        label = s[3]\n",
    "        if label not in index_labels:\n",
    "            index_labels[label] = j\n",
    "            j += 1\n",
    "        for tup in words:\n",
    "            w = tup[0]\n",
    "            if w not in index_words:\n",
    "                index_words[w] = i\n",
    "                i += 1\n",
    "    idx = {'words': index_words, 'labels':index_labels, 'maxlen':max_length}     \n",
    "    \n",
    "    return idx\n",
    "    \n",
    "# Add '<PAD>': 0 and '<UNK>':1 codes to 'words' index. The coding of the rest of the words/labels is arbitrary.\n",
    "# You may add to the dictionary entries with indexes for other elements you want to use (lemmas, PoS, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5070"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = create_index(dataset, 100)\n",
    "#print(idx[\"words\"])\n",
    "#print(idx[\"labels\"])\n",
    "#print(idx['maxlen'])\n",
    "vocab_size = len(idx[\"words\"]); vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the words in a sentence dataset formed by lists of tokens into lists of indexes\n",
    "          suitable for NN input.\n",
    "    Input: \n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, as well\n",
    "             as the maximum sentence length.\n",
    "             \n",
    "    Output: The dataset encoded as a list of sentence, each of them is a list of word indices.\n",
    "            If the word is not in the index, <UNK> code is used. If the sentence is shorter than\n",
    "            max_len it is padded with <PAD> code.\n",
    "    Example: \n",
    "        >>> encode_words(traindata, idx)\n",
    "            [ [6882 1049 4911 ... 0 0 0 ]\n",
    "            [  2290 7548 8069 ... 0 0 0 ]\n",
    "               ...\n",
    "            [  2002 6582 7518 ... 0 0 0 ] ]\n",
    "    '''\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for s in dataset:\n",
    "        words = s[4]\n",
    "        aux = []\n",
    "        for tup in words:\n",
    "            w = tup[0]\n",
    "            if w in idx['words']:\n",
    "                i = idx['words'][w]\n",
    "            else:\n",
    "                i = idx['words']['<UNK>']\n",
    "            aux.append(i)\n",
    "        seq.append(aux)\n",
    "        \n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    \n",
    "    return seq_padded\n",
    "        \n",
    "        \n",
    "def encode_labels(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the ground truth labels in a dataset of classification examples (sentence + entity pair).\n",
    "    Input:\n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_index, containing word and label indexes, as well as the maximum length.\n",
    "        \n",
    "    Output: The dataset encoded as a list DDI labels, one per classification example. \n",
    "    \n",
    "    Example :\n",
    "     >>> encode_labels ( traindata , idx )\n",
    "     [  [0] [0] [2] ... [4] [0] [0] [1] [0] ]\n",
    "     [  [4] [6] [4] [4] [4] [4] ... [0] [0] ]\n",
    "     [  [4] [4] [8] [4] [6] [4] ... [0] [0] ]\n",
    "         ...\n",
    "     [  [4] [8] [9] [4] [4] [4] ... [0] [0] ]\n",
    "     ] \n",
    "    '''\n",
    "    \n",
    "    seq = []\n",
    "    for s in dataset:\n",
    "        label = s[3]\n",
    "        i = idx['labels'][label]\n",
    "        seq.append([i])\n",
    "        \n",
    "    Y = [to_categorical(i, num_classes=5) for i in seq]\n",
    "    return seq, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    3    4 ...    0    0    0]\n",
      " [  19   20   21 ...    0    0    0]\n",
      " [  19   20   21 ...    0    0    0]\n",
      " ...\n",
      " [1280 1075  146 ...    0    0    0]\n",
      " [1280 1075  146 ...    0    0    0]\n",
      " [1280 1075  146 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "X_train = encode_words(dataset, idx)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Y, Y_train = encode_labels(dataset, idx)\n",
    "#print(idx['labels'])\n",
    "print(Y_train[0])\n",
    "#print(Y[0:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx, embedding_dim):\n",
    "    '''\n",
    "    Task: Create network for the learner. \n",
    "    Input:\n",
    "        idx: index dictionary with word/labels codes, plus maximum sentence length.\n",
    "    Output: Returns a compiled Keras neural network with the specified layers\n",
    "    '''\n",
    "    \n",
    "    #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # create network layers\n",
    "    inp = Input(shape=(max_len,))\n",
    "    word_emb = Embedding(input_dim=n_words + 1, output_dim=embedding_dim, input_length=max_len, mask_zero=True, trainable=False)(inp) # TransferLearning -> GloVe\n",
    "    model = Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.4))(word_emb)\n",
    "    out = TimeDistributed(Dense(n_labels, activation='softmax'))(model)\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "    optimiz = Adam(lr=0.01, decay=1e-6)\n",
    "    model.compile(optimizer=optimiz, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, 100, 16)           81136     \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 100, 128)          41472     \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 100, 5)            645       \n",
      "=================================================================\n",
      "Total params: 123,253\n",
      "Trainable params: 42,117\n",
      "Non-trainable params: 81,136\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23148, 1, 5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_network(idx,16)\n",
    "model.summary()\n",
    "np.array(Y_train).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:755 train_step\n        loss = self.compiled_loss(\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1537 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4833 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 1, 5) and (None, 100, 5) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-944bee9fe832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(np.array(X_train), np.array(Y_train),\n\u001b[0m\u001b[1;32m      2\u001b[0m                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    verbose=2)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 725\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    726\u001b[0m             *args, **kwds))\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:755 train_step\n        loss = self.compiled_loss(\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1537 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4833 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /Users/mponsclo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 1, 5) and (None, 100, 5) are incompatible\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(X_train), np.array(Y_train),\n",
    "                   batch_size=16,\n",
    "                   epochs=5,\n",
    "                   verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_indexes(model, idx, filename):\n",
    "    '''\n",
    "    Task: Save given model and indexs to disk\n",
    "    Input: \n",
    "        model: Keras model created by _build_network, and trained.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, \n",
    "             as well as the maximum sentence length. \n",
    "        filename: Saves the mode into filename.nn and the indexes into filename.idx\n",
    "    '''\n",
    "    \n",
    "    # Use Keras.model.save and keras.models.load_model functions to save/load the model\n",
    "    # Use your preferred method (pickel, plain text, etc) to save/load the index dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Learner()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner(traindir, validationdir, modelname):\n",
    "    '''\n",
    "    Learns a NN model using traindir as training data, and validationdir as validation data.\n",
    "    Saves learnt model in a file named modelname\n",
    "    '''\n",
    "    # load train and validation data in a suitable form\n",
    "    traindata = load_data(traindir)\n",
    "    valdata = load_data(validationdir)\n",
    "    \n",
    "    # create indexes from trainindg data \n",
    "    max_len = 100\n",
    "    idx = create_indexs(traindata, max_len)\n",
    "    \n",
    "    # build network \n",
    "    model = build_model(idx)\n",
    "    \n",
    "    # encode datasets\n",
    "    Xtrain = encode_words(traindata, idx)\n",
    "    Ytrain = encode_labels(traindata, idx)\n",
    "    Xval = encode_words(valdata, idx)\n",
    "    Yval = encode_labels(valdata, idx)\n",
    "    \n",
    "    # train model\n",
    "    model.fit(Xtrain, Ytrain, validation_data=(Xval, Yval))\n",
    "    \n",
    "    # save model and indexs, for later use in prediction\n",
    "    save_model_and_indexs(model, idx, modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Classifier Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_indexs(filename):\n",
    "    '''\n",
    "    Task: Load model and associate indexs from disk.\n",
    "    Input:\n",
    "        filename: filename to be loaded\n",
    "    Output: Loads a model from filename.nn and its indexes from filename.idx\n",
    "            Returns the loaded model and indexes. \n",
    "    '''\n",
    "    \n",
    "    # Use Keras.model.save and keras.models.load_model functions to save/load the model\n",
    "    # Use your preferred method (pickel, plain text, etc) to save/load the index dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_interactions(dataset, preds): \n",
    "    '''\n",
    "    Task: Output detected DDIs in the format expected by the evaluator.\n",
    "    Input:\n",
    "        dataset: A dataset produced by load_data\n",
    "        preds: Fore each sentence in dataset, a label for its DDI type (or 'null' if no DDI detected)\n",
    "        \n",
    "    Output: prints the detected interactions to dtdout in the format required by the evaluator.\n",
    "    Example:\n",
    "        >>> output_interactions ( dataset , preds )\n",
    "            DDI - DrugBank . d398 .s0|DDI - DrugBank . d398 .s0.e0|DDI - DrugBank . d398 .s0.e1|effect\n",
    "            DDI - DrugBank . d398 .s0|DDI - DrugBank . d398 .s0.e0|DDI - DrugBank . d398 .s0.e2|effect\n",
    "            DDI - DrugBank . d211 .s2|DDI - DrugBank . d211 .s2.e0|DDI - DrugBank . d211 .s2.e5|mechanism\n",
    "            ...\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Classifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(modelname, datadir, outfile):\n",
    "    '''\n",
    "    Loads a NN model from a file 'modelname' and uses it to extract drugs in datadir. Saves\n",
    "    results to 'outfile' in the appropriate format\n",
    "    '''\n",
    "    \n",
    "    # load model and associated encoding data\n",
    "    model, idx = load_model_and_indexs(modelname)\n",
    "    \n",
    "    # load data to annotate\n",
    "    testdata = load_data(datadir)\n",
    "    \n",
    "    # encode dataset\n",
    "    X = encode_words(testdata, idx)\n",
    "    \n",
    "    # tag sentences in dataset\n",
    "    Y = model.predict(X)\n",
    "    # get most likely tag for each word\n",
    "    Y = [[idx['labels'][np.argmax(y)] for y in s] for s in Y]\n",
    "    \n",
    "    # extract entities and dump them to output file\n",
    "    output_entities(testdata, Y, outfile)\n",
    "    \n",
    "    # evaluate using official evaluator\n",
    "    evaluation(datadir, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = '../../labAHLT/data/train/Balsalazide_ddi.xml'\n",
    "file = '../../labAHLT/data/train/Idoxuridine_ddi.xml'\n",
    "dataset = []\n",
    "\n",
    "tree = parse(file)\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentences = tree.getElementsByTagName(\"sentence\")\n",
    "\n",
    "for s in sentences: \n",
    "    sid = s.attributes[\"id\"].value\n",
    "            \n",
    "    stext = s.attributes[\"text\"].value\n",
    "    stext = stext.replace(\"-\",\" \")\n",
    "            \n",
    "    ents = {}\n",
    "    entities = s.getElementsByTagName(\"entity\")\n",
    "    \n",
    "    for e in entities:\n",
    "        e_id = e.attributes[\"id\"].value\n",
    "        offset = e.attributes[\"charOffset\"].value\n",
    "        name = e.attributes[\"text\"].value\n",
    "        ents[e_id] = name\n",
    "\n",
    "    pairs = s.getElementsByTagName(\"pair\")\n",
    "    for p in pairs:\n",
    "        pair_sent = stext\n",
    "        e1_id = p.attributes['e1'].value\n",
    "        e2_id = p.attributes['e2'].value\n",
    "        \n",
    "        \n",
    "        if p.attributes['ddi'].value == 'true':\n",
    "            ddi = p.attributes['type'].value\n",
    "        else:\n",
    "            ddi = 'none'\n",
    "\n",
    "        aux = [sid, e1_id, e2_id, ddi]\n",
    "        for key, item in ents.items():\n",
    "            if key == e1_id:\n",
    "                drug_1 = item\n",
    "                pair_sent = pair_sent.replace(drug_1, 'DRUG1')\n",
    "            elif key == e2_id:\n",
    "                drug_2 = item\n",
    "                pair_sent = pair_sent.replace(drug_2, 'DRUG2')\n",
    "            else:\n",
    "                other_drug = item\n",
    "                pair_sent = pair_sent.replace(other_drug, 'DRUG_OTHER')\n",
    "        \n",
    "        #punct = [\".\",\",\",\";\",\":\",\"?\",\"!\"] # Remove stopwords\n",
    "        tokens = word_tokenize(pair_sent)\n",
    "        aux2 = []\n",
    "        for t in tokens:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(t)\n",
    "            tk = (t, lemma) # TODO: Add PoS Tag and check lemmatizer\n",
    "            aux2.append(tk)\n",
    "        aux.append(aux2)\n",
    "        dataset.append(aux)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiriQ3",
   "language": "python",
   "name": "miriq3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
