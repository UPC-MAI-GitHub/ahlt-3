{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network DDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "#from keras_contrib.layers import CRF\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_data()`- Use XML parsing and tokenization functions from previous exercises. Adding a PoS tagger or lemmatizer may be useful. \\\n",
    "Masking the target drugs as e.g. `<DRUG1>`, `<DRUG2>`, and the rest as `<DRUG_OTHER>` will help the algorithm generalize and avoid it focusing in the drug names, which are not relevant for the DDI task (and also make it easier for it to spot the target entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['DDI-DrugBank.d486.s0', 'DDI-DrugBank.d486.s0.e0', 'DDI-DrugBank.d486.s0.e1', 'false', [('No', 'No'), ('drug', 'drug'), ('interaction', 'interaction'), ('studies', 'study'), ('have', 'have'), ('been', 'been'), ('conducted', 'conducted'), ('for', 'for'), ('<', '<'), ('DRUG1', 'DRUG1'), ('>', '>'), (',', ','), ('however', 'however'), ('the', 'the'), ('use', 'use'), ('of', 'of'), ('orally', 'orally'), ('administered', 'administered'), ('<', '<'), ('DRUG2', 'DRUG2'), ('>', '>'), ('could', 'could'), (',', ','), ('theoretically', 'theoretically'), (',', ','), ('interfere', 'interfere'), ('with', 'with'), ('the', 'the'), ('release', 'release'), ('of', 'of'), ('<', '<'), ('DRUG_OTHER', 'DRUG_OTHER'), ('>', '>'), ('in', 'in'), ('the', 'the'), ('colon', 'colon'), ('.', '.')]], ['DDI-DrugBank.d486.s0', 'DDI-DrugBank.d486.s0.e0', 'DDI-DrugBank.d486.s0.e2', 'false', [('No', 'No'), ('drug', 'drug'), ('interaction', 'interaction'), ('studies', 'study'), ('have', 'have'), ('been', 'been'), ('conducted', 'conducted'), ('for', 'for'), ('<', '<'), ('DRUG1', 'DRUG1'), ('>', '>'), (',', ','), ('however', 'however'), ('the', 'the'), ('use', 'use'), ('of', 'of'), ('orally', 'orally'), ('administered', 'administered'), ('<', '<'), ('DRUG_OTHER', 'DRUG_OTHER'), ('>', '>'), ('could', 'could'), (',', ','), ('theoretically', 'theoretically'), (',', ','), ('interfere', 'interfere'), ('with', 'with'), ('the', 'the'), ('release', 'release'), ('of', 'of'), ('<', '<'), ('DRUG2', 'DRUG2'), ('>', '>'), ('in', 'in'), ('the', 'the'), ('colon', 'colon'), ('.', '.')]], ['DDI-DrugBank.d486.s0', 'DDI-DrugBank.d486.s0.e1', 'DDI-DrugBank.d486.s0.e2', 'false', [('No', 'No'), ('drug', 'drug'), ('interaction', 'interaction'), ('studies', 'study'), ('have', 'have'), ('been', 'been'), ('conducted', 'conducted'), ('for', 'for'), ('<', '<'), ('DRUG_OTHER', 'DRUG_OTHER'), ('>', '>'), (',', ','), ('however', 'however'), ('the', 'the'), ('use', 'use'), ('of', 'of'), ('orally', 'orally'), ('administered', 'administered'), ('<', '<'), ('DRUG1', 'DRUG1'), ('>', '>'), ('could', 'could'), (',', ','), ('theoretically', 'theoretically'), (',', ','), ('interfere', 'interfere'), ('with', 'with'), ('the', 'the'), ('release', 'release'), ('of', 'of'), ('<', '<'), ('DRUG2', 'DRUG2'), ('>', '>'), ('in', 'in'), ('the', 'the'), ('colon', 'colon'), ('.', '.')]]]\n"
     ]
    }
   ],
   "source": [
    "file = '/Users/mponsclo/Documents/Master/labAHLT/data/train/Balsalazide_ddi.xml'\n",
    "dataset = []\n",
    "\n",
    "tree = parse(file)\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentences = tree.getElementsByTagName(\"sentence\")\n",
    "\n",
    "for s in sentences: \n",
    "    sid = s.attributes[\"id\"].value\n",
    "            \n",
    "    stext = s.attributes[\"text\"].value\n",
    "    stext = stext.replace(\"-\",\" \")\n",
    "            \n",
    "    ents = {}\n",
    "    entities = s.getElementsByTagName(\"entity\")\n",
    "    \n",
    "    for e in entities:\n",
    "        e_id = e.attributes[\"id\"].value\n",
    "        offset = e.attributes[\"charOffset\"].value\n",
    "        name = e.attributes[\"text\"].value\n",
    "        ents[e_id] = name\n",
    "\n",
    "    pairs = s.getElementsByTagName(\"pair\")\n",
    "    for p in pairs:\n",
    "        pair_sent = stext\n",
    "        e1_id = p.attributes['e1'].value\n",
    "        e2_id = p.attributes['e2'].value\n",
    "        ddi = p.attributes['ddi'].value\n",
    "        aux = [sid, e1_id, e2_id, ddi]\n",
    "        for key, item in ents.items():\n",
    "            if key == e1_id:\n",
    "                drug_1 = item\n",
    "                pair_sent = pair_sent.replace(drug_1, '<DRUG1>')\n",
    "            elif key == e2_id:\n",
    "                drug_2 = item\n",
    "                pair_sent = pair_sent.replace(drug_2, '<DRUG2>')\n",
    "            else:\n",
    "                other_drug = item\n",
    "                pair_sent = pair_sent.replace(other_drug, '<DRUG_OTHER>')\n",
    "        \n",
    "        #punct = [\".\",\",\",\";\",\":\",\"?\",\"!\"] # Remove stopwords\n",
    "        tokens = word_tokenize(pair_sent)\n",
    "        aux2 = []\n",
    "        for t in tokens:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(t)\n",
    "            tk = (t, lemma) # TODO: Add PoS Tag and check lemmatizer\n",
    "            aux2.append(tk)\n",
    "        aux.append(aux2)\n",
    "        dataset.append(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    '''\n",
    "    Task: Load XML files in given directory, tokenize each sentence, and extract\n",
    "    learning examples (tokenized sentence + entity pair).\n",
    "    \n",
    "    Input: \n",
    "        datadir: A directory containing XML files\n",
    "        \n",
    "    Output: A list of classification cases. Each case is a list containing sentenceid, entity1_id, entity2_id,\n",
    "            ground truth relation label, and a list of sentence tokens (each token containing any needed information:\n",
    "            word, lemma, PoS, offsets, etc.\n",
    "    '''\n",
    "    dataset = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for f in listdir(datadir):\n",
    "        \n",
    "        tree = parse(datadir + \"/\" + f)\n",
    "        sentences = tree.getElementsByTagName(\"sentence\")\n",
    "\n",
    "        for s in sentences: \n",
    "            sid = s.attributes[\"id\"].value   \n",
    "            stext = s.attributes[\"text\"].value\n",
    "            stext = stext.replace(\"-\",\" \")\n",
    "            \n",
    "            ents = {}\n",
    "            entities = s.getElementsByTagName(\"entity\")\n",
    "            for e in entities:\n",
    "                e_id = e.attributes[\"id\"].value\n",
    "                offset = e.attributes[\"charOffset\"].value\n",
    "                name = e.attributes[\"text\"].value\n",
    "                ents[e_id] = name\n",
    "\n",
    "            pairs = s.getElementsByTagName(\"pair\")\n",
    "            for p in pairs:\n",
    "                pair_sent = stext\n",
    "                e1_id = p.attributes['e1'].value\n",
    "                e2_id = p.attributes['e2'].value\n",
    "                if p.attributes['ddi'].value == 'true':\n",
    "                    ddi = p.attributes['type'].value\n",
    "                else:\n",
    "                    ddi = 'none'\n",
    "                \n",
    "                aux = [sid, e1_id, e2_id, ddi]\n",
    "                for key, item in ents.items():\n",
    "                    if key == e1_id:\n",
    "                        drug_1 = item\n",
    "                        pair_sent = pair_sent.replace(drug_1, 'DRUG1')\n",
    "                    elif key == e2_id:\n",
    "                        drug_2 = item\n",
    "                        pair_sent = pair_sent.replace(drug_2, 'DRUG2')\n",
    "                    else:\n",
    "                        other_drug = item\n",
    "                        pair_sent = pair_sent.replace(other_drug, 'DRUGOTHER')\n",
    "        \n",
    "                #punct = [\".\",\",\",\";\",\":\",\"?\",\"!\"] # Remove stopwords\n",
    "                tokens = word_tokenize(pair_sent)\n",
    "                aux2 = []\n",
    "                for t in tokens:\n",
    "                    lemma = wordnet_lemmatizer.lemmatize(t)\n",
    "                    tk = (t, lemma) # TODO: Add PoS Tag and check lemmatizer\n",
    "                    aux2.append(tk)\n",
    "                aux.append(aux2)\n",
    "                dataset.append(aux)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DDI-DrugBank.d234.s2', 'DDI-DrugBank.d234.s2.e0', 'DDI-DrugBank.d234.s2.e1', 'effect', [('Particular', 'Particular'), ('caution', 'caution'), ('is', 'is'), ('necessary', 'necessary'), ('when', 'when'), ('using', 'using'), ('DRUG1', 'DRUG1'), ('in', 'in'), ('cases', 'case'), ('of', 'of'), ('mixed', 'mixed'), ('drug', 'drug'), ('overdosage', 'overdosage'), ('since', 'since'), ('the', 'the'), ('toxic', 'toxic'), ('effects', 'effect'), ('(', '('), ('such', 'such'), ('as', 'a'), ('convulsions', 'convulsion'), ('and', 'and'), ('cardiac', 'cardiac'), ('dysrhythmias', 'dysrhythmias'), (')', ')'), ('of', 'of'), ('other', 'other'), ('drugs', 'drug'), ('taken', 'taken'), ('in', 'in'), ('overdose', 'overdose'), ('(', '('), ('especially', 'especially'), ('DRUG2', 'DRUG2'), (')', ')'), ('may', 'may'), ('emerge', 'emerge'), ('with', 'with'), ('the', 'the'), ('reversal', 'reversal'), ('of', 'of'), ('the', 'the'), ('DRUGOTHER', 'DRUGOTHER'), ('effect', 'effect'), ('by', 'by'), ('DRUGOTHER', 'DRUGOTHER'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "train_path = '/Users/mponsclo/Documents/Master/labAHLT/data/train'\n",
    "dataset = load_data(train_path)\n",
    "print(dataset[65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(dataset, max_length):\n",
    "    '''\n",
    "    Task: Create index dictionaries both for input (words) and output (labels) from given dataset\n",
    "    Input: \n",
    "        dataset: dataset produced by load_data.\n",
    "        max_length: maximum length of a sentence (longer sentences will be cut, shorter ones will be padded).\n",
    "        \n",
    "    Output: A dictionary where each key is an index name (e.g. \"words\", \"labels\"), and the value is a \n",
    "            dictionary mapping each word/label to a number. An entry with the value for maxlen is also stored\n",
    "    Example: \n",
    "        >>> create_indx(traindata)\n",
    "        {'words': {'<PAD>':0, '<UNK>':1, '11-day':2, 'murine':3, 'criteria':4,\n",
    "                   'stroke':5, ... ,'levodopa':8511, 'terfenadine': 8512}\n",
    "         'labels': {'<PAD>':0, 'B-group':1, 'B-drug_n':2, 'I-drug_n':3, 'O':4, \n",
    "                    'I-group':5, 'B-drug':6, 'I-drug':7, 'B-brand':8, 'I-brand':9}\n",
    "         'maxlen': 100 }\n",
    "    '''\n",
    "    \n",
    "# Add '<PAD>': 0 and '<UNK>':1 codes to 'words' index. The coding of the rest of the words/labels is arbitrary.\n",
    "# You may add to the dictionary entries with indexes for other elements you want to use (lemmas, PoS, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the words in a sentence dataset formed by lists of tokens into lists of indexes\n",
    "          suitable for NN input.\n",
    "    Input: \n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, as well\n",
    "             as the maximum sentence length.\n",
    "             \n",
    "    Output: The dataset encoded as a list of sentence, each of them is a list of word indices.\n",
    "            If the word is not in the index, <UNK> code is used. If the sentence is shorter than\n",
    "            max_len it is padded with <PAD> code.\n",
    "    Example: \n",
    "        >>> encode_words(traindata, idx)\n",
    "            [ [6882 1049 4911 ... 0 0 0 ]\n",
    "            [  2290 7548 8069 ... 0 0 0 ]\n",
    "               ...\n",
    "            [  2002 6582 7518 ... 0 0 0 ] ]\n",
    "    '''\n",
    "    \n",
    "def encode_labels(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the ground truth labels in a dataset of classification examples (sentence + entity pair).\n",
    "    Input:\n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_index, containing word and label indexes, as well as the maximum length.\n",
    "        \n",
    "    Output: The dataset encoded as a list DDI labels, one per classification example. \n",
    "    \n",
    "    Example :\n",
    "     >>> encode_labels ( traindata , idx )\n",
    "     [  [0] [0] [2] ... [4] [0] [0] [1] [0] ]\n",
    "     [[ [4] [6] [4] [4] [4] [4] ... [0] [0] ]\n",
    "     [  [4] [4] [8] [4] [6] [4] ... [0] [0] ]\n",
    "         ...\n",
    "     [  [4] [8] [9] [4] [4] [4] ... [0] [0] ]\n",
    "     ] \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx):\n",
    "    '''\n",
    "    Task: Create network for the learner. \n",
    "    Input:\n",
    "        idx: index dictionary with word/labels codes, plus maximum sentence length.\n",
    "    Output: Returns a compiled Keras neural network with the specified layers\n",
    "    '''\n",
    "    \n",
    "    #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # create network layers\n",
    "    inp = Input(shape=(max_len,))\n",
    "    ##addd layers\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inp, out)\n",
    "    model.compile() # set appropriate parameters (optimizer, loss, etc)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_indexes(model, idx, filename):\n",
    "    '''\n",
    "    Task: Save given model and indexs to disk\n",
    "    Input: \n",
    "        model: Keras model created by _build_network, and trained.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, \n",
    "             as well as the maximum sentence length. \n",
    "        filename: Saves the mode into filename.nn and the indexes into filename.idx\n",
    "    '''\n",
    "    \n",
    "    # Use Keras.model.save and keras.models.load_model functions to save/load the model\n",
    "    # Use your preferred method (pickel, plain text, etc) to save/load the index dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Learner()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner(traindir, validationdir, modelname):\n",
    "    '''\n",
    "    Learns a NN model using traindir as training data, and validationdir as validation data.\n",
    "    Saves learnt model in a file named modelname\n",
    "    '''\n",
    "    # load train and validation data in a suitable form\n",
    "    traindata = load_data(traindir)\n",
    "    valdata = load_data(validationdir)\n",
    "    \n",
    "    # create indexes from trainindg data \n",
    "    max_len = 100\n",
    "    idx = create_indexs(traindata, max_len)\n",
    "    \n",
    "    # build network \n",
    "    model = build_model(idx)\n",
    "    \n",
    "    # encode datasets\n",
    "    Xtrain = encode_words(traindata, idx)\n",
    "    Ytrain = encode_labels(traindata, idx)\n",
    "    Xval = encode_words(valdata, idx)\n",
    "    Yval = encode_labels(valdata, idx)\n",
    "    \n",
    "    # train model\n",
    "    model.fit(Xtrain, Ytrain, validation_data=(Xval, Yval))\n",
    "    \n",
    "    # save model and indexs, for later use in prediction\n",
    "    save_model_and_indexs(model, idx, modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Classifier Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_indexs(filename):\n",
    "    '''\n",
    "    Task: Load model and associate indexs from disk.\n",
    "    Input:\n",
    "        filename: filename to be loaded\n",
    "    Output: Loads a model from filename.nn and its indexes from filename.idx\n",
    "            Returns the loaded model and indexes. \n",
    "    '''\n",
    "    \n",
    "    # Use Keras.model.save and keras.models.load_model functions to save/load the model\n",
    "    # Use your preferred method (pickel, plain text, etc) to save/load the index dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_interactions(dataset, preds): \n",
    "    '''\n",
    "    Task: Output detected DDIs in the format expected by the evaluator.\n",
    "    Input:\n",
    "        dataset: A dataset produced by load_data\n",
    "        preds: Fore each sentence in dataset, a label for its DDI type (or 'null' if no DDI detected)\n",
    "        \n",
    "    Output: prints the detected interactions to dtdout in the format required by the evaluator.\n",
    "    Example:\n",
    "        >>> output_interactions ( dataset , preds )\n",
    "            DDI - DrugBank . d398 .s0|DDI - DrugBank . d398 .s0.e0|DDI - DrugBank . d398 .s0.e1|effect\n",
    "            DDI - DrugBank . d398 .s0|DDI - DrugBank . d398 .s0.e0|DDI - DrugBank . d398 .s0.e2|effect\n",
    "            DDI - DrugBank . d211 .s2|DDI - DrugBank . d211 .s2.e0|DDI - DrugBank . d211 .s2.e5|mechanism\n",
    "            ...\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Classifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(modelname, datadir, outfile):\n",
    "    '''\n",
    "    Loads a NN model from a file 'modelname' and uses it to extract drugs in datadir. Saves\n",
    "    results to 'outfile' in the appropriate format\n",
    "    '''\n",
    "    \n",
    "    # load model and associated encoding data\n",
    "    model, idx = load_model_and_indexs(modelname)\n",
    "    \n",
    "    # load data to annotate\n",
    "    testdata = load_data(datadir)\n",
    "    \n",
    "    # encode dataset\n",
    "    X = encode_words(testdata, idx)\n",
    "    \n",
    "    # tag sentences in dataset\n",
    "    Y = model.predict(X)\n",
    "    # get most likely tag for each word\n",
    "    Y = [[idx['labels'][np.argmax(y)] for y in s] for s in Y]\n",
    "    \n",
    "    # extract entities and dump them to output file\n",
    "    output_entities(testdata, Y, outfile)\n",
    "    \n",
    "    # evaluate using official evaluator\n",
    "    evaluation(datadir, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiriQ3",
   "language": "python",
   "name": "miriq3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
