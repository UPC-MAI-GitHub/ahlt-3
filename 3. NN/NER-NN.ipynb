{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network NERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "#from keras_contrib.layers import CRF\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    '''\n",
    "    Task: Load XML files in given directory, tokenize each sentence, and extract\n",
    "    ground truth BIO labels for each token.\n",
    "    \n",
    "    Input: \n",
    "        datadir: A directory containing XML files\n",
    "        \n",
    "    Output: A directory containing the dataset. Dictionary key is sentence_id, and the \n",
    "            value is a list of token tuples (word, start, end, ground truth).\n",
    "            \n",
    "    Example: \n",
    "            >>> load_data('data/Train')\n",
    "            {'DDI-DrugBank.d370.s0': [('as', 0, 1,'O'), ('differin', 3, 10,'B-brand'),\n",
    "                     ('gel', 12, 14,'O'), ... , ('with', 343, 346, 'O'),\n",
    "                     ('caution', 348, 354, 'O'), ('.', 355, 355, 'O')],\n",
    "            'DDI-DrugBank.d370.s1': [('particular', 0, 9, 'O'), ('caution', 11, 17, 'O'),\n",
    "                     ('should', 19, 24, 'O'), ... , ('differin', 130, 137, 'B-brand'),\n",
    "                     ('gel', 139, 141, 'O'), ('.', 142, 142, 'O')], ... }\n",
    "    '''\n",
    "    \n",
    "    sentences_ids = []\n",
    "    sentences_dataset = []\n",
    "    labels = []\n",
    "    dict_dataset = {}\n",
    "    dict_labels = {}\n",
    "    \n",
    "    for f in listdir(datadir):\n",
    "        tree = parse(datadir + \"/\" + f)\n",
    "        \n",
    "        sentences = tree.getElementsByTagName(\"sentence\")\n",
    "        for s in sentences: \n",
    "            sid = s.attributes[\"id\"].value\n",
    "            sentences_ids.append(sid)\n",
    "            \n",
    "            stext = s.attributes[\"text\"].value\n",
    "            stext = stext.replace(\"-\",\" \")\n",
    "            sentences_dataset.append(stext)\n",
    "            \n",
    "            ents = {}\n",
    "            entities = s.getElementsByTagName(\"entity\")\n",
    "            for e in entities:\n",
    "                offset = e.attributes[\"charOffset\"].value\n",
    "                start = offset.split('-')[0]\n",
    "                end = offset.split('-')[1]\n",
    "                name = e.attributes[\"text\"].value\n",
    "                e_type = e.attributes[\"type\"].value\n",
    "                ents[name] = [e_type, start]\n",
    "            \n",
    "            punct = [\".\",\",\",\";\",\":\",\"?\",\"!\"]\n",
    "            tokens = word_tokenize(stext)\n",
    "            tokens_cleaned = []\n",
    "            for t in tokens:\n",
    "                if t not in punct:\n",
    "                    tokens_cleaned.append(t)\n",
    "            \n",
    "            tags = []\n",
    "            tokens = []\n",
    "            for t in tokens_cleaned:\n",
    "                offsetFrom = stext.find(t)\n",
    "                offsetTo = offsetFrom + len(t) - 1\n",
    "                if t in ents:\n",
    "                    if (int(ents[t][1]) == 0):\n",
    "                        tag = \"B-\"+ents[t][0] # TODO: ents after .?\n",
    "                    else:\n",
    "                        tag = \"I-\"+ents[t][0]\n",
    "                else:\n",
    "                    tag = \"O\"\n",
    "                tags.append(tag)\n",
    "                tupl = (t, offsetFrom, offsetTo, tag)\n",
    "                tokens.append(tupl)\n",
    "            \n",
    "            dict_dataset[sid] = tokens\n",
    "            dict_labels[sid] = tags\n",
    "            labels.append(tags)\n",
    "        \n",
    "    return dict_dataset, dict_labels, sentences_dataset, sentences_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"/Users/mponsclo/Documents/Master/labAHLT/data/train\"\n",
    "train_dataset, dict_labels, train_sentences, train_ids, train_labels = load_data(path_train)\n",
    "n_words = sum([len(t) for t in train_sentences]) ; n_words # total number of words in train dataset\n",
    "n_tags = 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(dataset, max_length):\n",
    "    '''\n",
    "    Task: Create index dictionaries both for input (words) and output (labels) from given dataset\n",
    "    Input: \n",
    "        dataset: dataset produced by load_data.\n",
    "        max_length: maximum length of a sentence (longer sentences will be cut, shorter ones will be padded).\n",
    "        \n",
    "    Output: A dictionary where each key is an index name (e.g. \"words\", \"labels\"), and the value is a \n",
    "            dictionary mapping each word/label to a number. An entry with the value for maxlen is also stored\n",
    "    Example: \n",
    "        >>> create_indx(traindata)\n",
    "        {'words': {'<PAD>':0, '<UNK>':1, '11-day':2, 'murine':3, 'criteria':4,\n",
    "                   'stroke':5, ... ,'levodopa':8511, 'terfenadine': 8512}\n",
    "         'labels': {'<PAD>':0, 'B-group':1, 'B-drug_n':2, 'I-drug_n':3, 'O':4, \n",
    "                    'I-group':5, 'B-drug':6, 'I-drug':7, 'B-brand':8, 'I-brand':9}\n",
    "         'maxlen': 100 }\n",
    "    '''\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=8000, lower = True, char_level=False, oov_token=\"<UNK>\")\n",
    "    tokenizer_labs = Tokenizer(num_words=12, lower=False, char_level=False, oov_token=\"<UNK>\")\n",
    "    \n",
    "    tokenizer.fit_on_texts(train_sentences)\n",
    "    word_index = tokenizer.word_index\n",
    "    #print(word_index)\n",
    "\n",
    "    tokenizer_labs.fit_on_texts(train_labels)\n",
    "    word_index_labs = tokenizer_labs.word_index\n",
    "    #print(word_index_labs)\n",
    "\n",
    "    indexs = {}\n",
    "    indexs['words'] = word_index\n",
    "    indexs['labels'] = word_index_labs\n",
    "    indexs['maxlen'] = max_length\n",
    "    \n",
    "    return indexs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 1,\n",
       " 'O': 2,\n",
       " 'I-drug': 3,\n",
       " 'I-group': 4,\n",
       " 'I-brand': 5,\n",
       " 'B-drug': 6,\n",
       " 'I-drug_n': 7,\n",
       " 'B-group': 8,\n",
       " 'B-brand': 9,\n",
       " 'B-drug_n': 10}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = create_index(train_sentences, 100)\n",
    "#idx['words']\n",
    "idx['labels']\n",
    "#idx['maxlen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the words in a sentence dataset formed by lists of tokens into lists of indexes\n",
    "          suitable for NN input.\n",
    "    Input: \n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, as well\n",
    "             as the maximum sentence length.\n",
    "             \n",
    "    Output: The dataset encoded as a list of sentence, each of them is a list of word indices.\n",
    "            If the word is not in the index, <UNK> code is used. If the sentence is shorter than\n",
    "            max_len it is padded with <PAD> code.\n",
    "    Example: \n",
    "        >>> encode_words(traindata, idx)\n",
    "            [ [6882 1049 4911 ... 0 0 0 ]\n",
    "            [  2290 7548 8069 ... 0 0 0 ]\n",
    "               ...\n",
    "            [  2002 6582 7518 ... 0 0 0 ] ]\n",
    "    '''\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for key, item in train_dataset.items():\n",
    "        aux = []\n",
    "        for t in item:\n",
    "            w = str(t[0]).lower()\n",
    "            if w in idx['words']:\n",
    "                i = idx['words'][w]\n",
    "            else:\n",
    "                i = idx['words']['<UNK>']\n",
    "            aux.append(i)\n",
    "        seq.append(aux)  \n",
    "    \n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    \n",
    "    return seq_padded\n",
    "\n",
    "def encode_labels(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the ground truth labels in a dataset formed by lists of tokens into lists of indexes\n",
    "        suitable for NN output.\n",
    "    Input:\n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_index, containing word and label indexes, as well as the maximum length.\n",
    "        \n",
    "    Output: The dataset encoded as a list of sentence, each of them is a list of BIO label indices. If the sentence\n",
    "            is shorter than max_len it is padded with <PAD> code. \n",
    "    \n",
    "    Example :\n",
    "     >>> encode_labels ( traindata , idx )\n",
    "        [[ [4] [6] [4] [4] [4] [4] ... [0] [0] ]\n",
    "        [  [4] [4] [8] [4] [6] [4] ... [0] [0] ]\n",
    "          ...\n",
    "        [\n",
    "    '''\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for key, item in dict_labels.items():\n",
    "        aux = []\n",
    "        for t in item:\n",
    "            w = str(t)\n",
    "            i = idx['labels'][w]\n",
    "            aux.append(i)\n",
    "        seq.append(aux)\n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    seq_categ = [to_categorical(i, num_classes = 11) for i in seq_padded]  # 10 classes + 1 UNK\n",
    "    \n",
    "    return seq_padded, seq_categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encode_words(train_dataset, idx)\n",
    "Y, Y_train = encode_labels(dict_labels, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1489 1489  386 ...    0    0    0]\n",
      " [  50  101 2385 ...    0    0    0]\n",
      " [ 181   48 2056 ...    0    0    0]\n",
      " ...\n",
      " [  80    5   16 ...    0    0    0]\n",
      " [  57  573   41 ...    0    0    0]\n",
      " [  12  317    6 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 3 2 2 2 2 2 2 2 2 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y[0])\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx):\n",
    "    '''\n",
    "    Task: Create network for the learner. \n",
    "    Input:\n",
    "        idx: index dictionary with word/labels codes, plus maximum sentence length.\n",
    "    Output: Returns a compiled Keras neural network with the specified layers\n",
    "    '''\n",
    "    \n",
    "    #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])+1\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # create network layers\n",
    "    inp = Input(shape=(max_len,))\n",
    "    model = Embedding(input_dim=n_words + 1, output_dim = n_labels, input_length = max_len)(inp)\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "    out = TimeDistributed(Dense(n_labels, activation=\"softmax\"))(model)\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "    optimiz = Adam(lr=0.01, decay=1e-6)\n",
    "    model.compile(optimizer=optimiz, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 100, 11)           81136     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100, 11)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 100, 200)          89600     \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 100, 11)           2211      \n",
      "=================================================================\n",
      "Total params: 172,947\n",
      "Trainable params: 172,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_network(idx)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "170/170 [==============================] - 18s 91ms/step - loss: 0.2974 - accuracy: 0.9126\n",
      "Epoch 2/2\n",
      "170/170 [==============================] - 17s 97ms/step - loss: 0.0227 - accuracy: 0.9931\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, np.array(Y_train),\n",
    "                   batch_size=32,\n",
    "                   epochs=2,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_indexes(model, idx, filename):\n",
    "    '''\n",
    "    Task: Save given model and indexs to disk\n",
    "    Input: \n",
    "        model: Keras model created by _build_network, and trained.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, \n",
    "             as well as the maximum sentence length. \n",
    "        filename: Saves the mode into filename.nn and the indexes into filename.idx\n",
    "    '''\n",
    "    \n",
    "    # Use Keras.model.save and keras.models.load_model functions to save/load the model\n",
    "    # Use your preferred method (pickel, plain text, etc) to save/load the index dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Learner()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner(traindir, validationdir):#, modelname):\n",
    "    '''\n",
    "    Learns a NN model using traindir as training data, and validationdir as validation data.\n",
    "    Saves learnt model in a file named modelname\n",
    "    '''\n",
    "    # load train and validation data in a suitable form\n",
    "    train_dataset, dict_labels, train_sentences, train_ids, train_labels = load_data(traindir)\n",
    "    val_dataset, dictv_labels, val_sentences, val_ids, val_labels = load_data(validationdir)\n",
    "    \n",
    "    # create indexes from trainindg data \n",
    "    max_len = 100\n",
    "    idx = create_index(train_sentences, max_len)\n",
    "    \n",
    "    # build network \n",
    "    model = build_network(idx)\n",
    "    \n",
    "    # encode datasets\n",
    "    Xtrain = encode_words(train_dataset, idx['words'], idx['maxlen'])\n",
    "    Y, Ytrain = encode_labels(dict_labels, idx['labels'], idx['maxlen'])\n",
    "    Xval = encode_words(val_dataset, idx['words'], idx['maxlen'])\n",
    "    Yv, Yval = encode_labels(dictv_labels, idx['labels'], idx['maxlen'])\n",
    "    \n",
    "    # train model\n",
    "    history = model.fit(Xtrain, np.array(Ytrain),\n",
    "                        batch_size=32,\n",
    "                        epochs=2,\n",
    "                        verbose=1,\n",
    "                        validation_data=(Xval, np.array(Yval)))\n",
    "    \n",
    "    # save model and indexs, for later use in prediction\n",
    "    #save_model_and_indexs(model, idx, modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "170/170 [==============================] - 21s 107ms/step - loss: 0.2969 - accuracy: 0.9231 - val_loss: 0.0252 - val_accuracy: 0.9931\n",
      "Epoch 2/2\n",
      "170/170 [==============================] - 19s 111ms/step - loss: 0.0227 - accuracy: 0.9931 - val_loss: 0.0144 - val_accuracy: 0.9951\n"
     ]
    }
   ],
   "source": [
    "path_train = \"/Users/mponsclo/Documents/Master/labAHLT/data/train\"\n",
    "path_dev = \"/Users/mponsclo/Documents/Master/labAHLT/data/devel\"\n",
    "learner(path_train, path_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Functions Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_indexs(filename):\n",
    "    '''\n",
    "    Task: Load model and associate indexs from disk.\n",
    "    Input:\n",
    "        filename: filename to be loaded\n",
    "    Output: Loads a model from filename.nn and its indexes from filename.idx\n",
    "            Returns the loaded model and indexes. \n",
    "    '''\n",
    "    \n",
    "    # Use Keras.model.save and keras.models.load_model functions to save/load the model\n",
    "    # Use your preferred method (pickel, plain text, etc) to save/load the index dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_entities(dataset, preds):\n",
    "    '''\n",
    "    Task: Output detected entities in the format expected by the evaluator\n",
    "    Input: \n",
    "        dataset: A dataset produced by load_data.\n",
    "        preds: For each sentence in dataset, a list with the labels for each sentence token, \n",
    "               as predicted by the model.\n",
    "    Output: prints the detected entities to stdout in the format required by the evaluator. \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Classifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(modelname, datadir, outfile):\n",
    "    '''\n",
    "    Loads a NN model from a file 'modelname' and uses it to extract drugs in datadir. Saves\n",
    "    results to 'outfile' in the appropriate format\n",
    "    '''\n",
    "    \n",
    "    # load model and associated encoding data\n",
    "    model, idx = load_model_and_indexs(modelname)\n",
    "    \n",
    "    # load data to annotate\n",
    "    testdata = load_data(datadir)\n",
    "    \n",
    "    # encode dataset\n",
    "    X = encode_words(testdata, idx)\n",
    "    \n",
    "    # tag sentences in dataset\n",
    "    Y = model.predict(X)\n",
    "    # get most likely tag for each word\n",
    "    Y = [[idx['labels'][np.argmax(y)] for y in s] for s in Y]\n",
    "    \n",
    "    # extract entities and dump them to output file\n",
    "    output_entities(testdata, Y, outfile)\n",
    "    \n",
    "    # evaluate using official evaluator\n",
    "    evaluation(datadir, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
