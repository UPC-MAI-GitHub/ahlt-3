{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "import pickle\n",
    "from xml.dom.minidom import parse\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import *\n",
    "#from keras_contrib.layers import CRF\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "from evaluator import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_multi_token_entity(ents, token):\n",
    "    keys = ents.keys()\n",
    "    for k in keys:\n",
    "        if token in k:\n",
    "            return ents[k]\n",
    "\n",
    "def load_data(datadir):\n",
    "    '''\n",
    "    Task: Load XML files in given directory, tokenize each sentence, and extract\n",
    "    ground truth BIO labels for each token.\n",
    "    \n",
    "    Input: \n",
    "        datadir: A directory containing XML files\n",
    "        \n",
    "    Output: A directory containing the dataset. Dictionary key is sentence_id, and the \n",
    "            value is a list of token tuples (word, start, end, ground truth).\n",
    "            \n",
    "    Example: \n",
    "            >>> load_data('data/Train')\n",
    "            {'DDI-DrugBank.d370.s0': [('as', 0, 1,'O'), ('differin', 3, 10,'B-brand'),\n",
    "                     ('gel', 12, 14,'O'), ... , ('with', 343, 346, 'O'),\n",
    "                     ('caution', 348, 354, 'O'), ('.', 355, 355, 'O')],\n",
    "            'DDI-DrugBank.d370.s1': [('particular', 0, 9, 'O'), ('caution', 11, 17, 'O'),\n",
    "                     ('should', 19, 24, 'O'), ... , ('differin', 130, 137, 'B-brand'),\n",
    "                     ('gel', 139, 141, 'O'), ('.', 142, 142, 'O')], ... }\n",
    "    '''\n",
    "    \n",
    "    dict_dataset = {}\n",
    "    \n",
    "    for f in listdir(datadir):\n",
    "        tree = parse(datadir + \"/\" + f)\n",
    "        \n",
    "        sentences = tree.getElementsByTagName(\"sentence\")\n",
    "        for s in sentences: \n",
    "            sid = s.attributes[\"id\"].value\n",
    "            \n",
    "            stext = s.attributes[\"text\"].value\n",
    "            #stext = stext.replace(\"-\",\" \") if used we lose beta-endorphin, if not use we lose calcium-rich tag\n",
    "            \n",
    "            ents = {}\n",
    "            entities = s.getElementsByTagName(\"entity\")\n",
    "            for e in entities:\n",
    "                offset = e.attributes[\"charOffset\"].value\n",
    "                offsets = offset.split(';')\n",
    "                name = e.attributes[\"text\"].value\n",
    "                e_type = e.attributes[\"type\"].value\n",
    "                ents[name] = {\"type\": e_type, \"offsets\": []}\n",
    "                for offset in offsets:\n",
    "                    start = offset.split('-')[0]\n",
    "                    end = offset.split('-')[1]\n",
    "                    ents[name][\"offsets\"].append((start, end))\n",
    "            \n",
    "            punct = [\",\",\";\",\":\",\"?\",\"!\", \"(\", \")\"] # removed \".\"\n",
    "            tokens = word_tokenize(stext)\n",
    "            tokens_cleaned = []\n",
    "            for t in tokens:\n",
    "                if t not in punct and t not in stopwords:\n",
    "                    tokens_cleaned.append(t)\n",
    "            \n",
    "            tags = []\n",
    "            tokens = []\n",
    "            for t in tokens_cleaned:\n",
    "                offsetFrom = stext.find(t)\n",
    "                offsetTo = offsetFrom + len(t) - 1\n",
    "                # 1-token entities\n",
    "                if t in ents:\n",
    "                    if (int(ents[t][\"offsets\"][0][0]) == offsetFrom):\n",
    "                        tag = \"B-\"+ents[t][\"type\"] # TODO: ents after .?\n",
    "                    else:\n",
    "                        tag = \"I-\"+ents[t][\"type\"]                    \n",
    "                else:\n",
    "                    multi_token_ent = find_multi_token_entity(ents, t)\n",
    "                    if multi_token_ent:\n",
    "                        if (int(multi_token_ent[\"offsets\"][0][0]) == offsetFrom):\n",
    "                            tag = \"B-\"+multi_token_ent[\"type\"] # TODO: ents after .?\n",
    "                        else:\n",
    "                            tag = \"I-\"+multi_token_ent[\"type\"]\n",
    "                    else:\n",
    "                        tag = \"O\"\n",
    "                tags.append(tag)\n",
    "                tupl = (t, offsetFrom, offsetTo, tag)\n",
    "                tokens.append(tupl)\n",
    "            \n",
    "            dict_dataset[sid] = tokens\n",
    "        \n",
    "    return dict_dataset\n",
    "\n",
    "# -- TODO: handle multi-token entities # I think I've done it :D\n",
    "# tricyclic antidepressants - 'O' should be Group\n",
    "# chondroitin ABC lyase - 'O' should be drug_n\n",
    "# heparinase III - 'O' should be drug_n\n",
    "# hyaluronan lyase - 'O' should be drug_n\n",
    "# Mercaptopurine/Azathioprine - 'O' should be drug\n",
    "# muscle relaxants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Interactions', 0, 11, 'O'), ('observed', 23, 30, 'O'), ('nondepolarizing', 43, 57, 'B-group'), ('muscle', 59, 64, 'I-group'), ('relaxants', 66, 74, 'I-group'), ('administered', 86, 97, 'O'), ('succession', 102, 111, 'O'), ('.', 112, 112, 'O')]\n"
     ]
    }
   ],
   "source": [
    "path_train = \"../../labAHLT/data/train\"\n",
    "path_dev = \"../../labAHLT/data/devel\"\n",
    "\n",
    "train_dataset = load_data(path_train)\n",
    "devel_dataset = load_data(path_dev)\n",
    "\n",
    "print(train_dataset['DDI-DrugBank.d661.s5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(dataset, max_length):\n",
    "    '''\n",
    "    Task: Create index dictionaries both for input (words) and output (labels) from given dataset\n",
    "    Input: \n",
    "        dataset: dataset produced by load_data.\n",
    "        max_length: maximum length of a sentence (longer sentences will be cut, shorter ones will be padded).\n",
    "        \n",
    "    Output: A dictionary where each key is an index name (e.g. \"words\", \"labels\"), and the value is a \n",
    "            dictionary mapping each word/label to a number. An entry with the value for maxlen is also stored\n",
    "    Example: \n",
    "        >>> create_indx(traindata)\n",
    "        {'words': {'<PAD>':0, '<UNK>':1, '11-day':2, 'murine':3, 'criteria':4,\n",
    "                   'stroke':5, ... ,'levodopa':8511, 'terfenadine': 8512}\n",
    "         'labels': {'<PAD>':0, 'B-group':1, 'B-drug_n':2, 'I-drug_n':3, 'O':4, \n",
    "                    'I-group':5, 'B-drug':6, 'I-drug':7, 'B-brand':8, 'I-brand':9}\n",
    "         'maxlen': 100 }\n",
    "    '''\n",
    "    \n",
    "    index_words = {'<PAD>':0, '<UNK>':1}\n",
    "    i = 2\n",
    "    \n",
    "    index_labels = {'<PAD>':0}\n",
    "    j = 1\n",
    "    \n",
    "    for key, item in dataset.items():\n",
    "        for t in item:\n",
    "            word = t[0].lower() # use lower case words? \n",
    "            tag = t[3]\n",
    "            if word not in index_words:\n",
    "                index_words[word] = i\n",
    "                i += 1\n",
    "            if tag not in index_labels:\n",
    "                index_labels[tag] = j\n",
    "                j += 1\n",
    "\n",
    "    indexs = {'words': index_words, 'labels': index_labels, 'maxlen':max_length}\n",
    "    \n",
    "    return indexs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " 'O': 1,\n",
       " 'B-brand': 2,\n",
       " 'B-drug': 3,\n",
       " 'B-group': 4,\n",
       " 'I-group': 5,\n",
       " 'I-drug': 6,\n",
       " 'I-brand': 7,\n",
       " 'B-drug_n': 8,\n",
       " 'I-drug_n': 9}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = create_index(train_dataset, 100)\n",
    "idx['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the words in a sentence dataset formed by lists of tokens into lists of indexes\n",
    "          suitable for NN input.\n",
    "    Input: \n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, as well\n",
    "             as the maximum sentence length.\n",
    "             \n",
    "    Output: The dataset encoded as a list of sentence, each of them is a list of word indices.\n",
    "            If the word is not in the index, <UNK> code is used. If the sentence is shorter than\n",
    "            max_len it is padded with <PAD> code.\n",
    "    Example: \n",
    "        >>> encode_words(traindata, idx)\n",
    "            [ [6882 1049 4911 ... 0 0 0 ]\n",
    "            [  2290 7548 8069 ... 0 0 0 ]\n",
    "               ...\n",
    "            [  2002 6582 7518 ... 0 0 0 ] ]\n",
    "    '''\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for key, item in dataset.items():\n",
    "        aux = []\n",
    "        for t in item:\n",
    "            w = str(t[0]).lower() # When using lower case words\n",
    "            if w in idx['words']:\n",
    "                i = idx['words'][w]\n",
    "            else:\n",
    "                i = idx['words']['<UNK>']\n",
    "            aux.append(i)\n",
    "        seq.append(aux)  \n",
    "    \n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    \n",
    "    return seq_padded\n",
    "\n",
    "def encode_labels(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the ground truth labels in a dataset formed by lists of tokens into lists of indexes\n",
    "        suitable for NN output.\n",
    "    Input:\n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_index, containing word and label indexes, as well as the maximum length.\n",
    "        \n",
    "    Output: The dataset encoded as a list of sentence, each of them is a list of BIO label indices. If the sentence\n",
    "            is shorter than max_len it is padded with <PAD> code. \n",
    "    \n",
    "    Example :\n",
    "     >>> encode_labels ( traindata , idx )\n",
    "        [[ [4] [6] [4] [4] [4] [4] ... [0] [0] ]\n",
    "        [  [4] [4] [8] [4] [6] [4] ... [0] [0] ]\n",
    "          ...\n",
    "        [\n",
    "    '''\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for key, item in dataset.items():\n",
    "        aux = []\n",
    "        for t in item:\n",
    "            w = t[3]\n",
    "            i = idx['labels'][w]\n",
    "            aux.append(i)\n",
    "        seq.append(aux)\n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    seq_categ = [to_categorical(i, num_classes = 10) for i in seq_padded]  # 9 classes + 1 PAD\n",
    "    \n",
    "    return seq_padded, seq_categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encode_words(train_dataset, idx)\n",
    "Y, Y_train = encode_labels(train_dataset, idx)\n",
    "\n",
    "X_dev = encode_words(devel_dataset, idx)\n",
    "Ydev, Y_dev = encode_labels(devel_dataset, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    3    4 ...    0    0    0]\n",
      " [   2   12    3 ...    0    0    0]\n",
      " [  19   20   21 ...    0    0    0]\n",
      " ...\n",
      " [   2  101 3387 ...    0    0    0]\n",
      " [1159  910 5135 ...    0    0    0]\n",
      " [  19  910 5135 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 1 1 3 1 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y[0])\n",
    "print(Y_train[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx):\n",
    "    '''\n",
    "    Task: Create network for the learner. \n",
    "    Input:\n",
    "        idx: index dictionary with word/labels codes, plus maximum sentence length.\n",
    "    Output: Returns a compiled Keras neural network with the specified layers\n",
    "    '''\n",
    "    \n",
    "    #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])#+1\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # create network layers\n",
    "    inp = Input(shape=(max_len,))\n",
    "    model = Embedding(input_dim=n_words + 1, output_dim = n_labels, input_length = max_len)(inp)\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "    out = TimeDistributed(Dense(n_labels, activation=\"softmax\"))(model)\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "    optimiz = Adam(lr=0.01, decay=1e-6)\n",
    "    model.compile(optimizer=optimiz, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosia/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f0831f727f0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0f4f563d2e74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-54326dffb8f0>\u001b[0m in \u001b[0;36mbuild_network\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0moptimiz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimiz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_eagerly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m       self.compiled_loss = compile_utils.LossesContainer(\n\u001b[1;32m    550\u001b[0m           loss, loss_weights, output_names=self.output_names)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_get_optimizer\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    584\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_single_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_get_single_optimizer\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_single_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m       \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m       if (loss_scale is not None and\n\u001b[1;32m    579\u001b[0m           not isinstance(opt, lso.LossScaleOptimizer)):\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    133\u001b[0m         'Could not interpret optimizer identifier: {}'.format(identifier))\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret optimizer identifier: <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f0831f727f0>"
     ]
    }
   ],
   "source": [
    "model = build_network(idx)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, np.array(Y_train),\n",
    "                   batch_size=32,\n",
    "                   epochs=2,\n",
    "                   verbose=1, \n",
    "                   validation_data=(X_dev, np.array(Y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_indexes(model, idx, filename):\n",
    "    '''\n",
    "    Task: Save given model and indexs to disk\n",
    "    Input: \n",
    "        model: Keras model created by _build_network, and trained.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, \n",
    "             as well as the maximum sentence length. \n",
    "        filename: Saves the mode into filename.nn and the indexes into filename.idx\n",
    "    '''\n",
    "    model.save(\"ner-nn.nn\")\n",
    "    \n",
    "    file = open(\"index_ner.pkl\", \"wb\")\n",
    "    pickle.dump(idx, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Learner()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner(traindir, validationdir):#, modelname):\n",
    "    '''\n",
    "    Learns a NN model using traindir as training data, and validationdir as validation data.\n",
    "    Saves learnt model in a file named modelname\n",
    "    '''\n",
    "    # load train and validation data in a suitable form\n",
    "    train_dataset = load_data(traindir)\n",
    "    val_dataset = load_data(validationdir)\n",
    "    \n",
    "    # create indexes from trainindg data \n",
    "    max_len = 100\n",
    "    idx = create_index(train_dataset, max_len)\n",
    "    \n",
    "    # build network \n",
    "    model = build_network(idx)\n",
    "    \n",
    "    # encode datasets\n",
    "    Xtrain = encode_words(train_dataset, idx)\n",
    "    Y, Ytrain = encode_labels(train_dataset, idx)\n",
    "    Xval = encode_words(val_dataset, idx)\n",
    "    Yv, Yval = encode_labels(val_dataset, idx)\n",
    "    \n",
    "    # train model\n",
    "    model.fit(Xtrain, np.array(Ytrain),\n",
    "              batch_size=32,\n",
    "              epochs=2,\n",
    "              verbose=1,\n",
    "              validation_data=(Xval, np.array(Yval)))\n",
    "    \n",
    "    # save model and indexs, for later use in prediction\n",
    "    save_model_and_indexs(model, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"../../labAHLT/data/train\"\n",
    "path_dev = \"../../labAHLT/data/devel\"\n",
    "learner(path_train, path_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Functions Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_indexs(filename):\n",
    "    '''\n",
    "    Task: Load model and associate indexs from disk.\n",
    "    Input:\n",
    "        filename: filename to be loaded\n",
    "    Output: Loads a model from filename.nn and its indexes from filename.idx\n",
    "            Returns the loaded model and indexes. \n",
    "    '''\n",
    "    model = load_model(\"ner-nn.nn\")\n",
    "    index = open(\"index_ner.pkl\", \"rb\")\n",
    "    idx = pickle.load(index)\n",
    "    \n",
    "    return model, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_entities(dataset, preds):\n",
    "    '''\n",
    "    Task: Output detected entities in the format expected by the evaluator\n",
    "    Input: \n",
    "        dataset: A dataset produced by load_data.\n",
    "        preds: For each sentence in dataset, a list with the labels for each sentence token, \n",
    "               as predicted by the model.\n",
    "    Output: prints the detected entities to stdout in the format required by the evaluator. \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = model.predict(X_dev)\n",
    "#Y = np.argmax(Y, axis=-1)\n",
    "key_list = list(idx['labels'].keys())\n",
    "val_list = list(idx['labels'].values())\n",
    "for y in Y:\n",
    "    for t in y:\n",
    "        if t in [2,3,4,5,6,7,8,9]: \n",
    "            print(key_list[t])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Classifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(modelname, datadir, outfile):\n",
    "    '''\n",
    "    Loads a NN model from a file 'modelname' and uses it to extract drugs in datadir. Saves\n",
    "    results to 'outfile' in the appropriate format\n",
    "    '''\n",
    "    \n",
    "    # load model and associated encoding data\n",
    "    model, idx = load_model_and_indexs(modelname)\n",
    "    \n",
    "    # load data to annotate\n",
    "    testdata = load_data(datadir)\n",
    "    \n",
    "    # encode dataset\n",
    "    X = encode_words(testdata, idx)\n",
    "    \n",
    "    # tag sentences in dataset\n",
    "    Y = model.predict(X)\n",
    "    # get most likely tag for each word\n",
    "    Y = [[idx['labels'][np.argmax(y)] for y in s] for s in Y]\n",
    "    \n",
    "    # extract entities and dump them to output file\n",
    "    output_entities(testdata, Y, outfile)\n",
    "    \n",
    "    # evaluate using official evaluator\n",
    "    evaluation(datadir, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
