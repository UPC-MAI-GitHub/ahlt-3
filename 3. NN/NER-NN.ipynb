{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "import pickle\n",
    "from xml.dom.minidom import parse\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import *\n",
    "#from keras_contrib.layers import CRF\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "from evaluator import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    '''\n",
    "    Task: Load XML files in given directory, tokenize each sentence, and extract\n",
    "    ground truth BIO labels for each token.\n",
    "    \n",
    "    Input: \n",
    "        datadir: A directory containing XML files\n",
    "        \n",
    "    Output: A directory containing the dataset. Dictionary key is sentence_id, and the \n",
    "            value is a list of token tuples (word, start, end, ground truth).\n",
    "            \n",
    "    Example: \n",
    "            >>> load_data('data/Train')\n",
    "            {'DDI-DrugBank.d370.s0': [('as', 0, 1,'O'), ('differin', 3, 10,'B-brand'),\n",
    "                     ('gel', 12, 14,'O'), ... , ('with', 343, 346, 'O'),\n",
    "                     ('caution', 348, 354, 'O'), ('.', 355, 355, 'O')],\n",
    "            'DDI-DrugBank.d370.s1': [('particular', 0, 9, 'O'), ('caution', 11, 17, 'O'),\n",
    "                     ('should', 19, 24, 'O'), ... , ('differin', 130, 137, 'B-brand'),\n",
    "                     ('gel', 139, 141, 'O'), ('.', 142, 142, 'O')], ... }\n",
    "    '''\n",
    "    \n",
    "    dict_dataset = {}\n",
    "    \n",
    "    for f in listdir(datadir):\n",
    "        tree = parse(datadir + \"/\" + f)\n",
    "        \n",
    "        sentences = tree.getElementsByTagName(\"sentence\")\n",
    "        for s in sentences: \n",
    "            sid = s.attributes[\"id\"].value\n",
    "            \n",
    "            stext = s.attributes[\"text\"].value\n",
    "            #stext = stext.replace(\"-\",\" \") if used we lose beta-endorphin, if not use we lose calcium-rich tag\n",
    "            \n",
    "            ents = {}\n",
    "            entities = s.getElementsByTagName(\"entity\")\n",
    "            for e in entities:\n",
    "                offset = e.attributes[\"charOffset\"].value\n",
    "                start = offset.split('-')[0]\n",
    "                end = offset.split('-')[1]\n",
    "                name = e.attributes[\"text\"].value\n",
    "                e_type = e.attributes[\"type\"].value\n",
    "                ents[name] = [e_type, start, end]\n",
    "            \n",
    "            punct = [\",\",\";\",\":\",\"?\",\"!\", \"(\", \")\"] # removed \".\"\n",
    "            tokens = word_tokenize(stext)\n",
    "            tokens_cleaned = []\n",
    "            for t in tokens:\n",
    "                if t not in punct:\n",
    "                    tokens_cleaned.append(t)\n",
    "            \n",
    "            tags = []\n",
    "            tokens = []\n",
    "            for t in tokens_cleaned:\n",
    "                offsetFrom = stext.find(t)\n",
    "                offsetTo = offsetFrom + len(t) - 1\n",
    "                if t in ents:\n",
    "                    if (int(ents[t][1]) == 0):\n",
    "                        tag = \"B-\"+ents[t][0] # TODO: ents after .?\n",
    "                    else:\n",
    "                        tag = \"I-\"+ents[t][0]\n",
    "                else:\n",
    "                    tag = \"O\"\n",
    "                tags.append(tag)\n",
    "                tupl = (t, offsetFrom, offsetTo, tag)\n",
    "                tokens.append(tupl)\n",
    "            \n",
    "            dict_dataset[sid] = tokens\n",
    "        \n",
    "    return dict_dataset\n",
    "\n",
    "# -- TODO: handle multi-token entities\n",
    "# tricyclic antidepressants - 'O' should be Group\n",
    "# chondroitin ABC lyase - 'O' should be drug_n\n",
    "# heparinase III - 'O' should be drug_n\n",
    "# hyaluronan lyase - 'O' should be drug_n\n",
    "# Mercaptopurine/Azathioprine - 'O' should be drug\n",
    "# muscle relaxants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Interactions', 0, 11, 'O'), ('have', 13, 16, 'O'), ('been', 18, 21, 'O'), ('observed', 23, 30, 'O'), ('when', 32, 35, 'O'), ('other', 37, 41, 'O'), ('nondepolarizing', 43, 57, 'O'), ('muscle', 59, 64, 'O'), ('relaxants', 66, 74, 'O'), ('have', 13, 16, 'O'), ('been', 18, 21, 'O'), ('administered', 86, 97, 'O'), ('in', 55, 56, 'O'), ('succession', 102, 111, 'O'), ('.', 112, 112, 'O')]\n"
     ]
    }
   ],
   "source": [
    "path_train = \"../../labAHLT/data/train\"\n",
    "path_dev = \"../../labAHLT/data/devel\"\n",
    "\n",
    "train_dataset = load_data(path_train)\n",
    "devel_dataset = load_data(path_dev)\n",
    "\n",
    "print(train_dataset['DDI-DrugBank.d661.s5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(dataset, max_length):\n",
    "    '''\n",
    "    Task: Create index dictionaries both for input (words) and output (labels) from given dataset\n",
    "    Input: \n",
    "        dataset: dataset produced by load_data.\n",
    "        max_length: maximum length of a sentence (longer sentences will be cut, shorter ones will be padded).\n",
    "        \n",
    "    Output: A dictionary where each key is an index name (e.g. \"words\", \"labels\"), and the value is a \n",
    "            dictionary mapping each word/label to a number. An entry with the value for maxlen is also stored\n",
    "    Example: \n",
    "        >>> create_indx(traindata)\n",
    "        {'words': {'<PAD>':0, '<UNK>':1, '11-day':2, 'murine':3, 'criteria':4,\n",
    "                   'stroke':5, ... ,'levodopa':8511, 'terfenadine': 8512}\n",
    "         'labels': {'<PAD>':0, 'B-group':1, 'B-drug_n':2, 'I-drug_n':3, 'O':4, \n",
    "                    'I-group':5, 'B-drug':6, 'I-drug':7, 'B-brand':8, 'I-brand':9}\n",
    "         'maxlen': 100 }\n",
    "    '''\n",
    "    \n",
    "    index_words = {'<PAD>':0, '<UNK>':1}\n",
    "    i = 2\n",
    "    \n",
    "    index_labels = {'<PAD>':0}\n",
    "    j = 1\n",
    "    \n",
    "    for key, item in dataset.items():\n",
    "        for t in item:\n",
    "            word = t[0].lower() # use lower case words? \n",
    "            tag = t[3]\n",
    "            if word not in index_words:\n",
    "                index_words[word] = i\n",
    "                i += 1\n",
    "            if tag not in index_labels:\n",
    "                index_labels[tag] = j\n",
    "                j += 1\n",
    "\n",
    "    indexs = {'words': index_words, 'labels': index_labels, 'maxlen':max_length}\n",
    "    \n",
    "    return indexs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " 'O': 1,\n",
       " 'I-brand': 2,\n",
       " 'I-group': 3,\n",
       " 'I-drug': 4,\n",
       " 'I-drug_n': 5,\n",
       " 'B-drug_n': 6,\n",
       " 'B-brand': 7,\n",
       " 'B-drug': 8,\n",
       " 'B-group': 9}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = create_index(train_dataset, 100)\n",
    "idx['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the words in a sentence dataset formed by lists of tokens into lists of indexes\n",
    "          suitable for NN input.\n",
    "    Input: \n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, as well\n",
    "             as the maximum sentence length.\n",
    "             \n",
    "    Output: The dataset encoded as a list of sentence, each of them is a list of word indices.\n",
    "            If the word is not in the index, <UNK> code is used. If the sentence is shorter than\n",
    "            max_len it is padded with <PAD> code.\n",
    "    Example: \n",
    "        >>> encode_words(traindata, idx)\n",
    "            [ [6882 1049 4911 ... 0 0 0 ]\n",
    "            [  2290 7548 8069 ... 0 0 0 ]\n",
    "               ...\n",
    "            [  2002 6582 7518 ... 0 0 0 ] ]\n",
    "    '''\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for key, item in dataset.items():\n",
    "        aux = []\n",
    "        for t in item:\n",
    "            w = str(t[0]).lower() # When using lower case words\n",
    "            if w in idx['words']:\n",
    "                i = idx['words'][w]\n",
    "            else:\n",
    "                i = idx['words']['<UNK>']\n",
    "            aux.append(i)\n",
    "        seq.append(aux)  \n",
    "    \n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    \n",
    "    return seq_padded\n",
    "\n",
    "def encode_labels(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the ground truth labels in a dataset formed by lists of tokens into lists of indexes\n",
    "        suitable for NN output.\n",
    "    Input:\n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_index, containing word and label indexes, as well as the maximum length.\n",
    "        \n",
    "    Output: The dataset encoded as a list of sentence, each of them is a list of BIO label indices. If the sentence\n",
    "            is shorter than max_len it is padded with <PAD> code. \n",
    "    \n",
    "    Example :\n",
    "     >>> encode_labels ( traindata , idx )\n",
    "        [[ [4] [6] [4] [4] [4] [4] ... [0] [0] ]\n",
    "        [  [4] [4] [8] [4] [6] [4] ... [0] [0] ]\n",
    "          ...\n",
    "        [\n",
    "    '''\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for key, item in dataset.items():\n",
    "        aux = []\n",
    "        for t in item:\n",
    "            w = t[3]\n",
    "            i = idx['labels'][w]\n",
    "            aux.append(i)\n",
    "        seq.append(aux)\n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    seq_categ = [to_categorical(i, num_classes = 10) for i in seq_padded]  # 9 classes + 1 PAD\n",
    "    \n",
    "    return seq_padded, seq_categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encode_words(train_dataset, idx)\n",
    "Y, Y_train = encode_labels(train_dataset, idx)\n",
    "\n",
    "X_dev = encode_words(devel_dataset, idx)\n",
    "Ydev, Y_dev = encode_labels(devel_dataset, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    2    3 ...    0    0    0]\n",
      " [  16   17   18 ...    0    0    0]\n",
      " [  25   26   27 ...    0    0    0]\n",
      " ...\n",
      " [1000   70   47 ...    0    0    0]\n",
      " [ 187  958  959 ...    0    0    0]\n",
      " [   9 1505  180 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y[0])\n",
    "print(Y_train[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx):\n",
    "    '''\n",
    "    Task: Create network for the learner. \n",
    "    Input:\n",
    "        idx: index dictionary with word/labels codes, plus maximum sentence length.\n",
    "    Output: Returns a compiled Keras neural network with the specified layers\n",
    "    '''\n",
    "    \n",
    "    #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])#+1\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # create network layers\n",
    "    inp = Input(shape=(max_len,))\n",
    "    model = Embedding(input_dim=n_words + 1, output_dim = n_labels, input_length = max_len)(inp)\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "    out = TimeDistributed(Dense(n_labels, activation=\"softmax\"))(model)\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "    optimiz = Adam(lr=0.01, decay=1e-6)\n",
    "    model.compile(optimizer=optimiz, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 100, 10)           83410     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100, 10)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 100, 200)          88800     \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 100, 10)           2010      \n",
      "=================================================================\n",
      "Total params: 174,220\n",
      "Trainable params: 174,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_network(idx)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "170/170 [==============================] - 19s 93ms/step - loss: 0.3138 - accuracy: 0.9178 - val_loss: 0.0366 - val_accuracy: 0.9886\n",
      "Epoch 2/2\n",
      "170/170 [==============================] - 17s 99ms/step - loss: 0.0259 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9906\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, np.array(Y_train),\n",
    "                   batch_size=32,\n",
    "                   epochs=2,\n",
    "                   verbose=1, \n",
    "                   validation_data=(X_dev, np.array(Y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_indexes(model, idx, filename):\n",
    "    '''\n",
    "    Task: Save given model and indexs to disk\n",
    "    Input: \n",
    "        model: Keras model created by _build_network, and trained.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, \n",
    "             as well as the maximum sentence length. \n",
    "        filename: Saves the mode into filename.nn and the indexes into filename.idx\n",
    "    '''\n",
    "    model.save(\"ner-nn.nn\")\n",
    "    \n",
    "    file = open(\"index_ner.pkl\", \"wb\")\n",
    "    pickle.dump(idx, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Learner()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner(traindir, validationdir):#, modelname):\n",
    "    '''\n",
    "    Learns a NN model using traindir as training data, and validationdir as validation data.\n",
    "    Saves learnt model in a file named modelname\n",
    "    '''\n",
    "    # load train and validation data in a suitable form\n",
    "    train_dataset = load_data(traindir)\n",
    "    val_dataset = load_data(validationdir)\n",
    "    \n",
    "    # create indexes from trainindg data \n",
    "    max_len = 100\n",
    "    idx = create_index(train_dataset, max_len)\n",
    "    \n",
    "    # build network \n",
    "    model = build_network(idx)\n",
    "    \n",
    "    # encode datasets\n",
    "    Xtrain = encode_words(train_dataset, idx)\n",
    "    Y, Ytrain = encode_labels(train_dataset, idx)\n",
    "    Xval = encode_words(val_dataset, idx)\n",
    "    Yv, Yval = encode_labels(val_dataset, idx)\n",
    "    \n",
    "    # train model\n",
    "    model.fit(Xtrain, np.array(Ytrain),\n",
    "              batch_size=32,\n",
    "              epochs=2,\n",
    "              verbose=1,\n",
    "              validation_data=(Xval, np.array(Yval)))\n",
    "    \n",
    "    # save model and indexs, for later use in prediction\n",
    "    save_model_and_indexs(model, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "170/170 [==============================] - 20s 101ms/step - loss: 0.2991 - accuracy: 0.9194 - val_loss: 0.0331 - val_accuracy: 0.9895\n",
      "Epoch 2/2\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.0212 - accuracy: 0.9937 - val_loss: 0.0242 - val_accuracy: 0.9924\n"
     ]
    }
   ],
   "source": [
    "path_train = \"../../labAHLT/data/train\"\n",
    "path_dev = \"../../labAHLT/data/devel\"\n",
    "learner(path_train, path_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Functions Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_indexs(filename):\n",
    "    '''\n",
    "    Task: Load model and associate indexs from disk.\n",
    "    Input:\n",
    "        filename: filename to be loaded\n",
    "    Output: Loads a model from filename.nn and its indexes from filename.idx\n",
    "            Returns the loaded model and indexes. \n",
    "    '''\n",
    "    model = load_model(\"ner-nn.nn\")\n",
    "    index = open(\"index_ner.pkl\", \"rb\")\n",
    "    idx = pickle.load(index)\n",
    "    \n",
    "    return model, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_entities(dataset, preds):\n",
    "    '''\n",
    "    Task: Output detected entities in the format expected by the evaluator\n",
    "    Input: \n",
    "        dataset: A dataset produced by load_data.\n",
    "        preds: For each sentence in dataset, a list with the labels for each sentence token, \n",
    "               as predicted by the model.\n",
    "    Output: prints the detected entities to stdout in the format required by the evaluator. \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = model.predict(X_dev)\n",
    "#Y = np.argmax(Y, axis=-1)\n",
    "key_list = list(idx['labels'].keys())\n",
    "val_list = list(idx['labels'].values())\n",
    "for y in Y:\n",
    "    for t in y:\n",
    "        if t in [2,3,4,5,6,7,8,9]: \n",
    "            print(key_list[t])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Classifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(modelname, datadir, outfile):\n",
    "    '''\n",
    "    Loads a NN model from a file 'modelname' and uses it to extract drugs in datadir. Saves\n",
    "    results to 'outfile' in the appropriate format\n",
    "    '''\n",
    "    \n",
    "    # load model and associated encoding data\n",
    "    model, idx = load_model_and_indexs(modelname)\n",
    "    \n",
    "    # load data to annotate\n",
    "    testdata = load_data(datadir)\n",
    "    \n",
    "    # encode dataset\n",
    "    X = encode_words(testdata, idx)\n",
    "    \n",
    "    # tag sentences in dataset\n",
    "    Y = model.predict(X)\n",
    "    # get most likely tag for each word\n",
    "    Y = [[idx['labels'][np.argmax(y)] for y in s] for s in Y]\n",
    "    \n",
    "    # extract entities and dump them to output file\n",
    "    output_entities(testdata, Y, outfile)\n",
    "    \n",
    "    # evaluate using official evaluator\n",
    "    evaluation(datadir, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
