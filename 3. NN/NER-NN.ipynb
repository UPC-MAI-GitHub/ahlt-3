{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "import pickle\n",
    "from xml.dom.minidom import parse\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import *\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "from evaluator import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_multi_token_entity(ents, token):\n",
    "    keys = ents.keys()\n",
    "    for k in keys:\n",
    "        if token in k:\n",
    "            return ents[k]\n",
    "\n",
    "def load_data(datadir):\n",
    "    '''\n",
    "    Task: Load XML files in given directory, tokenize each sentence, and extract\n",
    "    ground truth BIO labels for each token.\n",
    "    \n",
    "    Input: \n",
    "        datadir: A directory containing XML files\n",
    "        \n",
    "    Output: A directory containing the dataset. Dictionary key is sentence_id, and the \n",
    "            value is a list of token tuples (word, start, end, ground truth).\n",
    "            \n",
    "    Example: \n",
    "            >>> load_data('data/Train')\n",
    "            {'DDI-DrugBank.d370.s0': [('as', 0, 1,'O'), ('differin', 3, 10,'B-brand'),\n",
    "                     ('gel', 12, 14,'O'), ... , ('with', 343, 346, 'O'),\n",
    "                     ('caution', 348, 354, 'O'), ('.', 355, 355, 'O')],\n",
    "            'DDI-DrugBank.d370.s1': [('particular', 0, 9, 'O'), ('caution', 11, 17, 'O'),\n",
    "                     ('should', 19, 24, 'O'), ... , ('differin', 130, 137, 'B-brand'),\n",
    "                     ('gel', 139, 141, 'O'), ('.', 142, 142, 'O')], ... }\n",
    "    '''\n",
    "    \n",
    "    dict_dataset = {}\n",
    "    \n",
    "    for f in listdir(datadir):\n",
    "        tree = parse(datadir + \"/\" + f)\n",
    "        \n",
    "        sentences = tree.getElementsByTagName(\"sentence\")\n",
    "        for s in sentences: \n",
    "            sid = s.attributes[\"id\"].value\n",
    "            \n",
    "            stext = s.attributes[\"text\"].value\n",
    "            #stext = stext.replace(\"-\",\" \") if used we lose beta-endorphin, if not use we lose calcium-rich tag\n",
    "            \n",
    "            ents = {}\n",
    "            entities = s.getElementsByTagName(\"entity\")\n",
    "            for e in entities:\n",
    "                offset = e.attributes[\"charOffset\"].value\n",
    "                offsets = offset.split(';')\n",
    "                name = e.attributes[\"text\"].value\n",
    "                e_type = e.attributes[\"type\"].value\n",
    "                ents[name] = {\"type\": e_type, \"offsets\": []}\n",
    "                for offset in offsets:\n",
    "                    start = offset.split('-')[0]\n",
    "                    end = offset.split('-')[1]\n",
    "                    ents[name][\"offsets\"].append((start, end))\n",
    "            \n",
    "            punct = [\",\",\";\",\":\",\"?\",\"!\", \"(\", \")\"] # removed \".\"\n",
    "            tokens = word_tokenize(stext)\n",
    "            tokens_cleaned = []\n",
    "            for t in tokens:\n",
    "                if t not in punct and t not in stopwords:\n",
    "                    tokens_cleaned.append(t)\n",
    "            \n",
    "            tags = []\n",
    "            tokens = []\n",
    "            for t in tokens_cleaned:\n",
    "                offsetFrom = stext.find(t)\n",
    "                offsetTo = offsetFrom + len(t) - 1\n",
    "                # 1-token entities\n",
    "                if t in ents:\n",
    "                    if (int(ents[t][\"offsets\"][0][0]) == offsetFrom):\n",
    "                        tag = \"B-\"+ents[t][\"type\"] # TODO: ents after .?\n",
    "                    else:\n",
    "                        tag = \"I-\"+ents[t][\"type\"]                    \n",
    "                else:\n",
    "                    multi_token_ent = find_multi_token_entity(ents, t)\n",
    "                    if multi_token_ent:\n",
    "                        if (int(multi_token_ent[\"offsets\"][0][0]) == offsetFrom):\n",
    "                            tag = \"B-\"+multi_token_ent[\"type\"] # TODO: ents after .?\n",
    "                        else:\n",
    "                            tag = \"I-\"+multi_token_ent[\"type\"]\n",
    "                    else:\n",
    "                        tag = \"O\"\n",
    "                tags.append(tag)\n",
    "                tupl = (t, offsetFrom, offsetTo, tag)\n",
    "                tokens.append(tupl)\n",
    "            \n",
    "            dict_dataset[sid] = tokens\n",
    "        \n",
    "    return dict_dataset\n",
    "\n",
    "# -- TODO: handle multi-token entities # I think I've done it :D\n",
    "# tricyclic antidepressants - 'O' should be Group\n",
    "# chondroitin ABC lyase - 'O' should be drug_n\n",
    "# heparinase III - 'O' should be drug_n\n",
    "# hyaluronan lyase - 'O' should be drug_n\n",
    "# Mercaptopurine/Azathioprine - 'O' should be drug\n",
    "# muscle relaxants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Interactions', 0, 11, 'O'), ('observed', 23, 30, 'O'), ('nondepolarizing', 43, 57, 'B-group'), ('muscle', 59, 64, 'I-group'), ('relaxants', 66, 74, 'I-group'), ('administered', 86, 97, 'O'), ('succession', 102, 111, 'O'), ('.', 112, 112, 'O')]\n"
     ]
    }
   ],
   "source": [
    "path_train = \"../../labAHLT/data/train\"\n",
    "path_dev = \"../../labAHLT/data/devel\"\n",
    "\n",
    "train_dataset = load_data(path_train)\n",
    "devel_dataset = load_data(path_dev)\n",
    "\n",
    "print(train_dataset['DDI-DrugBank.d661.s5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(dataset, max_length):\n",
    "    '''\n",
    "    Task: Create index dictionaries both for input (words) and output (labels) from given dataset\n",
    "    Input: \n",
    "        dataset: dataset produced by load_data.\n",
    "        max_length: maximum length of a sentence (longer sentences will be cut, shorter ones will be padded).\n",
    "        \n",
    "    Output: A dictionary where each key is an index name (e.g. \"words\", \"labels\"), and the value is a \n",
    "            dictionary mapping each word/label to a number. An entry with the value for maxlen is also stored\n",
    "    Example: \n",
    "        >>> create_indx(traindata)\n",
    "        {'words': {'<PAD>':0, '<UNK>':1, '11-day':2, 'murine':3, 'criteria':4,\n",
    "                   'stroke':5, ... ,'levodopa':8511, 'terfenadine': 8512}\n",
    "         'labels': {'<PAD>':0, 'B-group':1, 'B-drug_n':2, 'I-drug_n':3, 'O':4, \n",
    "                    'I-group':5, 'B-drug':6, 'I-drug':7, 'B-brand':8, 'I-brand':9}\n",
    "         'maxlen': 100 }\n",
    "    '''\n",
    "    \n",
    "    index_words = {'<PAD>':0, '<UNK>':1}\n",
    "    i = 2\n",
    "    \n",
    "    index_labels = {'<PAD>':0}\n",
    "    j = 1\n",
    "    \n",
    "    for key, item in dataset.items():\n",
    "        for t in item:\n",
    "            word = t[0].lower() # use lower case words? \n",
    "            tag = t[3]\n",
    "            if word not in index_words:\n",
    "                index_words[word] = i\n",
    "                i += 1\n",
    "            if tag not in index_labels:\n",
    "                index_labels[tag] = j\n",
    "                j += 1\n",
    "\n",
    "    indexs = {'words': index_words, 'labels': index_labels, 'maxlen':max_length}\n",
    "    \n",
    "    return indexs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " 'O': 1,\n",
       " 'B-brand': 2,\n",
       " 'B-group': 3,\n",
       " 'B-drug': 4,\n",
       " 'I-brand': 5,\n",
       " 'B-drug_n': 6,\n",
       " 'I-drug': 7,\n",
       " 'I-group': 8,\n",
       " 'I-drug_n': 9}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = create_index(train_dataset, 100)\n",
    "idx['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the words in a sentence dataset formed by lists of tokens into lists of indexes\n",
    "          suitable for NN input.\n",
    "    Input: \n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, as well\n",
    "             as the maximum sentence length.\n",
    "             \n",
    "    Output: The dataset encoded as a list of sentence, each of them is a list of word indices.\n",
    "            If the word is not in the index, <UNK> code is used. If the sentence is shorter than\n",
    "            max_len it is padded with <PAD> code.\n",
    "    Example: \n",
    "        >>> encode_words(traindata, idx)\n",
    "            [ [6882 1049 4911 ... 0 0 0 ]\n",
    "            [  2290 7548 8069 ... 0 0 0 ]\n",
    "               ...\n",
    "            [  2002 6582 7518 ... 0 0 0 ] ]\n",
    "    '''\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for key, item in dataset.items():\n",
    "        aux = []\n",
    "        for t in item:\n",
    "            w = str(t[0]).lower() # When using lower case words\n",
    "            if w in idx['words']:\n",
    "                i = idx['words'][w]\n",
    "            else:\n",
    "                i = idx['words']['<UNK>']\n",
    "            aux.append(i)\n",
    "        seq.append(aux)  \n",
    "    \n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    \n",
    "    return seq_padded\n",
    "\n",
    "def encode_labels(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the ground truth labels in a dataset formed by lists of tokens into lists of indexes\n",
    "        suitable for NN output.\n",
    "    Input:\n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_index, containing word and label indexes, as well as the maximum length.\n",
    "        \n",
    "    Output: The dataset encoded as a list of sentence, each of them is a list of BIO label indices. If the sentence\n",
    "            is shorter than max_len it is padded with <PAD> code. \n",
    "    \n",
    "    Example :\n",
    "     >>> encode_labels ( traindata , idx )\n",
    "        [[ [4] [6] [4] [4] [4] [4] ... [0] [0] ]\n",
    "        [  [4] [4] [8] [4] [6] [4] ... [0] [0] ]\n",
    "          ...\n",
    "        [\n",
    "    '''\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for key, item in dataset.items():\n",
    "        aux = []\n",
    "        for t in item:\n",
    "            w = t[3]\n",
    "            i = idx['labels'][w]\n",
    "            aux.append(i)\n",
    "        seq.append(aux)\n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    seq_categ = [to_categorical(i, num_classes = 10) for i in seq_padded]  # 9 classes + 1 PAD\n",
    "    \n",
    "    return seq_padded, seq_categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encode_words(train_dataset, idx)\n",
    "Y, Y_train = encode_labels(train_dataset, idx)\n",
    "\n",
    "X_dev = encode_words(devel_dataset, idx)\n",
    "Ydev, Y_dev = encode_labels(devel_dataset, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    2    3 ...    0    0    0]\n",
      " [  12   13    6 ...    0    0    0]\n",
      " [  16   17   18 ...    0    0    0]\n",
      " ...\n",
      " [ 941  128   31 ...    0    0    0]\n",
      " [ 154  898  899 ...    0    0    0]\n",
      " [   7 1451 3154 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y[0])\n",
    "print(Y_train[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx):\n",
    "    '''\n",
    "    Task: Create network for the learner. \n",
    "    Input:\n",
    "        idx: index dictionary with word/labels codes, plus maximum sentence length.\n",
    "    Output: Returns a compiled Keras neural network with the specified layers\n",
    "    '''\n",
    "    \n",
    "    #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])#+1\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # create network layers\n",
    "    inp = Input(shape=(max_len,))\n",
    "    model = Embedding(input_dim=n_words + 1, output_dim = n_labels, input_length = max_len)(inp)\n",
    "    model = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(model)\n",
    "    model2 = Bidirectional(LSTM(units=512, return_sequences=True,recurrent_dropout=0.2, dropout=0.2))(model)\n",
    "    model = add([model, model2])\n",
    "    out = TimeDistributed(Dense(n_labels, activation=\"softmax\"))(model)\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "#     optimiz = Adam(lr=0.01, decay=1e-6)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx):\n",
    "    '''\n",
    "    Task: Create network for the learner. \n",
    "    Input:\n",
    "        idx: index dictionary with word/labels codes, plus maximum sentence length.\n",
    "    Output: Returns a compiled Keras neural network with the specified layers\n",
    "    '''\n",
    "    \n",
    "    #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])#+1\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # create network layers\n",
    "    inp = Input(shape=(max_len,))\n",
    "    model = Embedding(input_dim=n_words + 1, output_dim = n_labels, input_length = max_len)(inp)\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.2))(model)\n",
    "    out = TimeDistributed(Dense(n_labels, activation=\"softmax\"))(model)\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "#     optimiz = Adam(lr=0.01, decay=1e-6)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 10)      83050       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 100, 1024)    2142208     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100, 1024)    6295552     bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 100, 1024)    0           bidirectional[0][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 100, 10)      10250       add[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 8,531,060\n",
      "Trainable params: 8,531,060\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_network(idx)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, np.array(Y_train),\n",
    "                   batch_size=32,\n",
    "                   epochs=2,\n",
    "                   verbose=1, \n",
    "                   validation_data=(X_dev, np.array(Y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_indexes(model, idx):#, filename):\n",
    "    '''\n",
    "    Task: Save given model and indexs to disk\n",
    "    Input: \n",
    "        model: Keras model created by _build_network, and trained.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, \n",
    "             as well as the maximum sentence length. \n",
    "        filename: Saves the mode into filename.nn and the indexes into filename.idx\n",
    "    '''\n",
    "    model.save(\"ner-nn.nn\")\n",
    "    \n",
    "    file = open(\"index_ner.pkl\", \"wb\")\n",
    "    pickle.dump(idx, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Learner()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner(traindir, validationdir):#, modelname):\n",
    "    '''\n",
    "    Learns a NN model using traindir as training data, and validationdir as validation data.\n",
    "    Saves learnt model in a file named modelname\n",
    "    '''\n",
    "    # load train and validation data in a suitable form\n",
    "    train_dataset = load_data(traindir)\n",
    "    val_dataset = load_data(validationdir)\n",
    "    \n",
    "    # create indexes from trainindg data \n",
    "    max_len = 100\n",
    "    idx = create_index(train_dataset, max_len)\n",
    "    \n",
    "    # build network \n",
    "    model = build_network(idx)\n",
    "    \n",
    "    # encode datasets\n",
    "    Xtrain = encode_words(train_dataset, idx)\n",
    "    Y, Ytrain = encode_labels(train_dataset, idx)\n",
    "    Xval = encode_words(val_dataset, idx)\n",
    "    Yv, Yval = encode_labels(val_dataset, idx)\n",
    "    \n",
    "    # train model\n",
    "    model.fit(Xtrain, np.array(Ytrain),\n",
    "              batch_size=32,\n",
    "              epochs=2,\n",
    "              verbose=1,\n",
    "              validation_data=(Xval, np.array(Yval)))\n",
    "    \n",
    "    # save model and indexs, for later use in prediction\n",
    "    save_model_and_indexes(model, idx)#, path_dev.split('/')[-1]+\"NER-learned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "170/170 [==============================] - 918s 5s/step - loss: 0.3656 - accuracy: 0.9074 - val_loss: 0.1073 - val_accuracy: 0.9744\n",
      "Epoch 2/2\n",
      "170/170 [==============================] - 1031s 6s/step - loss: 0.1136 - accuracy: 0.9725 - val_loss: 0.1018 - val_accuracy: 0.9744\n",
      "INFO:tensorflow:Assets written to: ner-nn.nn/assets\n"
     ]
    }
   ],
   "source": [
    "path_train = \"../../labAHLT/data/train\"\n",
    "path_dev = \"../../labAHLT/data/devel\"\n",
    "learner(path_train, path_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Functions Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_indexs():\n",
    "    '''\n",
    "    Task: Load model and associate indexs from disk.\n",
    "    Input:\n",
    "        filename: filename to be loaded\n",
    "    Output: Loads a model from filename.nn and its indexes from filename.idx\n",
    "            Returns the loaded model and indexes. \n",
    "    '''\n",
    "    model = load_model(\"ner-nn.nn\")\n",
    "    index = open(\"index_ner.pkl\", \"rb\")\n",
    "    idx = pickle.load(index)\n",
    "    \n",
    "    return model, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_entities(dataset, preds, outfile):\n",
    "    '''\n",
    "    Task: Output detected entities in the format expected by the evaluator\n",
    "    Input: \n",
    "        dataset: A dataset produced by load_data.\n",
    "        preds: For each sentence in dataset, a list with the labels for each sentence token, \n",
    "               as predicted by the model.\n",
    "    Output: prints the detected entities to stdout in the format required by the evaluator. \n",
    "    '''\n",
    "    outf = open(outfile, 'w')\n",
    "    for sentence, pred in zip(dataset.items(), preds):\n",
    "#         print(sentence, pred)\n",
    "        sid = sentence[0]\n",
    "        for token, label in zip(sentence[1], pred):\n",
    "            if label != '<PAD>': # and label != 'O': It only predicts 'O'\n",
    "                offset_from = str(token[1])\n",
    "                offset_to = str(token[2])\n",
    "                entity = token[0]\n",
    "                tag_name = label\n",
    "                outf.write(sid + \"|\" + offset_from + '-' + offset_to + \"|\" + entity + \"|\" + tag_name)\n",
    "                print(sid + \"|\" + offset_from + '-' + offset_to + \"|\" + entity + \"|\" + tag_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = model.predict(X_dev)\n",
    "#Y = np.argmax(Y, axis=-1)\n",
    "key_list = list(idx['labels'].keys())\n",
    "val_list = list(idx['labels'].values())\n",
    "for y in Y:\n",
    "    for t in y:\n",
    "        if t in [2,3,4,5,6,7,8,9]: \n",
    "            print(key_list[t])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Classifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(datadir, outfile):\n",
    "    '''\n",
    "    Loads a NN model from a file 'modelname' and uses it to extract drugs in datadir. Saves\n",
    "    results to 'outfile' in the appropriate format\n",
    "    '''\n",
    "    \n",
    "    # load model and associated encoding data\n",
    "    model, idx = load_model_and_indexs()\n",
    "    \n",
    "    # load data to annotate\n",
    "    testdata = load_data(datadir)\n",
    "    \n",
    "    # encode dataset\n",
    "    X = encode_words(testdata, idx)\n",
    "    \n",
    "    # tag sentences in dataset\n",
    "    Y = model.predict(X)\n",
    "    Y = [[find_label(idx, np.argmax(y)) for y in s] for s in Y]\n",
    "    \n",
    "    # extract entities and dump them to output file\n",
    "    output_entities(testdata, Y, outfile)\n",
    "    \n",
    "    # evaluate using official evaluator\n",
    "    evaluate(\"NER\", datadir, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label(idx, predicted):\n",
    "    for label, i in idx[\"labels\"].items():\n",
    "        if i == predicted:\n",
    "            return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
      "------------------------------------------------------------------------------\n",
      "brand               0\t   0\t 288\t   0\t 288\t0.0%\t0.0%\t0.0%\n",
      "drug                0\t   0\t2120\t   0\t2120\t0.0%\t0.0%\t0.0%\n",
      "drug_n              0\t   0\t  72\t   0\t  72\t0.0%\t0.0%\t0.0%\n",
      "group               0\t   0\t 699\t   0\t 699\t0.0%\t0.0%\t0.0%\n",
      "------------------------------------------------------------------------------\n",
      "M.avg            -\t-\t-\t-\t-\t0.0%\t0.0%\t0.0%\n",
      "------------------------------------------------------------------------------\n",
      "m.avg               0\t   0\t3179\t   0\t3179\t0.0%\t0.0%\t0.0%\n",
      "m.avg(no class)     0\t   0\t3179\t   0\t3179\t0.0%\t0.0%\t0.0%\n"
     ]
    }
   ],
   "source": [
    "path_test = \"../../labAHLT/data/test\"\n",
    "predict(path_test, \"NER-result2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, idx = load_model_and_indexs()\n",
    "testdata = load_data(path_test)\n",
    "X = encode_words(testdata, idx)\n",
    "Y = model.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
