{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDI-ML. Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "cd into ../stanford-corenlp-4.2.0 and run\n",
    "\n",
    "`java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "import networkx\n",
    "# import nltk CoreNLP module (just once)\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "# connect to your CoreNLP server (just once)\n",
    "corenlp_parser = CoreNLPDependencyParser(url=\"http://localhost:9000\")\n",
    "\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets(word, s):\n",
    "    '''\n",
    "    Task:\n",
    "        Given a word and sentence, returns its starting end ending index in the sentence.\n",
    "    \n",
    "    Input:\n",
    "        word: word to find offsets for\n",
    "        s: sentence containing the word\n",
    "    \n",
    "    Output:\n",
    "        Returns a tuple containing the start and end offset.\n",
    "    '''\n",
    "    start = s.find(word)\n",
    "    end = start + len(word) - 1\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    '''\n",
    "    Task:\n",
    "        Helper function\n",
    "    '''\n",
    "    # because otherwise CoreNLP throws 500\n",
    "    return s.replace(\"%\", \"<percentage>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(s):\n",
    "    '''\n",
    "    Task:\n",
    "        Given one sentence, sends it to CoreNLP to obtain the tokens, tags,\n",
    "        and dependency tree. It also adds the start/end offsets to each token.\n",
    "    \n",
    "    Input:\n",
    "        s: string containing the text for one sentence\n",
    "    \n",
    "    Output:\n",
    "        Returns the nltk DependencyGraph object produced by CoreNLP, enriched with token  offsets.\n",
    "\n",
    "    '''\n",
    "    s = s.replace(\"%\", \"<percentage>\")\n",
    "    tree, = corenlp_parser.raw_parse(s)\n",
    "    for n in tree.nodes.items():\n",
    "        node = n[1]\n",
    "        if node['word']:\n",
    "            start, end = get_offsets(node['word'], s)\n",
    "            node['start'] = start\n",
    "            node['end'] = end\n",
    "            \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_in_tree(eid, entities, tree):\n",
    "    start_e1 = entities[eid][0]\n",
    "    end_e1 = entities[eid][1]\n",
    "    \n",
    "    for n in tree.nodes.items():\n",
    "        node = n[1]\n",
    "        if node['word'] and node['start'] == int(start_e1): # and node['end'] == int(end_e1)):\n",
    "            return node\n",
    "    \n",
    "    # TODO: handle two-word entities like \"beta-endorphin\"\n",
    "    # We can extract here word, lemma, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_head(tree, entity):\n",
    "    for n in tree.nodes.items():\n",
    "            node = n[1]\n",
    "            if  node['address'] == entity['head']:\n",
    "                return node\n",
    "    # We can extract here word, lemma, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file for initial checks\n",
    "file = '/Users/mponsclo/Documents/Master/labAHLT/data/train/3155550.xml'\n",
    "file = '../../labAHLT/data/train/Dexamethasone_ddi.xml'\n",
    "tree = parse(file)\n",
    "sentences = tree.getElementsByTagName(\"sentence\")\n",
    "for s in sentences:\n",
    "    sid = s.attributes[\"id\"].value\n",
    "    stext = s.attributes[\"text\"].value\n",
    "    \n",
    "    entities = {}\n",
    "    ents = s.getElementsByTagName(\"entity\")\n",
    "    for e in ents:\n",
    "        eid = e . attributes[\"id\"].value\n",
    "        entities[eid] = e.attributes[\"charOffset\"].value.split(\"-\")\n",
    "    if len(entities) > 1: analysis = analyze(stext)\n",
    "    \n",
    "    pairs = s.getElementsByTagName(\"pair\")\n",
    "    for p in pairs:\n",
    "        # get ground truth\n",
    "        ddi = p.attributes[\"ddi\"].value\n",
    "        dditype = p.attributes[\"type\"].value if ddi == \"true\" else \"null\"\n",
    "            \n",
    "        # target entities\n",
    "        id_e1 = p.attributes[\"e1\"].value\n",
    "        id_e2 = p.attributes[\"e2\"].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_path(path, tree):\n",
    "    if len(path) == 0:\n",
    "        return None\n",
    "    path_nodes = [tree.nodes[x] for x in path]\n",
    "    str_path = \"\"\n",
    "    # traverse from e1 up\n",
    "    current_node = path_nodes[0]\n",
    "    while (current_node['head'] in path):\n",
    "        \n",
    "        rel = current_node['rel']\n",
    "        current_node = tree.nodes[current_node['head']]\n",
    "        str_path += (rel + '<')\n",
    "    \n",
    "    str_path += current_node['lemma']\n",
    "    # traverse from e2 up\n",
    "    current_node = path_nodes[-1]\n",
    "    while(current_node['head'] in path):\n",
    "        rel = current_node['rel']\n",
    "        current_node = tree.nodes[current_node['head']]\n",
    "        str_path += ('>' + rel)\n",
    "        \n",
    "    return str_path\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUE_VERBS = ['administer', 'enhance', 'interact', 'coadminister', 'increase', 'decrease'] # add more?\n",
    "\n",
    "def find_clue_verbs(path, tree):\n",
    "    path_nodes = [tree.nodes[x]['lemma'] for x in path]\n",
    "    feats = []\n",
    "    for pn in path_nodes:\n",
    "        if pn in CLUE_VERBS:\n",
    "            feats.append('lemmainbetween=%s' % pn)\n",
    "            \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tree, entities, e1, e2) :\n",
    "    '''\n",
    "    Task:\n",
    "        Given an analyzed sentence and two target entities , compute a feature\n",
    "        vector for this classification example .\n",
    "    Input:\n",
    "        tree: a DependencyGraph object with all sentence information .\n",
    "        entities: A list of all entities in the sentence (id and offsets).\n",
    "        e1, e2: ids of the two entities to be checked for an interaction\n",
    "    Output:\n",
    "        A vector of binary features .\n",
    "        Features are binary and vectors are in sparse representation (i.e. only\n",
    "        active features are listed)\n",
    "   '''\n",
    "    \n",
    "    \n",
    "        \n",
    "    e1_node = find_entity_in_tree(e1, entities, tree)\n",
    "    e2_node = find_entity_in_tree(e2, entities, tree)\n",
    "    \n",
    "    e1_head = find_head(tree, e1_node) if e1_node else None\n",
    "    e2_head = find_head(tree, e2_node) if e2_node else None\n",
    "    \n",
    "    h1_lemma = e1_head['lemma'] if e1_node else None\n",
    "    h2_lemma = e2_head['lemma'] if e2_node else None\n",
    "    \n",
    "\n",
    "    \n",
    "    tag_head_e1 = e1_head['tag'] if e1_head else None\n",
    "    tag_head_e2 = e2_head['tag'] if e2_head else None\n",
    "    \n",
    "    nxgraph = tree.nx_graph().to_undirected()\n",
    "    shortest_path = networkx.shortest_path(nxgraph, e1_node['address'], e2_node['address']) if (e1_node and e2_node) else []\n",
    "    path = traverse_path(shortest_path, analysis)\n",
    "    find_clue_verbs(shortest_path, analysis)\n",
    "\n",
    "    \n",
    "    #e1_tag = e1_node['tag'] if e1_node else None\n",
    "    #e2_tag = e2_node['tag'] if e2_node else None\n",
    "    \n",
    "    #h_e1 = e1_node['head'] if e1_node else None      \n",
    "    #l_h1 = analysis.nodes[h_e1]['lemma'] if e1_node else None\n",
    "    #w_h1 = analysis.nodes[h_e1]['word'] if e1_node else None\n",
    "    #tag_head_e1 = analysis.nodes[h_e1]['tag'][0].lower() if e1_node else None\n",
    "    \n",
    "    #h_e2 = e2_node['head'] if e2_node else None\n",
    "    #l_h2 = analysis.nodes[h_e2]['lemma'] if e2_node else None\n",
    "    #tag_head_e2 = analysis.nodes[h_e2]['tag'][0].lower() if e2_node else None\n",
    "    \n",
    "    # --- FEATURES ---\n",
    "    features = ['h1_lemma=%s' %h1_lemma if h1_lemma else None,\n",
    "                'h2_lemma=%s' %h2_lemma if h2_lemma else None,\n",
    "                'h1_tag=%s' %tag_head_e1 if tag_head_e1 else None,\n",
    "                'h2_tag=%s' %tag_head_e2 if tag_head_e2 else None,\n",
    "                'path=%s' % path if path else None\n",
    "                \n",
    "                ] + find_clue_verbs(shortest_path, analysis)\n",
    "    print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, 'h1_tag=TOP', None, None]\n",
      "[None, 'h2_lemma=diminish', 'h1_tag=TOP', 'h2_tag=VB', 'path=Aminoglutethimide>obl>parataxis']\n",
      "[None, 'h2_lemma=diminish', None, 'h2_tag=VB', None]\n",
      "['h1_lemma=b', 'h2_lemma=administer', 'h1_tag=NN', 'h2_tag=VBN', 'path=compound<compound<injection>nsubj:pass>advcl>parataxis', 'lemmainbetween=administer']\n",
      "['h1_lemma=b', 'h2_lemma=b', 'h1_tag=NN', 'h2_tag=NN', 'path=compound<compound<injection>compound>dep>dep>advcl>parataxis', 'lemmainbetween=administer']\n",
      "['h1_lemma=b', 'h2_lemma=administer', 'h1_tag=NN', 'h2_tag=VBN', 'path=compound<compound<injection>dep>advcl>parataxis', 'lemmainbetween=administer']\n",
      "['h1_lemma=administer', 'h2_lemma=b', 'h1_tag=VBN', 'h2_tag=NN', 'path=nsubj:pass<administer>compound>dep>dep', 'lemmainbetween=administer']\n",
      "['h1_lemma=administer', 'h2_lemma=administer', 'h1_tag=VBN', 'h2_tag=VBN', 'path=nsubj:pass<administer>dep', 'lemmainbetween=administer']\n",
      "['h1_lemma=b', 'h2_lemma=administer', 'h1_tag=NN', 'h2_tag=VBN', 'path=compound<dep<diuretic']\n",
      "['h1_lemma=b', 'h2_lemma=b', 'h1_tag=NN', 'h2_tag=NN', 'path=compound<b>conj']\n",
      "[None, 'h2_lemma=antibiotic', 'h1_tag=TOP', 'h2_tag=NNS', 'path=antibiotic>amod>nsubj:pass>appos']\n",
      "[None, 'h2_lemma=clearance', 'h1_tag=TOP', 'h2_tag=NN', 'path=antibiotic>amod>nmod>obj>xcomp>appos', 'lemmainbetween=decrease']\n",
      "['h1_lemma=antibiotic', 'h2_lemma=clearance', 'h1_tag=NNS', 'h2_tag=NN', 'path=amod<nsubj:pass<report>amod>nmod>obj>xcomp', 'lemmainbetween=decrease']\n",
      "[None, 'h2_lemma=agent', 'h1_tag=TOP', 'h2_tag=NNS', 'path=Anticholinesterases>compound>nmod>nsubj>appos']\n",
      "[None, 'h2_lemma=agent', 'h1_tag=TOP', 'h2_tag=NNS', 'path=Anticholinesterases>conj>nmod>nsubj>appos']\n",
      "['h1_lemma=agent', 'h2_lemma=agent', 'h1_tag=NNS', 'h2_tag=NNS', 'path=compound<agent>conj']\n",
      "['h1_lemma=agent', 'h2_lemma=therapy', 'h1_tag=NNS', 'h2_tag=NN', 'path=amod<nsubj:pass<withdraw>compound>obl']\n",
      "[None, 'h2_lemma=co-administration', 'h1_tag=TOP', 'h2_tag=NN', 'path=anticoagulant>nmod>nsubj>parataxis>appos']\n",
      "[None, 'h2_lemma=corticosteroid', 'h1_tag=TOP', 'h2_tag=NNS', 'path=anticoagulant>conj>nmod>nsubj>parataxis>appos']\n",
      "[None, None, 'h1_tag=TOP', None, None]\n",
      "['h1_lemma=co-administration', 'h2_lemma=corticosteroid', 'h1_tag=NN', 'h2_tag=NNS', 'path=corticosteroid>conj']\n",
      "['h1_lemma=co-administration', None, 'h1_tag=NN', None, None]\n",
      "['h1_lemma=corticosteroid', None, 'h1_tag=NNS', None, None]\n",
      "[None, 'h2_lemma=increase', 'h1_tag=TOP', 'h2_tag=VB', 'path=Antidiabetics>nsubj>advcl>parataxis', 'lemmainbetween=increase']\n",
      "[None, 'h2_lemma=agent', 'h1_tag=TOP', 'h2_tag=NNS', 'path=Antidiabetics>amod>nmod>nsubj:pass>parataxis']\n",
      "['h1_lemma=increase', 'h2_lemma=agent', 'h1_tag=VB', 'h2_tag=NNS', 'path=nsubj<advcl<require>amod>nmod>nsubj:pass', 'lemmainbetween=increase']\n",
      "[None, None, 'h1_tag=TOP', None, None]\n",
      "[None, 'h2_lemma=clearance', 'h1_tag=TOP', 'h2_tag=NN', 'path=cholestyramine>nmod>obj>appos', 'lemmainbetween=increase']\n",
      "[None, 'h2_lemma=clearance', None, 'h2_tag=NN', None]\n",
      "[None, 'h2_lemma=activity', 'h1_tag=TOP', 'h2_tag=NN', 'path=cyclosporine>nmod>nsubj>parataxis']\n",
      "[None, 'h2_lemma=cyclosporine', 'h1_tag=TOP', 'h2_tag=NN', 'path=cyclosporine>conj>nmod>nsubj>parataxis']\n",
      "['h1_lemma=activity', 'h2_lemma=cyclosporine', 'h1_tag=NN', 'h2_tag=NN', 'path=cyclosporine>conj']\n",
      "['h1_lemma=glycoside', 'h2_lemma=glycoside', 'h1_tag=NNS', 'h2_tag=NNS', 'path=compound<glycoside>compound>nmod>nsubj>parataxis']\n",
      "[None, None, 'h1_tag=TOP', None, None]\n",
      "[None, 'h2_lemma=clearance', 'h1_tag=TOP', 'h2_tag=NN', 'path=Ephedrine>nmod>obj>parataxis', 'lemmainbetween=enhance']\n",
      "[None, None, 'h1_tag=TOP', None, None]\n",
      "[None, 'h2_lemma=clearance', None, 'h2_tag=NN', None]\n",
      "[None, None, None, None, None]\n",
      "['h1_lemma=clearance', None, 'h1_tag=NN', None, None]\n",
      "[None, 'h2_lemma=Estrogens', 'h1_tag=TOP', 'h2_tag=NNP', 'path=Estrogens>nmod']\n",
      "[None, None, 'h1_tag=TOP', None, None]\n",
      "[None, 'h2_lemma=metabolism', 'h1_tag=TOP', 'h2_tag=NN', 'path=Estrogens>nmod>obj>parataxis', 'lemmainbetween=decrease']\n",
      "['h1_lemma=Estrogens', None, 'h1_tag=NNP', None, None]\n",
      "['h1_lemma=Estrogens', 'h2_lemma=metabolism', 'h1_tag=NNP', 'h2_tag=NN', 'path=nmod<Estrogens>nmod>obj>parataxis', 'lemmainbetween=decrease']\n",
      "[None, 'h2_lemma=metabolism', None, 'h2_tag=NN', None]\n",
      "['h1_lemma=rifampin', 'h2_lemma=rifampin', 'h1_tag=NN', 'h2_tag=NN', 'path=dep<rifampin>dep']\n",
      "['h1_lemma=rifampin', 'h2_lemma=rifampin', 'h1_tag=NN', 'h2_tag=NN', 'path=dep<rifampin>dep']\n",
      "['h1_lemma=rifampin', 'h2_lemma=activity', 'h1_tag=NN', 'h2_tag=NN', 'path=dep<rifampin']\n",
      "['h1_lemma=rifampin', 'h2_lemma=metabolism', 'h1_tag=NN', 'h2_tag=NN', 'path=dep<appos<nsubj<enhance>nmod>obj', 'lemmainbetween=enhance']\n",
      "['h1_lemma=rifampin', None, 'h1_tag=NN', None, None]\n",
      "['h1_lemma=rifampin', 'h2_lemma=rifampin', 'h1_tag=NN', 'h2_tag=NN', 'path=dep<rifampin>dep']\n",
      "['h1_lemma=rifampin', 'h2_lemma=activity', 'h1_tag=NN', 'h2_tag=NN', 'path=dep<rifampin']\n",
      "['h1_lemma=rifampin', 'h2_lemma=metabolism', 'h1_tag=NN', 'h2_tag=NN', 'path=dep<appos<nsubj<enhance>nmod>obj', 'lemmainbetween=enhance']\n",
      "['h1_lemma=rifampin', None, 'h1_tag=NN', None, None]\n",
      "['h1_lemma=rifampin', 'h2_lemma=activity', 'h1_tag=NN', 'h2_tag=NN', 'path=dep<rifampin']\n",
      "['h1_lemma=rifampin', 'h2_lemma=metabolism', 'h1_tag=NN', 'h2_tag=NN', 'path=dep<appos<nsubj<enhance>nmod>obj', 'lemmainbetween=enhance']\n",
      "['h1_lemma=rifampin', None, 'h1_tag=NN', None, None]\n",
      "['h1_lemma=activity', 'h2_lemma=metabolism', 'h1_tag=NN', 'h2_tag=NN', 'path=appos<nsubj<enhance>nmod>obj', 'lemmainbetween=enhance']\n",
      "['h1_lemma=activity', None, 'h1_tag=NN', None, None]\n",
      "['h1_lemma=metabolism', None, 'h1_tag=NN', None, None]\n",
      "['h1_lemma=3a4', 'h2_lemma=antibiotic', 'h1_tag=NN', 'h2_tag=NNS', 'path=ketoconazole>amod>appos']\n",
      "['h1_lemma=3a4', 'h2_lemma=antibiotic', 'h1_tag=NN', 'h2_tag=NNS', 'path=ketoconazole>nmod>appos']\n",
      "['h1_lemma=3a4', 'h2_lemma=concentration', 'h1_tag=NN', 'h2_tag=NNS', 'path=dep<nsubj<have>nmod>obl>ccomp']\n",
      "['h1_lemma=antibiotic', 'h2_lemma=antibiotic', 'h1_tag=NNS', 'h2_tag=NNS', 'path=amod<antibiotic>nmod']\n",
      "['h1_lemma=antibiotic', 'h2_lemma=concentration', 'h1_tag=NNS', 'h2_tag=NNS', 'path=amod<appos<dep<nsubj<have>nmod>obl>ccomp']\n",
      "['h1_lemma=antibiotic', 'h2_lemma=concentration', 'h1_tag=NNS', 'h2_tag=NNS', 'path=nmod<appos<dep<nsubj<have>nmod>obl>ccomp']\n",
      "['h1_lemma=3a4', 'h2_lemma=indinavir', 'h1_tag=NN', 'h2_tag=NN', 'path=indinavir>dep']\n",
      "[None, None, 'h1_tag=TOP', None, None]\n",
      "[None, 'h2_lemma=metabolism', 'h1_tag=TOP', 'h2_tag=NN', 'path=Ketoconazole>nmod>obj>xcomp>parataxis', 'lemmainbetween=decrease']\n",
      "[None, None, 'h1_tag=TOP', None, None]\n",
      "[None, 'h2_lemma=metabolism', None, 'h2_tag=NN', None]\n",
      "[None, None, None, None, None]\n",
      "['h1_lemma=metabolism', None, 'h1_tag=NN', None, None]\n",
      "['h1_lemma=inhibit', 'h2_lemma=synthesis', 'h1_tag=VB', 'h2_tag=NN', 'path=nsubj<inhibit>amod>obj']\n",
      "['h1_lemma=agent', 'h2_lemma=agent', 'h1_tag=NNS', 'h2_tag=NNS', 'path=amod<agent>dep']\n",
      "['h1_lemma=agent', 'h2_lemma=use', 'h1_tag=NNS', 'h2_tag=NN', 'path=amod<agent>nmod>nsubj>dep', 'lemmainbetween=increase']\n",
      "['h1_lemma=agent', 'h2_lemma=agent', 'h1_tag=NNS', 'h2_tag=NNS', 'path=amod<agent>amod>conj>nmod>nsubj>dep', 'lemmainbetween=increase']\n",
      "['h1_lemma=agent', 'h2_lemma=use', 'h1_tag=NNS', 'h2_tag=NN', 'path=amod<agent>conj>nsubj>dep', 'lemmainbetween=increase']\n",
      "['h1_lemma=agent', 'h2_lemma=use', 'h1_tag=NNS', 'h2_tag=NN', 'path=dep<agent>nmod>nsubj>dep', 'lemmainbetween=increase']\n",
      "['h1_lemma=agent', 'h2_lemma=agent', 'h1_tag=NNS', 'h2_tag=NNS', 'path=dep<agent>amod>conj>nmod>nsubj>dep', 'lemmainbetween=increase']\n",
      "['h1_lemma=agent', 'h2_lemma=use', 'h1_tag=NNS', 'h2_tag=NN', 'path=dep<agent>conj>nsubj>dep', 'lemmainbetween=increase']\n",
      "['h1_lemma=use', 'h2_lemma=agent', 'h1_tag=NN', 'h2_tag=NNS', 'path=aspirin>amod>conj']\n",
      "['h1_lemma=use', 'h2_lemma=use', 'h1_tag=NN', 'h2_tag=NN', 'path=nmod<use>conj']\n",
      "['h1_lemma=agent', 'h2_lemma=use', 'h1_tag=NNS', 'h2_tag=NN', 'path=amod<conj<nmod<use>conj']\n",
      "['h1_lemma=use', 'h2_lemma=conjunction', 'h1_tag=VBN', 'h2_tag=NN', 'path=nsubj:pass<use>nmod>obl']\n",
      "['h1_lemma=clearance', 'h2_lemma=use', 'h1_tag=NN', 'h2_tag=NN', 'path=nmod<nsubj:pass<increase>nmod>obl', 'lemmainbetween=increase']\n",
      "[None, 'h2_lemma=level', 'h1_tag=TOP', 'h2_tag=NNS', 'path=Phenytoin>compound>nmod>nmod>appos', 'lemmainbetween=increase']\n",
      "[None, 'h2_lemma=co-administration', 'h1_tag=TOP', 'h2_tag=NN', 'path=Phenytoin>compound>nmod>nmod>appos', 'lemmainbetween=increase']\n",
      "['h1_lemma=level', 'h2_lemma=co-administration', 'h1_tag=NNS', 'h2_tag=NN', 'path=compound<nmod<increase>compound>nmod', 'lemmainbetween=increase']\n",
      "[None, 'h2_lemma=co-administration', 'h1_tag=TOP', 'h2_tag=NN', 'path=Thalidomide>nmod>nsubj:pass>parataxis']\n",
      "[None, 'h2_lemma=therapy', 'h1_tag=TOP', 'h2_tag=NN', 'path=vaccine>amod>nmod>nsubj>appos']\n",
      "[None, 'h2_lemma=vaccine', 'h1_tag=TOP', 'h2_tag=NNS', 'path=vaccine>amod>conj>nmod>obj>appos']\n",
      "[None, 'h2_lemma=live', 'h1_tag=TOP', 'h2_tag=JJ', 'path=vaccine>conj>amod>conj>nmod>obj>appos']\n",
      "['h1_lemma=therapy', 'h2_lemma=vaccine', 'h1_tag=NN', 'h2_tag=NNS', 'path=amod<nmod<nsubj<exhibit>amod>conj>nmod>obj']\n",
      "['h1_lemma=therapy', 'h2_lemma=live', 'h1_tag=NN', 'h2_tag=JJ', 'path=amod<nmod<nsubj<exhibit>conj>amod>conj>nmod>obj']\n",
      "['h1_lemma=vaccine', 'h2_lemma=live', 'h1_tag=NNS', 'h2_tag=JJ', 'path=live>conj']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h1_lemma=potentiate', 'h2_lemma=vaccine', 'h1_tag=VB', 'h2_tag=NNS', 'path=nsubj<potentiate>amod>obl>ccomp']\n",
      "['h1_lemma=administration', 'h2_lemma=therapy', 'h1_tag=NN', 'h2_tag=NN', 'path=nmod<nsubj:pass<defer>compound>nsubj:pass>advcl']\n"
     ]
    }
   ],
   "source": [
    "# file for initial checks\n",
    "file = '/Users/mponsclo/Documents/Master/labAHLT/data/train/3155550.xml'\n",
    "file = '../../labAHLT/data/train/Dexamethasone_ddi.xml'\n",
    "tree = parse(file)\n",
    "sentences = tree.getElementsByTagName(\"sentence\")\n",
    "for s in sentences:\n",
    "    sid = s.attributes[\"id\"].value\n",
    "    stext = s.attributes[\"text\"].value\n",
    "    \n",
    "    entities = {}\n",
    "    ents = s.getElementsByTagName(\"entity\")\n",
    "    for e in ents:\n",
    "        eid = e . attributes[\"id\"].value\n",
    "        entities[eid] = e.attributes[\"charOffset\"].value.split(\"-\")\n",
    "    if len(entities) > 1: analysis = analyze(stext)\n",
    "    \n",
    "    pairs = s.getElementsByTagName(\"pair\")\n",
    "    for p in pairs:\n",
    "        # get ground truth\n",
    "        ddi = p.attributes[\"ddi\"].value\n",
    "        dditype = p.attributes[\"type\"].value if ddi == \"true\" else \"null\"\n",
    "            \n",
    "        # target entities\n",
    "        id_e1 = p.attributes[\"e1\"].value\n",
    "        id_e2 = p.attributes[\"e2\"].value\n",
    "        extract_features(analysis, entities, id_e1, id_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only join an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-1663c96f9d8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# resulting feature vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_e1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_e2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdditype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only join an iterable"
     ]
    }
   ],
   "source": [
    "datadir = \"../../labAHLT/data/devel\"\n",
    "\n",
    "# process each file in directory\n",
    "for f in listdir(datadir):\n",
    "    # parse XML file , obtaining a DOM tree\n",
    "    tree = parse(datadir + \"/\" + f)\n",
    "    # process each sentence in the file\n",
    "    sentences = tree.getElementsByTagName(\"sentence\")\n",
    "    for s in sentences:\n",
    "\n",
    "        sid = s.attributes[\"id\"].value # get sentence id\n",
    "        stext = s.attributes[\"text\"].value # get sentence text\n",
    "        \n",
    "        # CoreNLP throws error for empty sentences\n",
    "        if len(stext) == 0:\n",
    "            continue\n",
    "\n",
    "        # load sentence ground truth entities\n",
    "        entities = {}\n",
    "        ents = s.getElementsByTagName(\"entity\")\n",
    "        for e in ents:\n",
    "            eid = e . attributes[\"id\"].value\n",
    "            entities[eid] = e.attributes[\"charOffset\"].value.split(\"-\")\n",
    "\n",
    "        # analyze sentence if there is at least a pair of entities\n",
    "        if len(entities) > 1: analysis = analyze(stext)\n",
    "\n",
    "        # for each pair in the sentence , decide whether it is DDI and its type\n",
    "        pairs = s.getElementsByTagName(\"pair\")\n",
    "        for p in pairs:\n",
    "            # get ground truth\n",
    "            ddi = p.attributes[\"ddi\"].value\n",
    "            dditype = p.attributes[\"type\"].value if ddi == \"true\" else \"null\"\n",
    "            \n",
    "            # target entities\n",
    "            id_e1 = p.attributes[\"e1\"].value\n",
    "            id_e2 = p.attributes[\"e2\"].value\n",
    "            \n",
    "            # feature extraction\n",
    "            feats = extract_features(analysis, entities, id_e1, id_e2)\n",
    "            \n",
    "            # resulting feature vector\n",
    "            print(sid, id_e1, id_e2, dditype, \"\\t\".join(feats), sep=\"\\t\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
