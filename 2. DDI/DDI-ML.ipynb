{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "from nltk import pos_tag\n",
    "import networkx\n",
    "import argparse\n",
    "import string\n",
    "import nltk\n",
    "# import nltk CoreNLP module (just once)\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "# connect to your CoreNLP server (just once)\n",
    "corenlp_parser = CoreNLPDependencyParser(url=\"http://localhost:9000\")\n",
    "\n",
    "\n",
    "def do_indices_overlap(start1, end1, start2, end2):\n",
    "    if start1 == start2 and end1==end2:\n",
    "        return True\n",
    "\n",
    "def find_entity_in_tree(eid, entities, tree):\n",
    "    start_e1 = int(entities[eid]['offsets'][0])\n",
    "    end_e1 = int(entities[eid]['offsets'][1].split(';')[0])\n",
    "\n",
    "    for n in tree.nodes.items():\n",
    "        node = n[1]\n",
    "        if node['word'] and (node['start'] == start_e1 or node['end'] == end_e1):\n",
    "            return node\n",
    "\n",
    "def find_other_entities(eid1, eid2, sid, entities, tree):\n",
    "    other_entities = [(entity['eid'], entity['type']) for _, entity in entities.items() if entity['sid'] == sid and entity['eid'] not in [eid1, eid2]]\n",
    "    return [(find_entity_in_tree(eid, entities, tree),e_type) for eid, e_type in other_entities]\n",
    "\n",
    "def get_offsets(word, s):\n",
    "    '''\n",
    "    Task:\n",
    "        Given a word and sentence, returns its starting end ending index in the sentence.\n",
    "    \n",
    "    Input:\n",
    "        word: word to find offsets for\n",
    "        s: sentence containing the word\n",
    "    \n",
    "    Output:\n",
    "        Returns a tuple containing the start and end offset.\n",
    "    '''\n",
    "    start = s.find(word)\n",
    "    end = start + len(word) - 1\n",
    "    return start, end\n",
    "\n",
    "def preprocess(s):\n",
    "    '''\n",
    "    Task:\n",
    "        Helper function\n",
    "    '''\n",
    "    # because otherwise CoreNLP throws 500\n",
    "    return s.replace(\"%\", \"<percentage>\")\n",
    "\n",
    "def analyze(s):\n",
    "    '''\n",
    "    Task:\n",
    "        Given one sentence, sends it to CoreNLP to obtain the tokens, tags,\n",
    "        and dependency tree. It also adds the start/end offsets to each token.\n",
    "    \n",
    "    Input:\n",
    "        s: string containing the text for one sentence\n",
    "    \n",
    "    Output:\n",
    "        Returns the nltk DependencyGraph object produced by CoreNLP, enriched with token  offsets.\n",
    "\n",
    "    '''\n",
    "    s = s.replace(\"%\", \"<percentage>\")\n",
    "    tree, = corenlp_parser.raw_parse(s)\n",
    "    for n in tree.nodes.items():\n",
    "        node = n[1]\n",
    "        if node['word']:\n",
    "            start, end = get_offsets(node['word'], s)\n",
    "            node['start'] = start\n",
    "            node['end'] = end\n",
    "            \n",
    "    return tree\n",
    "\n",
    "CLUE_VERBS = ['administer', 'enhance', 'interact', 'coadminister', 'increase', 'decrease']\n",
    "NEGATIVE_WORDS = ['No', 'not', 'neither', 'without','lack', 'fail', 'unable', 'abrogate',\n",
    "                  'absence', 'prevent','unlikely', 'unchanged', 'rarely', 'inhibitor']\n",
    "\n",
    "def find_clue_verbs(path, tree):\n",
    "    path_nodes = [tree.nodes[x]['lemma'] for x in path]\n",
    "    feats = []\n",
    "    for pn in path_nodes:\n",
    "        if pn in CLUE_VERBS:\n",
    "            feats.append('lemmainbetween=%s' % pn)\n",
    "            \n",
    "    return feats\n",
    "\n",
    "def negative_words_path(path, tree):\n",
    "    path_nodes = [tree.nodes[x]['word'] for x in path]\n",
    "    count = 0\n",
    "    for pn in path_nodes:\n",
    "        if pn in NEGATIVE_WORDS or pn[-3:] == \"n't\":\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def negative_words_sentence(tree):\n",
    "    count = 0\n",
    "    for n in tree.nodes.items():\n",
    "        word = n[1]['word']\n",
    "        if word in NEGATIVE_WORDS:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def traverse_path(path, tree):\n",
    "    if len(path) == 0:\n",
    "        return None, None\n",
    "    path_nodes = [tree.nodes[x] for x in path]\n",
    "    str_path = \"\"\n",
    "    # traverse from e1 up\n",
    "    current_node = path_nodes[0]\n",
    "    while (current_node['head'] in path):\n",
    "        rel = current_node['rel']\n",
    "        current_node = tree.nodes[current_node['head']]\n",
    "        str_path += (rel + '<')\n",
    "\n",
    "    tag_path = str_path + current_node['tag']\n",
    "    str_path += current_node['lemma']\n",
    "    # traverse from e2 up\n",
    "    current_node = path_nodes[-1]\n",
    "    while(current_node['head'] in path):\n",
    "        rel = current_node['rel']\n",
    "        current_node = tree.nodes[current_node['head']]\n",
    "        str_path += ('>' + rel)\n",
    "        tag_path += ('>' + rel)\n",
    "\n",
    "    return str_path, tag_path\n",
    "\n",
    "def find_words_outside_path(path, tree):\n",
    "    if len(path) < 1:\n",
    "        return [], []\n",
    "    words_before = []\n",
    "    words_after = []\n",
    "    nodes_before = [node[1] for node in tree.nodes.items()][:path[0]]\n",
    "    nodes_after = [node[1] for node in tree.nodes.items()][path[-1]:]\n",
    "\n",
    "    for node in nodes_before:\n",
    "        if node['address'] not in path and node['lemma'] and node['lemma'] not in string.punctuation and not node['lemma'].isdigit():\n",
    "            words_before.append(node['lemma'])\n",
    "    for node in nodes_after:\n",
    "        if node['address'] not in path and node['lemma'] and node['lemma'] not in string.punctuation and not node['lemma'].isdigit():\n",
    "            words_after.append(node['lemma'])\n",
    "    return words_before, words_after\n",
    "\n",
    "\n",
    "\n",
    "def find_head(tree, entity):\n",
    "    for n in tree.nodes.items():\n",
    "            node = n[1]\n",
    "            if  node['address'] == entity['head']:\n",
    "                return node\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(datadir):\n",
    "    pairs = []\n",
    "    for f in listdir(datadir):\n",
    "        # parse XML file , obtaining a DOM tree\n",
    "        tree = parse(datadir + \"/\" + f)\n",
    "        # process each sentence in the file\n",
    "        sentences = tree.getElementsByTagName(\"sentence\")\n",
    "        \n",
    "        \n",
    "        for s in sentences:\n",
    "\n",
    "            sid = s.attributes[\"id\"].value # get sentence id\n",
    "            stext = s.attributes[\"text\"].value # get sentence text\n",
    "            \n",
    "            # CoreNLP throws error for empty sentences\n",
    "            if len(stext) == 0:\n",
    "                continue\n",
    "\n",
    "            # load sentence ground truth entities\n",
    "            entities = {}\n",
    "            ents = s.getElementsByTagName(\"entity\")\n",
    "            for e in ents:\n",
    "                eid = e . attributes[\"id\"].value\n",
    "                entities[eid] = {\"offsets\": e.attributes[\"charOffset\"].value.split(\"-\"), \"type\": e.attributes[\"type\"].value, 'sid': sid, 'eid': eid}\n",
    "\n",
    "            # analyze sentence if there is at least a pair of entities\n",
    "            if len(entities) > 1:\n",
    "                analysis = analyze(stext)\n",
    "\n",
    "            # for each pair in the sentence , decide whether it is DDI and its type\n",
    "                pairs.append((analysis, entities, s.getElementsByTagName(\"pair\")))\n",
    "    return pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(sentence_pairs, outputf):\n",
    "    out = open(outputf, 'w')\n",
    "    for analysis, entities, pairs in sentence_pairs:\n",
    "        for p in pairs:\n",
    "            \n",
    "            # get ground truth\n",
    "            ddi = p.attributes[\"ddi\"].value\n",
    "            dditype = p.attributes[\"type\"].value if ddi == \"true\" else \"null\"\n",
    "\n",
    "            # target entities\n",
    "            id_e1 = p.attributes[\"e1\"].value\n",
    "            id_e2 = p.attributes[\"e2\"].value\n",
    "\n",
    "            sid = '.'.join(id_e1.split('.')[:-1])\n",
    "\n",
    "            # feature extraction\n",
    "            feats = extract_features(analysis, entities, id_e1, id_e2, sid)\n",
    "\n",
    "            # resulting feature vector\n",
    "            out.write('\\t'.join([sid, id_e1, id_e2, dditype, \"\\t\".join(feats), '\\n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindir = \"../../labAHLT/data/train\"\n",
    "testdir = \"../../labAHLT/data/test\"\n",
    "develdir = \"../../labAHLT/data/devel\"\n",
    "\n",
    "\n",
    "\n",
    "trainpairs = main(traindir)\n",
    "testpairs = main(testdir)\n",
    "develpairs = main(develdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeats = \"feats.dat\"\n",
    "testfeats = \"feats_test.dat\"\n",
    "develfeats = \"feats_devel.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tree, entities, e1, e2, sid) :\n",
    "    '''\n",
    "    Task:\n",
    "        Given an analyzed sentence and two target entities , compute a feature\n",
    "        vector for this classification example .\n",
    "    Input:\n",
    "        tree: a DependencyGraph object with all sentence information .\n",
    "        entities: A list of all entities in the sentence (id and offsets).\n",
    "        e1, e2: ids of the two entities to be checked for an interaction\n",
    "        sid: sentence id\n",
    "    Output:\n",
    "        A vector of binary features .\n",
    "        Features are binary and vectors are in sparse representation (i.e. only\n",
    "        active features are listed)\n",
    "   '''\n",
    "    int_verbs = ['interact', 'interaction']\n",
    "    mech_verbs = ['metabolism', 'concentration', 'clearance', 'level', 'absorption', 'dose',\n",
    "                 'presence', 'interfere']\n",
    "    adv_verbs = ['co-administration', 'take', 'coadminister', 'treatment', 'therapy', 'tell']\n",
    "    eff_verbs = ['effect', 'alcohol', 'action','use', 'combination', 'inhibitor',\n",
    "                'response', 'effect', 'enhance', 'diminish']\n",
    "      \n",
    "    e1_node = find_entity_in_tree(e1, entities, tree)\n",
    "    e2_node = find_entity_in_tree(e2, entities, tree)\n",
    "    \n",
    "    e1_head = find_head(tree, e1_node) if e1_node else None\n",
    "    e2_head = find_head(tree, e2_node) if e2_node else None\n",
    "    \n",
    "    h1_lemma = e1_head['lemma'] if e1_node else None\n",
    "    h2_lemma = e2_head['lemma'] if e2_node else None\n",
    "    \n",
    "    tag_head_e1 = e1_head['tag'] if e1_head else None\n",
    "    tag_head_e2 = e2_head['tag'] if e2_head else None\n",
    "    \n",
    "    nxgraph = tree.nx_graph().to_undirected()\n",
    "    shortest_path = networkx.shortest_path(nxgraph, e1_node['address'], e2_node['address']) if (e1_node and e2_node) else []\n",
    "    path_with_word, path_with_tag = traverse_path(shortest_path, tree)\n",
    "    find_clue_verbs(shortest_path, tree)\n",
    "    count_neg_p = negative_words_path(shortest_path, tree)\n",
    "    count_neg_s = negative_words_sentence(tree)\n",
    "\n",
    "    \n",
    "    # --- FEATURES ---\n",
    "    features = ['h1_lemma=%s' %h1_lemma,\n",
    "                'h2_lemma=%s' %h2_lemma,\n",
    "                'h1_tag=%s' %tag_head_e1,\n",
    "                'h2_tag=%s' %tag_head_e2,\n",
    "#                 'path=%s' % path_with_word,\n",
    "                'tagpath=%s' % path_with_tag,\n",
    "                # 'neg_words_p=%s' %count_neg_p,  # only 28 with 1, 1 with 2\n",
    "                'neg_words_s=%s' %count_neg_s,  # 3144 with 1, 270 with 2, 4 with 3 \n",
    "                'e1_type=%s' % entities[e1]['type'],\n",
    "                'e2_type=%s' % entities[e2]['type'],\n",
    "                ] + find_clue_verbs(shortest_path, tree)\n",
    "    \n",
    "    \n",
    "    if (e1_head and e2_head):\n",
    "        if h1_lemma == h2_lemma:  # should use address? \n",
    "            features.append('under_same=True') # 5609 occurrences\n",
    "            if tag_head_e1[0].lower() == 'v':\n",
    "                features.append('under_same_verb=True') # 173 occurrences\n",
    "            else:\n",
    "                features.append('under_same_verb=False')\n",
    "        else:\n",
    "            features.append('under_same=False')\n",
    "            features.append('under_same_verb=False')\n",
    "            \n",
    "        if h1_lemma == e2_node['lemma']:\n",
    "            features.append('1under2=True') # 136 occ\n",
    "        else:\n",
    "            features.append('1under2=False')\n",
    "            \n",
    "        if h2_lemma == e1_node['lemma']:\n",
    "            features.append('2under1=True') # 1953 occ\n",
    "        else:\n",
    "            features.append('2under1=False')\n",
    "        \n",
    "        if h1_lemma in int_verbs or h2_lemma in int_verbs:\n",
    "            features.append('intVerbs=True') # 458\n",
    "        else:\n",
    "            features.append('intVerbs=False')\n",
    "            \n",
    "        if h1_lemma in mech_verbs or h2_lemma in mech_verbs:\n",
    "            features.append('mechVerbs=True') # 1030\n",
    "        else:\n",
    "            features.append('mechVerbs=False')\n",
    "\n",
    "        if h1_lemma in adv_verbs or h2_lemma in adv_verbs:\n",
    "            features.append('advVerbs=True') # 569\n",
    "        else:\n",
    "            features.append('advVerbs=False')\n",
    "\n",
    "        if h1_lemma in eff_verbs or h2_lemma in eff_verbs:\n",
    "            features.append('effVerbs=True') # 3480\n",
    "        else:\n",
    "            features.append('effVerbs=False')\n",
    "        \n",
    "    else:\n",
    "        None\n",
    "\n",
    "    words_before, words_after = find_words_outside_path(shortest_path, tree)\n",
    "    for word in words_before:\n",
    "        features.append(f'lemmabefore={word}')\n",
    "        features.append(f'tagbefore={pos_tag(word)[0][1]}')\n",
    "    for word in words_after:\n",
    "        features.append(f'lemmaafter={word}')\n",
    "        features.append(f'tagafter={pos_tag(word)[0][1]}')\n",
    "\n",
    "    other_entities = find_other_entities(e1, e2, sid, entities, tree)\n",
    "    for e_node, e_type in other_entities:\n",
    "        if e_node and e_node['address'] in shortest_path:\n",
    "            features.append('entityinbetween=%s' % e_type)\n",
    "        else:\n",
    "            features.append('entityother=%s' % e_type)\n",
    "        \n",
    "        \n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "# helper functions for crf-learner and crf-classifier\n",
    "\n",
    "def parse_string(line):\n",
    "    split_data = line[:-1].split('\\t')\n",
    "    sentence_id = split_data[0]\n",
    "    e1_id = split_data[1]\n",
    "    e2_id = split_data[2]\n",
    "    interaction = split_data[3]\n",
    "    feats = split_data[4:]\n",
    "    features = dict()\n",
    "    for f in feats:\n",
    "        features[f] = True\n",
    "    \n",
    "    return sentence_id, e1_id, e2_id, features, interaction\n",
    "\n",
    "def read_feature_file(filepath):\n",
    "    '''\n",
    "    Task:\n",
    "        Given the path to the file containing tokenized sentences, read it and return the necessary data structures\n",
    "    Input:\n",
    "        filepath: Path to the data\n",
    "    Output:\n",
    "        tokens_by_sentence: list of tuples: (sentence_id, list_of_tokens). Each tuple represents a sentence,\n",
    "            where in the list_of_tokens each token is represented by a tuple (word, offset_from, offset_to)\n",
    "        features: list of lists of features per sentence\n",
    "        tags: list of lists of B-I-O tags per sentence\n",
    "    '''\n",
    "    \n",
    "    f = open(filepath, 'r')\n",
    "    lines = f.readlines()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for line in lines:\n",
    "        if len(line) > 1:\n",
    "            sentence_id, e1, e2, features, interaction = parse_string(line)\n",
    "            data.append((features, interaction))\n",
    "    return data\n",
    "\n",
    "def read_test_feature_file(filepath):\n",
    "    \n",
    "    f = open(filepath, 'r')\n",
    "    lines = f.readlines()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for line in lines:\n",
    "        if len(line) > 1:\n",
    "            sentence_id, e1, e2, features, interaction = parse_string(line)\n",
    "            data.append((sentence_id, e1, e2, features))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extract(trainpairs, trainfeats)\n",
    "extract(testpairs, testfeats)\n",
    "extract(develpairs, develfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = read_feature_file(trainfeats)\n",
    "test_data = read_test_feature_file(testfeats)\n",
    "nltk.classify.megam.config_megam('/home/zosia/uni-dev/ahlt/2. DDI/megam_i686.opt')\n",
    "\n",
    "mymodel = nltk.classify.MaxentClassifier.train(train_data, 'megam', gaussian_prior_sigma=10)\n",
    "\n",
    "output_f = open('output.dat', 'w')\n",
    "\n",
    "for sid, e1, e2, feats in test_data:\n",
    "    prediction = mymodel.classify(feats)\n",
    "    if prediction != \"null\":\n",
    "        # print ( sid , e1 , e2 , prediction , sep =\"|\")\n",
    "        output_f.write(f'{sid}|{e1}|{e2}|{prediction}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\r\n",
      "------------------------------------------------------------------------------\r\n",
      "advise             68\t  28\t 144\t  96\t 212\t70.8%\t32.1%\t44.2%\r\n",
      "effect            122\t  77\t 161\t 199\t 283\t61.3%\t43.1%\t50.6%\r\n",
      "int                 3\t   3\t  15\t   6\t  18\t50.0%\t16.7%\t25.0%\r\n",
      "mechanism         140\t  87\t 197\t 227\t 337\t61.7%\t41.5%\t49.6%\r\n",
      "------------------------------------------------------------------------------\r\n",
      "M.avg            -\t-\t-\t-\t-\t61.0%\t33.3%\t42.4%\r\n",
      "------------------------------------------------------------------------------\r\n",
      "m.avg             333\t 195\t 517\t 528\t 850\t63.1%\t39.2%\t48.3%\r\n",
      "m.avg(no class)   374\t 154\t 476\t 528\t 850\t70.8%\t44.0%\t54.3%\r\n"
     ]
    }
   ],
   "source": [
    "!python3 evaluator.py DDI ../../labAHLT/data/test output.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "devel_data = read_test_feature_file(develfeats)\n",
    "output_f = open('output.dat', 'w')\n",
    "\n",
    "for sid, e1, e2, feats in devel_data:\n",
    "    prediction = mymodel.classify(feats)\n",
    "    if prediction != \"null\":\n",
    "        # print ( sid , e1 , e2 , prediction , sep =\"|\")\n",
    "        output_f.write(f'{sid}|{e1}|{e2}|{prediction}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\r\n",
      "------------------------------------------------------------------------------\r\n",
      "advise             63\t  42\t  75\t 105\t 138\t60.0%\t45.7%\t51.9%\r\n",
      "effect            112\t  63\t 203\t 175\t 315\t64.0%\t35.6%\t45.7%\r\n",
      "int                28\t  12\t   7\t  40\t  35\t70.0%\t80.0%\t74.7%\r\n",
      "mechanism          85\t  55\t 179\t 140\t 264\t60.7%\t32.2%\t42.1%\r\n",
      "------------------------------------------------------------------------------\r\n",
      "M.avg            -\t-\t-\t-\t-\t63.7%\t48.4%\t53.6%\r\n",
      "------------------------------------------------------------------------------\r\n",
      "m.avg             288\t 172\t 464\t 460\t 752\t62.6%\t38.3%\t47.5%\r\n",
      "m.avg(no class)   319\t 141\t 433\t 460\t 752\t69.3%\t42.4%\t52.6%\r\n"
     ]
    }
   ],
   "source": [
    "!python3 evaluator.py DDI ../../labAHLT/data/devel output.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
