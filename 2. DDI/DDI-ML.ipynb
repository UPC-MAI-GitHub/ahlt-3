{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDI-ML. Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "cd into ../stanford-corenlp-4.2.0 and run\n",
    "\n",
    "`java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "# import nltk CoreNLP module (just once)\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "# connect to your CoreNLP server (just once)\n",
    "corenlp_parser = CoreNLPDependencyParser(url=\"http://localhost:9000\")\n",
    "\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets(word, s):\n",
    "    '''\n",
    "    Task:\n",
    "        Given a word and sentence, returns its starting end ending index in the sentence.\n",
    "    \n",
    "    Input:\n",
    "        word: word to find offsets for\n",
    "        s: sentence containing the word\n",
    "    \n",
    "    Output:\n",
    "        Returns a tuple containing the start and end offset.\n",
    "    '''\n",
    "    start = s.find(word)\n",
    "    end = start + len(word) - 1\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    '''\n",
    "    Task:\n",
    "        Helper function\n",
    "    '''\n",
    "    # because otherwise CoreNLP throws 500\n",
    "    return s.replace(\"%\", \"<percentage>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(s):\n",
    "    '''\n",
    "    Task:\n",
    "        Given one sentence, sends it to CoreNLP to obtain the tokens, tags,\n",
    "        and dependency tree. It also adds the start/end offsets to each token.\n",
    "    \n",
    "    Input:\n",
    "        s: string containing the text for one sentence\n",
    "    \n",
    "    Output:\n",
    "        Returns the nltk DependencyGraph object produced by CoreNLP, enriched with token  offsets.\n",
    "\n",
    "    '''\n",
    "    s = s.replace(\"%\", \"<percentage>\")\n",
    "    tree, = corenlp_parser.raw_parse(s)\n",
    "    for n in tree.nodes.items():\n",
    "        node = n[1]\n",
    "        if node['word']:\n",
    "            start, end = get_offsets(node['word'], s)\n",
    "            node['start'] = start\n",
    "            node['end'] = end\n",
    "            \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_in_tree(eid, entities, tree):\n",
    "    start_e1 = entities[eid][0]\n",
    "    end_e1 = entities[eid][1]\n",
    "    \n",
    "    for n in tree.nodes.items():\n",
    "        node = n[1]\n",
    "        if node['word'] and (node['start'] == int(start_e1) and node['end'] == int(end_e1)):\n",
    "            return node\n",
    "    \n",
    "    # TODO: handle two-word entities like \"beta-endorphin\"\n",
    "    # We can extract here word, lemma, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_head(tree, entity):\n",
    "    for n in tree.nodes.items():\n",
    "            node = n[1]\n",
    "            if  node['address'] == entity['head']:\n",
    "                return node\n",
    "    # We can extract here word, lemma, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphine\n",
      "{'address': 11, 'word': 'morphine', 'lemma': 'morphine', 'ctag': 'NN', 'tag': 'NN', 'feats': '_', 'head': 9, 'deps': defaultdict(<class 'list'>, {'cc': [10]}), 'rel': 'conj', 'start': 57, 'end': 64}\n",
      "{'address': 9, 'word': 'endorphin', 'lemma': 'endorphin', 'ctag': 'JJ', 'tag': 'JJ', 'feats': '_', 'head': 16, 'deps': defaultdict(<class 'list'>, {'obl:npmod': [7], 'punct': [8, 12], 'conj': [11], 'advmod': [13]}), 'rel': 'ccomp', 'start': 43, 'end': 51}\n"
     ]
    }
   ],
   "source": [
    "e = find_entity_in_tree(id_e2, entities, analysis)\n",
    "h = find_head(analysis, e)\n",
    "print(e['word'])\n",
    "print(e)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DDI-MedLine.d63.s8.e0': ['38', '51'], 'DDI-MedLine.d63.s8.e1': ['57', '64']}\n",
      "38\n",
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x7ff42c945310>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'ROOT': [1]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'VBG',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'obj': [2],\n",
      "                                      'punct': [17]}),\n",
      "                 'end': 9,\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': 'descend',\n",
      "                 'rel': 'ROOT',\n",
      "                 'start': 0,\n",
      "                 'tag': 'VBG',\n",
      "                 'word': 'descending'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'conj': [5],\n",
      "                                      'nmod': [16]}),\n",
      "                 'end': 17,\n",
      "                 'feats': '_',\n",
      "                 'head': 1,\n",
      "                 'lemma': 'epsilon',\n",
      "                 'rel': 'obj',\n",
      "                 'start': 11,\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'epsilon'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'CC',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'end': 21,\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': 'and',\n",
      "                 'rel': 'cc',\n",
      "                 'start': 19,\n",
      "                 'tag': 'CC',\n",
      "                 'word': 'and'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'end': 24,\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': 'mu',\n",
      "                 'rel': 'compound',\n",
      "                 'start': 23,\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'mu'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'NNS',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'cc': [3],\n",
      "                                      'compound': [4]}),\n",
      "                 'end': 32,\n",
      "                 'feats': '_',\n",
      "                 'head': 2,\n",
      "                 'lemma': 'system',\n",
      "                 'rel': 'conj',\n",
      "                 'start': 26,\n",
      "                 'tag': 'NNS',\n",
      "                 'word': 'systems'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'end': 36,\n",
      "                 'feats': '_',\n",
      "                 'head': 16,\n",
      "                 'lemma': 'for',\n",
      "                 'rel': 'case',\n",
      "                 'start': 34,\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'for'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'end': 41,\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': 'beta',\n",
      "                 'rel': 'obl:npmod',\n",
      "                 'start': 38,\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'beta'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'HYPH',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'end': 42,\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '-',\n",
      "                 'rel': 'punct',\n",
      "                 'start': 42,\n",
      "                 'tag': 'HYPH',\n",
      "                 'word': '-'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'advmod': [13],\n",
      "                                      'conj': [11],\n",
      "                                      'obl:npmod': [7],\n",
      "                                      'punct': [8, 12]}),\n",
      "                 'end': 51,\n",
      "                 'feats': '_',\n",
      "                 'head': 16,\n",
      "                 'lemma': 'endorphin',\n",
      "                 'rel': 'ccomp',\n",
      "                 'start': 43,\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'endorphin'},\n",
      "             10: {'address': 10,\n",
      "                  'ctag': 'CC',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'end': 21,\n",
      "                  'feats': '_',\n",
      "                  'head': 11,\n",
      "                  'lemma': 'and',\n",
      "                  'rel': 'cc',\n",
      "                  'start': 19,\n",
      "                  'tag': 'CC',\n",
      "                  'word': 'and'},\n",
      "             11: {'address': 11,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'cc': [10]}),\n",
      "                  'end': 64,\n",
      "                  'feats': '_',\n",
      "                  'head': 9,\n",
      "                  'lemma': 'morphine',\n",
      "                  'rel': 'conj',\n",
      "                  'start': 57,\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'morphine'},\n",
      "             12: {'address': 12,\n",
      "                  'ctag': ',',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'end': 65,\n",
      "                  'feats': '_',\n",
      "                  'head': 9,\n",
      "                  'lemma': ',',\n",
      "                  'rel': 'punct',\n",
      "                  'start': 65,\n",
      "                  'tag': ',',\n",
      "                  'word': ','},\n",
      "             13: {'address': 13,\n",
      "                  'ctag': 'RB',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'end': 78,\n",
      "                  'feats': '_',\n",
      "                  'head': 9,\n",
      "                  'lemma': 'respectively',\n",
      "                  'rel': 'advmod',\n",
      "                  'start': 67,\n",
      "                  'tag': 'RB',\n",
      "                  'word': 'respectively'},\n",
      "             14: {'address': 14,\n",
      "                  'ctag': ',',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'end': 65,\n",
      "                  'feats': '_',\n",
      "                  'head': 16,\n",
      "                  'lemma': ',',\n",
      "                  'rel': 'punct',\n",
      "                  'start': 65,\n",
      "                  'tag': ',',\n",
      "                  'word': ','},\n",
      "             15: {'address': 15,\n",
      "                  'ctag': 'VBP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'end': 83,\n",
      "                  'feats': '_',\n",
      "                  'head': 16,\n",
      "                  'lemma': 'be',\n",
      "                  'rel': 'aux:pass',\n",
      "                  'start': 81,\n",
      "                  'tag': 'VBP',\n",
      "                  'word': 'are'},\n",
      "             16: {'address': 16,\n",
      "                  'ctag': 'VBN',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'aux:pass': [15],\n",
      "                                       'case': [6],\n",
      "                                       'ccomp': [9],\n",
      "                                       'punct': [14]}),\n",
      "                  'end': 92,\n",
      "                  'feats': '_',\n",
      "                  'head': 2,\n",
      "                  'lemma': 'propose',\n",
      "                  'rel': 'nmod',\n",
      "                  'start': 85,\n",
      "                  'tag': 'VBN',\n",
      "                  'word': 'proposed'},\n",
      "             17: {'address': 17,\n",
      "                  'ctag': '.',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'end': 93,\n",
      "                  'feats': '_',\n",
      "                  'head': 1,\n",
      "                  'lemma': '.',\n",
      "                  'rel': 'punct',\n",
      "                  'start': 93,\n",
      "                  'tag': '.',\n",
      "                  'word': '.'}})\n"
     ]
    }
   ],
   "source": [
    "# file for initial checks\n",
    "file = '/Users/mponsclo/Documents/Master/labAHLT/data/train/3155550.xml'\n",
    "tree = parse(file)\n",
    "sentences = tree.getElementsByTagName(\"sentence\")\n",
    "for s in sentences:\n",
    "    sid = s.attributes[\"id\"].value\n",
    "    stext = s.attributes[\"text\"].value\n",
    "    \n",
    "    entities = {}\n",
    "    ents = s.getElementsByTagName(\"entity\")\n",
    "    for e in ents:\n",
    "        eid = e . attributes[\"id\"].value\n",
    "        entities[eid] = e.attributes[\"charOffset\"].value.split(\"-\")\n",
    "    if len(entities) > 1: analysis = analyze(stext)\n",
    "    \n",
    "    pairs = s.getElementsByTagName(\"pair\")\n",
    "    for p in pairs:\n",
    "        # get ground truth\n",
    "        ddi = p.attributes[\"ddi\"].value\n",
    "        dditype = p.attributes[\"type\"].value if ddi == \"true\" else \"null\"\n",
    "            \n",
    "        # target entities\n",
    "        id_e1 = p.attributes[\"e1\"].value\n",
    "        id_e2 = p.attributes[\"e2\"].value\n",
    "\n",
    "print(entities)\n",
    "print(entities[id_e1][0])\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tree, entities, e1, e2) :\n",
    "    '''\n",
    "    Task:\n",
    "        Given an analyzed sentence and two target entities , compute a feature\n",
    "        vector for this classification example .\n",
    "    Input:\n",
    "        tree: a DependencyGraph object with all sentence information .\n",
    "        entities: A list of all entities in the sentence (id and offsets).\n",
    "        e1, e2: ids of the two entities to be checked for an interaction\n",
    "    Output:\n",
    "        A vector of binary features .\n",
    "        Features are binary and vectors are in sparse representation (i.e. only\n",
    "        active features are listed)\n",
    "   '''\n",
    "        \n",
    "    e1_node = find_entity_in_tree(e1, entities, tree)\n",
    "    e2_node = find_entity_in_tree(e2, entities, tree)\n",
    "    \n",
    "    e1_head = find_head(tree, e1_node) if e1_node else None\n",
    "    e2_head = find_head(tree, e2_node) if e2_node else None\n",
    "    \n",
    "    h1_lemma = e1_head['lemma'] if e1_node else None\n",
    "    h2_lemma = e2_head['lemma'] if e2_node else None\n",
    "\n",
    "    \n",
    "    #e1_tag = e1_node['tag'] if e1_node else None\n",
    "    #e2_tag = e2_node['tag'] if e2_node else None\n",
    "    \n",
    "    #h_e1 = e1_node['head'] if e1_node else None      \n",
    "    #l_h1 = analysis.nodes[h_e1]['lemma'] if e1_node else None\n",
    "    #w_h1 = analysis.nodes[h_e1]['word'] if e1_node else None\n",
    "    #tag_head_e1 = analysis.nodes[h_e1]['tag'][0].lower() if e1_node else None\n",
    "    \n",
    "    #h_e2 = e2_node['head'] if e2_node else None\n",
    "    #l_h2 = analysis.nodes[h_e2]['lemma'] if e2_node else None\n",
    "    #tag_head_e2 = analysis.nodes[h_e2]['tag'][0].lower() if e2_node else None\n",
    "    \n",
    "    # --- FEATURES ---\n",
    "    features = ['h1_lemma=%s' %h1_lemma,\n",
    "                #'h1_word=%s' %e1_head['word'],\n",
    "                'h2_lemma=%s' %h2_lemma,\n",
    "                #'h2_word=%s' %e1_head['word']\n",
    "                ]\n",
    "    \n",
    "    print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h1_lemma=None', 'h2_lemma=endorphin']\n"
     ]
    }
   ],
   "source": [
    "extract_features(analysis, entities, id_e1, id_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../../labAHLT/data/devel\"\n",
    "\n",
    "# process each file in directory\n",
    "for f in listdir(datadir):\n",
    "    # parse XML file , obtaining a DOM tree\n",
    "    tree = parse(datadir + \"/\" + f)\n",
    "    # process each sentence in the file\n",
    "    sentences = tree.getElementsByTagName(\"sentence\")\n",
    "    for s in sentences:\n",
    "\n",
    "        sid = s.attributes[\"id\"].value # get sentence id\n",
    "        stext = s.attributes[\"text\"].value # get sentence text\n",
    "        \n",
    "        # CoreNLP throws error for empty sentences\n",
    "        if len(stext) == 0:\n",
    "            continue\n",
    "\n",
    "        # load sentence ground truth entities\n",
    "        entities = {}\n",
    "        ents = s.getElementsByTagName(\"entity\")\n",
    "        for e in ents:\n",
    "            eid = e . attributes[\"id\"].value\n",
    "            entities[eid] = e.attributes[\"charOffset\"].value.split(\"-\")\n",
    "\n",
    "        # analyze sentence if there is at least a pair of entities\n",
    "        if len(entities) > 1: analysis = analyze(stext)\n",
    "\n",
    "        # for each pair in the sentence , decide whether it is DDI and its type\n",
    "        pairs = s.getElementsByTagName(\"pair\")\n",
    "        for p in pairs:\n",
    "            # get ground truth\n",
    "            ddi = p.attributes[\"ddi\"].value\n",
    "            dditype = p.attributes[\"type\"].value if ddi == \"true\" else \"null\"\n",
    "            \n",
    "            # target entities\n",
    "            id_e1 = p.attributes[\"e1\"].value\n",
    "            id_e2 = p.attributes[\"e2\"].value\n",
    "            \n",
    "            # feature extraction\n",
    "            feats = extract_features(analysis, entities, id_e1, id_e2)\n",
    "            \n",
    "            # resulting feature vector\n",
    "            print(sid, id_e1, id_e2, dditype, \"\\t\".join(feats), sep=\"\\t\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
