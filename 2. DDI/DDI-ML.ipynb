{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDI-ML. Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "cd into ../stanford-corenlp-4.2.0 and run\n",
    "\n",
    "`java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "import networkx\n",
    "# import nltk CoreNLP module (just once)\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "# connect to your CoreNLP server (just once)\n",
    "corenlp_parser = CoreNLPDependencyParser(url=\"http://localhost:9000\")\n",
    "\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets(word, s):\n",
    "    '''\n",
    "    Task:\n",
    "        Given a word and sentence, returns its starting end ending index in the sentence.\n",
    "    \n",
    "    Input:\n",
    "        word: word to find offsets for\n",
    "        s: sentence containing the word\n",
    "    \n",
    "    Output:\n",
    "        Returns a tuple containing the start and end offset.\n",
    "    '''\n",
    "    start = s.find(word)\n",
    "    end = start + len(word) - 1\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    '''\n",
    "    Task:\n",
    "        Helper function\n",
    "    '''\n",
    "    # because otherwise CoreNLP throws 500\n",
    "    return s.replace(\"%\", \"<percentage>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(s):\n",
    "    '''\n",
    "    Task:\n",
    "        Given one sentence, sends it to CoreNLP to obtain the tokens, tags,\n",
    "        and dependency tree. It also adds the start/end offsets to each token.\n",
    "    \n",
    "    Input:\n",
    "        s: string containing the text for one sentence\n",
    "    \n",
    "    Output:\n",
    "        Returns the nltk DependencyGraph object produced by CoreNLP, enriched with token  offsets.\n",
    "\n",
    "    '''\n",
    "    s = s.replace(\"%\", \"<percentage>\")\n",
    "    tree, = corenlp_parser.raw_parse(s)\n",
    "    for n in tree.nodes.items():\n",
    "        node = n[1]\n",
    "        if node['word']:\n",
    "            start, end = get_offsets(node['word'], s)\n",
    "            node['start'] = start\n",
    "            node['end'] = end\n",
    "            \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_in_tree(eid, entities, tree):\n",
    "    start_e1 = entities[eid][0]\n",
    "    end_e1 = entities[eid][1]\n",
    "    \n",
    "    for n in tree.nodes.items():\n",
    "        node = n[1]\n",
    "        if node['word'] and (node['start'] == int(start_e1) and node['end'] == int(end_e1)):\n",
    "            return node\n",
    "    \n",
    "    # TODO: handle two-word entities like \"beta-endorphin\"\n",
    "    # We can extract here word, lemma, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_head(tree, entity):\n",
    "    for n in tree.nodes.items():\n",
    "            node = n[1]\n",
    "            if  node['address'] == entity['head']:\n",
    "                return node\n",
    "    # We can extract here word, lemma, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DDI-DrugBank.d314.s32.e0': ['26', '33'], 'DDI-DrugBank.d314.s32.e1': ['71', '84']}\n",
      "26\n",
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x7fc33f4d9700>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'ROOT': [9]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'end': 6,\n",
      "                 'feats': '_',\n",
      "                 'head': 2,\n",
      "                 'lemma': 'routine',\n",
      "                 'rel': 'amod',\n",
      "                 'start': 0,\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'Routine'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'amod': [1],\n",
      "                                      'nmod': [4]}),\n",
      "                 'end': 21,\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': 'administration',\n",
      "                 'rel': 'nsubj:pass',\n",
      "                 'start': 8,\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'administration'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'end': 24,\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': 'of',\n",
      "                 'rel': 'case',\n",
      "                 'start': 23,\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'of'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'NNS',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'case': [3],\n",
      "                                      'conj': [6]}),\n",
      "                 'end': 33,\n",
      "                 'feats': '_',\n",
      "                 'head': 2,\n",
      "                 'lemma': 'vaccine',\n",
      "                 'rel': 'nmod',\n",
      "                 'start': 26,\n",
      "                 'tag': 'NNS',\n",
      "                 'word': 'vaccines'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'CC',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'end': 36,\n",
      "                 'feats': '_',\n",
      "                 'head': 6,\n",
      "                 'lemma': 'or',\n",
      "                 'rel': 'cc',\n",
      "                 'start': 35,\n",
      "                 'tag': 'CC',\n",
      "                 'word': 'or'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'NNS',\n",
      "                 'deps': defaultdict(<class 'list'>, {'cc': [5]}),\n",
      "                 'end': 44,\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': 'toxoid',\n",
      "                 'rel': 'conj',\n",
      "                 'start': 38,\n",
      "                 'tag': 'NNS',\n",
      "                 'word': 'toxoids'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'MD',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'end': 51,\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': 'should',\n",
      "                 'rel': 'aux',\n",
      "                 'start': 46,\n",
      "                 'tag': 'MD',\n",
      "                 'word': 'should'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'VB',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'end': 54,\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': 'be',\n",
      "                 'rel': 'aux:pass',\n",
      "                 'start': 53,\n",
      "                 'tag': 'VB',\n",
      "                 'word': 'be'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'VBN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'advcl': [14],\n",
      "                                      'aux': [7],\n",
      "                                      'aux:pass': [8],\n",
      "                                      'nsubj:pass': [2],\n",
      "                                      'punct': [17]}),\n",
      "                 'end': 63,\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': 'defer',\n",
      "                 'rel': 'ROOT',\n",
      "                 'start': 56,\n",
      "                 'tag': 'VBN',\n",
      "                 'word': 'deferred'},\n",
      "             10: {'address': 10,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'end': 69,\n",
      "                  'feats': '_',\n",
      "                  'head': 14,\n",
      "                  'lemma': 'until',\n",
      "                  'rel': 'mark',\n",
      "                  'start': 65,\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'until'},\n",
      "             11: {'address': 11,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'end': 84,\n",
      "                  'feats': '_',\n",
      "                  'head': 12,\n",
      "                  'lemma': 'corticosteroid',\n",
      "                  'rel': 'compound',\n",
      "                  'start': 71,\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'corticosteroid'},\n",
      "             12: {'address': 12,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'compound': [11]}),\n",
      "                  'end': 92,\n",
      "                  'feats': '_',\n",
      "                  'head': 14,\n",
      "                  'lemma': 'therapy',\n",
      "                  'rel': 'nsubj:pass',\n",
      "                  'start': 86,\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'therapy'},\n",
      "             13: {'address': 13,\n",
      "                  'ctag': 'VBZ',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'end': 14,\n",
      "                  'feats': '_',\n",
      "                  'head': 14,\n",
      "                  'lemma': 'be',\n",
      "                  'rel': 'aux:pass',\n",
      "                  'start': 13,\n",
      "                  'tag': 'VBZ',\n",
      "                  'word': 'is'},\n",
      "             14: {'address': 14,\n",
      "                  'ctag': 'VBN',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'advcl': [16],\n",
      "                                       'aux:pass': [13],\n",
      "                                       'mark': [10],\n",
      "                                       'nsubj:pass': [12]}),\n",
      "                  'end': 108,\n",
      "                  'feats': '_',\n",
      "                  'head': 9,\n",
      "                  'lemma': 'discontinue',\n",
      "                  'rel': 'advcl',\n",
      "                  'start': 97,\n",
      "                  'tag': 'VBN',\n",
      "                  'word': 'discontinued'},\n",
      "             15: {'address': 15,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'end': 111,\n",
      "                  'feats': '_',\n",
      "                  'head': 16,\n",
      "                  'lemma': 'if',\n",
      "                  'rel': 'mark',\n",
      "                  'start': 110,\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'if'},\n",
      "             16: {'address': 16,\n",
      "                  'ctag': 'JJ',\n",
      "                  'deps': defaultdict(<class 'list'>, {'mark': [15]}),\n",
      "                  'end': 120,\n",
      "                  'feats': '_',\n",
      "                  'head': 14,\n",
      "                  'lemma': 'possible',\n",
      "                  'rel': 'advcl',\n",
      "                  'start': 113,\n",
      "                  'tag': 'JJ',\n",
      "                  'word': 'possible'},\n",
      "             17: {'address': 17,\n",
      "                  'ctag': '.',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'end': 121,\n",
      "                  'feats': '_',\n",
      "                  'head': 9,\n",
      "                  'lemma': '.',\n",
      "                  'rel': 'punct',\n",
      "                  'start': 121,\n",
      "                  'tag': '.',\n",
      "                  'word': '.'}})\n"
     ]
    }
   ],
   "source": [
    "# file for initial checks\n",
    "file = '/Users/mponsclo/Documents/Master/labAHLT/data/train/3155550.xml'\n",
    "file = '../../labAHLT/data/train/Dexamethasone_ddi.xml'\n",
    "tree = parse(file)\n",
    "sentences = tree.getElementsByTagName(\"sentence\")\n",
    "for s in sentences:\n",
    "    sid = s.attributes[\"id\"].value\n",
    "    stext = s.attributes[\"text\"].value\n",
    "    \n",
    "    entities = {}\n",
    "    ents = s.getElementsByTagName(\"entity\")\n",
    "    for e in ents:\n",
    "        eid = e . attributes[\"id\"].value\n",
    "        entities[eid] = e.attributes[\"charOffset\"].value.split(\"-\")\n",
    "    if len(entities) > 1: analysis = analyze(stext)\n",
    "    \n",
    "    pairs = s.getElementsByTagName(\"pair\")\n",
    "    for p in pairs:\n",
    "        # get ground truth\n",
    "        ddi = p.attributes[\"ddi\"].value\n",
    "        dditype = p.attributes[\"type\"].value if ddi == \"true\" else \"null\"\n",
    "            \n",
    "        # target entities\n",
    "        id_e1 = p.attributes[\"e1\"].value\n",
    "        id_e2 = p.attributes[\"e2\"].value\n",
    "\n",
    "print(entities)\n",
    "print(entities[id_e1][0])\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_path(path, tree):\n",
    "    if len(path) == 0:\n",
    "        return None\n",
    "    path_nodes = [tree.nodes[x] for x in path]\n",
    "    str_path = \"\"\n",
    "    # traverse from e1 up\n",
    "    current_node = path_nodes[0]\n",
    "    while (current_node['head'] in path):\n",
    "        \n",
    "        rel = current_node['rel']\n",
    "        current_node = tree.nodes[current_node['head']]\n",
    "        str_path += (rel + '<')\n",
    "    \n",
    "    str_path += current_node['lemma']\n",
    "    # traverse from e2 up\n",
    "    current_node = path_nodes[-1]\n",
    "    while(current_node['head'] in path):\n",
    "        rel = current_node['rel']\n",
    "        current_node = tree.nodes[current_node['head']]\n",
    "        str_path += ('>' + rel)\n",
    "        \n",
    "    return str_path\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tree, entities, e1, e2) :\n",
    "    '''\n",
    "    Task:\n",
    "        Given an analyzed sentence and two target entities , compute a feature\n",
    "        vector for this classification example .\n",
    "    Input:\n",
    "        tree: a DependencyGraph object with all sentence information .\n",
    "        entities: A list of all entities in the sentence (id and offsets).\n",
    "        e1, e2: ids of the two entities to be checked for an interaction\n",
    "    Output:\n",
    "        A vector of binary features .\n",
    "        Features are binary and vectors are in sparse representation (i.e. only\n",
    "        active features are listed)\n",
    "   '''\n",
    "    \n",
    "    \n",
    "        \n",
    "    e1_node = find_entity_in_tree(e1, entities, tree)\n",
    "    e2_node = find_entity_in_tree(e2, entities, tree)\n",
    "    \n",
    "    e1_head = find_head(tree, e1_node) if e1_node else None\n",
    "    e2_head = find_head(tree, e2_node) if e2_node else None\n",
    "    \n",
    "    h1_lemma = e1_head['lemma'] if e1_node else None\n",
    "    h2_lemma = e2_head['lemma'] if e2_node else None\n",
    "    \n",
    "    tag_head_e1 = e1_head['tag'] if e1_head else None\n",
    "    tag_head_e2 = e2_head['tag'] if e2_head else None\n",
    "    \n",
    "    nxgraph = tree.nx_graph().to_undirected()\n",
    "    shortest_path = networkx.shortest_path(nxgraph, e1_node['address'], e2_node['address']) if (e1_node and e2_node) else []\n",
    "    path = traverse_path(shortest_path, analysis)\n",
    "\n",
    "    \n",
    "    #e1_tag = e1_node['tag'] if e1_node else None\n",
    "    #e2_tag = e2_node['tag'] if e2_node else None\n",
    "    \n",
    "    #h_e1 = e1_node['head'] if e1_node else None      \n",
    "    #l_h1 = analysis.nodes[h_e1]['lemma'] if e1_node else None\n",
    "    #w_h1 = analysis.nodes[h_e1]['word'] if e1_node else None\n",
    "    #tag_head_e1 = analysis.nodes[h_e1]['tag'][0].lower() if e1_node else None\n",
    "    \n",
    "    #h_e2 = e2_node['head'] if e2_node else None\n",
    "    #l_h2 = analysis.nodes[h_e2]['lemma'] if e2_node else None\n",
    "    #tag_head_e2 = analysis.nodes[h_e2]['tag'][0].lower() if e2_node else None\n",
    "    \n",
    "    # --- FEATURES ---\n",
    "    features = ['h1_lemma=%s' %h1_lemma,\n",
    "                'h2_lemma=%s' %h2_lemma,\n",
    "                'h1_tag=%s' %tag_head_e1,\n",
    "                'h2_tag=%s' %tag_head_e2,\n",
    "                'path=%s' % path\n",
    "                \n",
    "                ]\n",
    "    print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h1_lemma=increase', 'h2_lemma=None', 'h1_tag=VB', 'h2_tag=None', 'path=None']\n",
      "['h1_lemma=increase', 'h2_lemma=None', 'h1_tag=VB', 'h2_tag=None', 'path=None']\n",
      "['h1_lemma=increase', 'h2_lemma=drug', 'h1_tag=VB', 'h2_tag=NNS', 'path=anticoagulant>dep>nmod>conj>nmod']\n",
      "['h1_lemma=increase', 'h2_lemma=drug', 'h1_tag=VB', 'h2_tag=NNS', 'path=anticoagulant>conj>nmod>conj>nmod']\n",
      "['h1_lemma=increase', 'h2_lemma=administer', 'h1_tag=VB', 'h2_tag=VBN', 'path=nsubj<increase>obl>dep>advmod']\n",
      "['h1_lemma=None', 'h2_lemma=None', 'h1_tag=None', 'h2_tag=None', 'path=None']\n",
      "['h1_lemma=None', 'h2_lemma=drug', 'h1_tag=None', 'h2_tag=NNS', 'path=None']\n",
      "['h1_lemma=None', 'h2_lemma=drug', 'h1_tag=None', 'h2_tag=NNS', 'path=None']\n",
      "['h1_lemma=None', 'h2_lemma=administer', 'h1_tag=None', 'h2_tag=VBN', 'path=None']\n",
      "['h1_lemma=None', 'h2_lemma=drug', 'h1_tag=None', 'h2_tag=NNS', 'path=None']\n",
      "['h1_lemma=None', 'h2_lemma=drug', 'h1_tag=None', 'h2_tag=NNS', 'path=None']\n",
      "['h1_lemma=None', 'h2_lemma=administer', 'h1_tag=None', 'h2_tag=VBN', 'path=None']\n",
      "['h1_lemma=drug', 'h2_lemma=drug', 'h1_tag=NNS', 'h2_tag=NNS', 'path=dep<drug>conj']\n",
      "['h1_lemma=drug', 'h2_lemma=administer', 'h1_tag=NNS', 'h2_tag=VBN', 'path=dep<nmod<conj<nmod<nsubj<increase>obl>dep>advmod']\n",
      "['h1_lemma=drug', 'h2_lemma=administer', 'h1_tag=NNS', 'h2_tag=VBN', 'path=conj<nmod<conj<nmod<nsubj<increase>obl>dep>advmod']\n"
     ]
    }
   ],
   "source": [
    "    for p in pairs:\n",
    "        # get ground truth\n",
    "        ddi = p.attributes[\"ddi\"].value\n",
    "        dditype = p.attributes[\"type\"].value if ddi == \"true\" else \"null\"\n",
    "            \n",
    "        # target entities\n",
    "        id_e1 = p.attributes[\"e1\"].value\n",
    "        id_e2 = p.attributes[\"e2\"].value\n",
    "\n",
    "        extract_features(analysis, entities, id_e1, id_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only join an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-1663c96f9d8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# resulting feature vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_e1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_e2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdditype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only join an iterable"
     ]
    }
   ],
   "source": [
    "datadir = \"../../labAHLT/data/devel\"\n",
    "\n",
    "# process each file in directory\n",
    "for f in listdir(datadir):\n",
    "    # parse XML file , obtaining a DOM tree\n",
    "    tree = parse(datadir + \"/\" + f)\n",
    "    # process each sentence in the file\n",
    "    sentences = tree.getElementsByTagName(\"sentence\")\n",
    "    for s in sentences:\n",
    "\n",
    "        sid = s.attributes[\"id\"].value # get sentence id\n",
    "        stext = s.attributes[\"text\"].value # get sentence text\n",
    "        \n",
    "        # CoreNLP throws error for empty sentences\n",
    "        if len(stext) == 0:\n",
    "            continue\n",
    "\n",
    "        # load sentence ground truth entities\n",
    "        entities = {}\n",
    "        ents = s.getElementsByTagName(\"entity\")\n",
    "        for e in ents:\n",
    "            eid = e . attributes[\"id\"].value\n",
    "            entities[eid] = e.attributes[\"charOffset\"].value.split(\"-\")\n",
    "\n",
    "        # analyze sentence if there is at least a pair of entities\n",
    "        if len(entities) > 1: analysis = analyze(stext)\n",
    "\n",
    "        # for each pair in the sentence , decide whether it is DDI and its type\n",
    "        pairs = s.getElementsByTagName(\"pair\")\n",
    "        for p in pairs:\n",
    "            # get ground truth\n",
    "            ddi = p.attributes[\"ddi\"].value\n",
    "            dditype = p.attributes[\"type\"].value if ddi == \"true\" else \"null\"\n",
    "            \n",
    "            # target entities\n",
    "            id_e1 = p.attributes[\"e1\"].value\n",
    "            id_e2 = p.attributes[\"e2\"].value\n",
    "            \n",
    "            # feature extraction\n",
    "            feats = extract_features(analysis, entities, id_e1, id_e2)\n",
    "            \n",
    "            # resulting feature vector\n",
    "            print(sid, id_e1, id_e2, dditype, \"\\t\".join(feats), sep=\"\\t\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
