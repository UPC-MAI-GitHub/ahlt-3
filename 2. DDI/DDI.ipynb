{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "cd into ../stanford-corenlp-4.2.0 and run\n",
    "\n",
    "`java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "# import nltk CoreNLP module (just once)\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "# connect to your CoreNLP server (just once)\n",
    "corenlp_parser = CoreNLPDependencyParser(url=\"http://localhost:9000\")\n",
    "\n",
    "from evaluator import *\n",
    "import networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets(word, s):\n",
    "    '''\n",
    "    Task:\n",
    "        Given a word and sentence, returns its starting end ending index in the sentence.\n",
    "    \n",
    "    Input:\n",
    "        word: word to find offsets for\n",
    "        s: sentence containing the word\n",
    "    \n",
    "    Output:\n",
    "        Returns a tuple containing the start and end offset.\n",
    "    '''\n",
    "    start = s.find(word)\n",
    "    end = start + len(word) - 1\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    '''\n",
    "    Task:\n",
    "        Helper function\n",
    "    '''\n",
    "    # because otherwise CoreNLP throws 500\n",
    "    return s.replace(\"%\", \"<percentage>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(s):\n",
    "    '''\n",
    "    Task:\n",
    "        Given one sentence, sends it to CoreNLP to obtain the tokens, tags,\n",
    "        and dependency tree. It also adds the start/end offsets to each token.\n",
    "    \n",
    "    Input:\n",
    "        s: string containing the text for one sentence\n",
    "    \n",
    "    Output:\n",
    "        Returns the nltk DependencyGraph object produced by CoreNLP, enriched with token  offsets.\n",
    "\n",
    "    '''\n",
    "    s = s.replace(\"%\", \"<percentage>\")\n",
    "    tree, = corenlp_parser.raw_parse(s)\n",
    "    for n in tree.nodes.items():\n",
    "        node = n[1]\n",
    "        if node['word']:\n",
    "            start, end = get_offsets(node['word'], s)\n",
    "            node['start'] = start\n",
    "            node['end'] = end\n",
    "            \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_in_tree(entity, tree):\n",
    "    for n in tree.nodes.items():\n",
    "        node = n[1]\n",
    "        try:\n",
    "            if node[\"word\"] and (node[\"start\"] == int(entity[0])): # and (node[\"end\"] == int(entity[1])): +1%\n",
    "                return node\n",
    "        except:\n",
    "            continue\n",
    "    # PROBLEM: Two-word entities?\n",
    "    # ValueError: invalid literal for int() with base 10: '34;50'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_interaction(analysis, entities, e1, e2):\n",
    "    '''\n",
    "    Task:\n",
    "        Decide whether a sentence is expressing a DDI between two drugs.\n",
    "    \n",
    "    Input:\n",
    "        analysis: a DependencyGraph object with all sentence information\n",
    "        entities: a list of all entities in the sentence (id and offsets)\n",
    "        e1, e2: ids of the two entities to be checked\n",
    "    \n",
    "    Output:\n",
    "        Returns the type of interaction ('effect', 'mechanism', 'advice', 'int') between e1 and e2\n",
    "        expressed by the sentence, or 'None' if no interaction is described.\n",
    "    '''\n",
    "    tree = analysis.tree()\n",
    "    \n",
    "    entity1 = entities[e1]\n",
    "    entity2 = entities[e2]\n",
    "    \n",
    "    e1_node = find_entity_in_tree(entity1, analysis)\n",
    "    e2_node = find_entity_in_tree(entity2, analysis)\n",
    "    \n",
    "    e1_tag = e1_node['tag'] if e1_node else None\n",
    "    e2_tag = e2_node['tag'] if e2_node else None\n",
    "    \n",
    "    h_e1 = e1_node['head'] if e1_node else None      # position of the head\n",
    "    head_e1 = analysis.nodes[h_e1]['lemma'] if e1_node else None\n",
    "    tag_head_e1 = analysis.nodes[h_e1]['tag'][0].lower() if e1_node else None\n",
    "    \n",
    "    h_e2 = e2_node['head'] if e2_node else None      # position of the head\n",
    "    head_e2 = analysis.nodes[h_e2]['lemma'] if e2_node else None\n",
    "    tag_head_e2 = analysis.nodes[h_e2]['tag'][0].lower() if e2_node else None\n",
    "    \n",
    "    nxgraph = analysis.nx_graph().to_undirected()\n",
    "    \n",
    "    shortest_path = networkx.shortest_path(nxgraph, e1_node['address'], e2_node['address']) if (e1_node and e2_node) else []\n",
    "    shortest_path = [analysis.nodes[x]['lemma'] for x in shortest_path]\n",
    "    \n",
    "    if head_e1 == head_e2 and tag_head_e1 == 'v' and tag_head_e2 == 'v':\n",
    "        under_same_verb = True\n",
    "    else:\n",
    "        under_same_verb = False\n",
    "        \n",
    "    if head_e1 == head_e2 and head_e1 != None and head_e2 != None:\n",
    "        under_same_word = True\n",
    "    else:\n",
    "        under_same_word = False\n",
    "    \n",
    "    if head_e1 == entity2 and head_e1 != None:\n",
    "        e1_under_e2 = True\n",
    "    else: \n",
    "        e1_under_e2 = False\n",
    "    \n",
    "    if head_e2 == entity1 and head_e2 != None:\n",
    "        e2_under_e1 = True\n",
    "    else:\n",
    "        e2_under_e1 = False\n",
    "        \n",
    "    # --- RULES ---\n",
    "    if 'effect' in shortest_path:\n",
    "        return 'effect'\n",
    "    if 'increase' in shortest_path or 'decrease' in shortest_path or 'concentration' in shortest_path:\n",
    "        return 'mechanism'\n",
    "    if 'interact' in shortest_path:\n",
    "        return 'int'\n",
    "    if 'patient' in shortest_path:\n",
    "        return 'advise'\n",
    "    if head_e1  == 'response' and head_e2 in ['man', 'alcohol', 'steroid']:\n",
    "        return 'effect'\n",
    "    if head_e1 == 'administer' and head_e2 == 'administer': # and under_same_word:\n",
    "        return 'advise'\n",
    "    if head_e1 in ['response', 'effect', 'enhance', 'diminish']:\n",
    "        return 'effect'\n",
    "    if head_e1 in ['concentration', 'presence', 'dose', 'absorption', 'interfere']:\n",
    "        return 'mechanism'\n",
    "    if head_e1 in ['interact', 'interaction']: # removed action & agent -> improved 2.7% points / try removing interactoin\n",
    "        return 'int'\n",
    "    if head_e1 in ['co-administration', 'take', 'coadminister', 'treatment', 'therapy', 'tell']:\n",
    "        return 'advise'\n",
    "    if head_e2 in ['effect', 'alcohol', 'action','use', 'combination', 'inhibitor']: # removed 'use' -0.4%\n",
    "        return 'effect'\n",
    "    if head_e2 in ['metabolism', 'concentration', 'clearance', 'level', 'absorption', 'dose']:\n",
    "        return 'mechanism'\n",
    "    if head_e2 in ['interact', 'interaction']: # add 'aminoglycoside', 'antibiotic' -> no improvement\n",
    "        return 'int'\n",
    "    #if under_same_word:\n",
    "     #   return 'mechanism' # or \"int\"  ## --> removing improves f1 to 4.5% points\n",
    "    #if e1_under_e2:\n",
    "     #   return 'effect'\n",
    "    \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../../labAHLT/data/devel\"\n",
    "\n",
    "# process each file in directory\n",
    "for f in listdir (datadir):\n",
    "    # parse XML file , obtaining a DOM tree\n",
    "    tree = parse ( datadir + \"/\" + f)\n",
    "    # process each sentence in the file\n",
    "    sentences = tree.getElementsByTagName(\"sentence\")\n",
    "    for s in sentences:\n",
    "\n",
    "        sid = s.attributes [\"id\"].value # get sentence id\n",
    "        stext = s.attributes [\"text\"].value # get sentence text\n",
    "        \n",
    "        # CoreNLP throws error for empty sentences\n",
    "        if len(stext) == 0:\n",
    "            continue\n",
    "\n",
    "        # load sentence entities into a dictionary\n",
    "        entities = {}\n",
    "        ents = s.getElementsByTagName(\"entity\")\n",
    "        for e in ents:\n",
    "            eid = e . attributes [\"id\"].value\n",
    "            entities[eid] = e.attributes[\"charOffset\"].value.split(\"-\")\n",
    "\n",
    "        # Tokenize, tag, and parse sentence\n",
    "        analysis = analyze(stext)\n",
    "\n",
    "        # for each pair in the sentence , decide whether it is DDI and its type\n",
    "        pairs = s.getElementsByTagName(\"pair\")\n",
    "        for p in pairs:\n",
    "            id_e1 = p.attributes[\"e1\"].value\n",
    "            id_e2 = p.attributes[\"e2\"].value\n",
    "            ddi_type = check_interaction(analysis, entities , id_e1 , id_e2 )\n",
    "            if ddi_type != None:\n",
    "                pass\n",
    "                #print(sid +\"|\"+ id_e1 +\"|\"+ id_e2 +\"|\"+ ddi_type)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../../labAHLT/data/devel\"\n",
    "datadir = \"../../labAHLT/data/test\" # Test Data\n",
    "outf = open('results.txt', \"w\")\n",
    "\n",
    "# process each file in directory\n",
    "for f in listdir(datadir):\n",
    "    \n",
    "    # parse XML file , obtaining a DOM tree\n",
    "    tree = parse(datadir + \"/\" + f)\n",
    "    # process each sentence in the file\n",
    "    sentences = tree.getElementsByTagName(\"sentence\")\n",
    "    for s in sentences:\n",
    "\n",
    "        sid = s.attributes[\"id\"].value # get sentence id\n",
    "        stext = s.attributes[\"text\"].value # get sentence text\n",
    "        \n",
    "        # CoreNLP throws error for empty sentences\n",
    "        if len(stext) == 0:\n",
    "            continue\n",
    "\n",
    "        # load sentence entities into a dictionary\n",
    "        entities = {}\n",
    "        ents = s.getElementsByTagName(\"entity\")\n",
    "        for e in ents:\n",
    "            eid = e.attributes[\"id\"].value\n",
    "            entities[eid] = e.attributes[\"charOffset\"].value.split(\"-\")\n",
    "\n",
    "        # Tokenize, tag, and parse sentence\n",
    "        analysis = analyze(stext)\n",
    "\n",
    "        # for each pair in the sentence , decide whether it is DDI and its type\n",
    "        pairs = s.getElementsByTagName(\"pair\")\n",
    "        for p in pairs:\n",
    "            id_e1 = p.attributes[\"e1\"].value\n",
    "            id_e2 = p.attributes[\"e2\"].value\n",
    "            ddi_type = check_interaction(analysis, entities, id_e1, id_e2)\n",
    "            if ddi_type != None:\n",
    "                entity = str(sid +\"|\"+ id_e1 +\"|\"+ id_e2 +\"|\"+ ddi_type)\n",
    "                outf.write(entity + '\\n')\n",
    "outf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
      "------------------------------------------------------------------------------\n",
      "advise             56\t 167\t 156\t 223\t 212\t25.1%\t26.4%\t25.7%\n",
      "effect            110\t 370\t 173\t 480\t 283\t22.9%\t38.9%\t28.8%\n",
      "int                 7\t  55\t  11\t  62\t  18\t11.3%\t38.9%\t17.5%\n",
      "mechanism         158\t 337\t 179\t 495\t 337\t31.9%\t46.9%\t38.0%\n",
      "------------------------------------------------------------------------------\n",
      "M.avg            -\t-\t-\t-\t-\t22.8%\t37.8%\t27.5%\n",
      "------------------------------------------------------------------------------\n",
      "m.avg             331\t 929\t 519\t1260\t 850\t26.3%\t38.9%\t31.4%\n",
      "m.avg(no class)   419\t 841\t 431\t1260\t 850\t33.3%\t49.3%\t39.7%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Check meaning -> Ignoring duplicated entity in system predictions file: ' + line\n",
    "# Removed print() for clarity\n",
    "\n",
    "evaluate('DDI', datadir, 'results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StanfordCoreNLP throws error 500 when a sentence contains '%':\n",
    "#    Illegal hex characters in escape (%) pattern - Error at index 0 in: \" a\"\n",
    "#  java.base/java.net.URLDecoder.decode(URLDecoder.java:232)\n",
    "#  java.base/java.net.URLDecoder.decode(URLDecoder.java:142)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
